# Technical Debt Catalog & Optimization Plan

**Generated:** 2025-11-23
**Validation Platform:** Windows x64
**Codebase Version:** Post-Phase 4.1 (Fusion Suite Validated)

---

## Executive Summary

This document catalogs all identified technical debt, missing features, optimization opportunities, and code quality issues in the Ternary Engine codebase. It prioritizes fixes by impact and provides an actionable roadmap for addressing each item.

**Overall Assessment:** The codebase is **well-architected** with excellent separation of concerns, comprehensive documentation, and validated performance claims. Technical debt is **moderate** and primarily consists of:
1. Missing build artifacts (fusion module)
2. Code duplication in template functions (~70% similarity)
3. Incomplete matmul optimization
4. Benchmark script import issues

---

## Priority Classification

- **ðŸ”´ CRITICAL** - Blocks functionality, must fix immediately
- **ðŸŸ  HIGH** - Significant impact on usability/performance
- **ðŸŸ¡ MEDIUM** - Moderate improvement potential
- **ðŸŸ¢ LOW** - Nice-to-have, minimal impact

---

## ðŸ”´ CRITICAL Issues

### 1. Missing Fusion Module Build Script

**Problem:**
Multiple benchmarks import `ternary_fusion_engine` but no build script exists.

**Impact:**
- `bench_fusion.py` fails to run
- `benchmarks/micro/bench_fusion_*.py` (4 files) unusable
- `benchmarks/macro/bench_neural_layer.py` cannot test fusion
- `benchmarks/macro/bench_image_pipeline.py` missing fusion backend

**Files Expecting Module:**
```python
# benchmarks/bench_fusion.py:24
import ternary_fusion_engine as fusion  # â† MODULE NOT BUILT

# All fusion-related benchmarks fail with ImportError
```

**Root Cause:**
Fusion operations (`fused_tnot_tadd`, `fused_tnot_tmul`, `fused_tnot_tmin`, `fused_tnot_tmax`) are:
- âœ… Implemented in `src/core/simd/ternary_fusion.h` (validated Phase 4.0/4.1)
- âœ… Exposed in main `ternary_simd_engine` module (lines 333-347, 369-376)
- âŒ NOT built as separate `ternary_fusion_engine` module

**Resolution Options:**

**Option A: Integration** (Recommended)
- Update benchmarks to import from `ternary_simd_engine`:
  ```python
  # OLD (broken)
  import ternary_fusion_engine as fusion
  fusion.fused_tnot_tadd(a, b)

  # NEW (working)
  import ternary_simd_engine as ternary
  ternary.fused_tnot_tadd(a, b)
  ```

**Option B: Separate Module**
- Create `build/build_fusion.py`
- Build standalone fusion module
- Maintains backward compatibility with existing benchmark imports

**Recommendation:** Option A (integration) - fusion operations are already in main engine, no need for separate module.

**Files to Fix:**
- `benchmarks/bench_fusion.py`
- `benchmarks/micro/bench_fusion_*.py` (4 files)
- `benchmarks/macro/bench_neural_layer.py`
- `benchmarks/macro/bench_image_pipeline.py`

---

### 2. Benchmark Import Consistency Issues

**Problem:**
Some benchmarks import modules that don't exist or have inconsistent naming.

**Examples:**
```python
# bench_fusion.py - expects separate fusion module (doesn't exist)
import ternary_fusion_engine as fusion

# bench_neural_layer.py - same issue
import ternary_fusion_engine as fusion

# bench_competitive.py - works correctly
import ternary_simd_engine as ternary
```

**Impact:**
6+ benchmark scripts fail with `ImportError`.

**Resolution:**
Standardize all benchmark imports to use existing modules:
- `ternary_simd_engine` (main operations + fusion)
- `ternary_dense243_module` (Dense243 encoding)

---

## ðŸŸ  HIGH Priority

### 3. Code Duplication in Process Templates

**Problem:**
`process_binary_array()` (87 lines) and `process_unary_array()` (54 lines) share ~70% identical code.

**Location:** `src/engine/ternary_simd_engine.cpp:138-301`

**Duplication Analysis:**
```cpp
// IDENTICAL PATTERN IN BOTH FUNCTIONS (70% duplication):
// 1. OMP threshold check
if (n >= OMP_THRESHOLD) {
    // Streaming stores logic
    bool use_streaming = (n >= STREAM_THRESHOLD) && is_aligned_32(r_ptr);

    #pragma omp parallel for schedule(guided, 4)
    for (ssize_t idx = 0; idx < n_simd_blocks; idx += 32) {
        // Prefetching
        if (idx + PREFETCH_DIST < n_simd_blocks) {
            _mm_prefetch(...);
        }

        // SIMD operation (ONLY DIFFERENCE)
        // Binary: simd_op(va, vb)
        // Unary:  simd_op(va)

        // Streaming store or regular store
        if (use_streaming) {
            _mm256_stream_si256(...);
        } else {
            _mm256_storeu_si256(...);
        }
    }

    if (use_streaming) _mm_sfence();
}
```

**Recommendation:**
Extract common loop patterns into helper templates. Current 141 lines â†’ ~90 lines (-36% code).

**Example Refactoring:**
```cpp
template <typename LoadFunc, typename StoreFunc, typename Op>
inline void simd_loop_with_prefetch(
    ssize_t n,
    const uint8_t* inputs[],
    uint8_t* output,
    LoadFunc load,
    StoreFunc store,
    Op op
) {
    // Common OMP + SIMD + prefetch logic here
}

// Then use in process_binary_array and process_unary_array
```

**Impact:**
- 36% code reduction
- Easier maintenance
- Single source of truth for optimization flags
- Better compiler optimization opportunities

---

### 4. Missing Proper Ternary Matrix Multiplication

**Problem:**
Current TritNet uses PyTorch's full-precision `F.linear()` for matmul. No SIMD-optimized ternary matmul exists.

**Current State:**
```python
# models/tritnet/src/ternary_layers.py:163
def forward(self, input):
    return F.linear(input, weight_to_use, self.bias)
    # â†‘ Uses PyTorch FP32/FP16 matmul, not ternary-optimized
```

**Benchmark Workaround:**
```python
# benchmarks/macro/bench_neural_layer.py:41
def ternary_matmul_simple(X, W):
    # NOT A REAL MATMUL - just element-wise proxy
    result = ternary.tmul(X, W)
    return result
```

**Optimization Opportunities:**

1. **Ternary Matrix Multiplication Kernel (SIMD)**
   - Input: `A[M, K]` and `B[K, N]` (ternary values {-1, 0, +1})
   - Output: `C[M, N]`
   - Operation: `C[i,j] = Î£(A[i,k] * B[k,j])` for k âˆˆ [0, K)

2. **SIMD Implementation Strategy:**
   - Use AVX2 for 32-wide dot products
   - Accumulate using `_mm256_add_epi32` (32-bit accumulators)
   - Exploit ternary sparsity (zero weights skip computation)

3. **Expected Speedup:**
   - 10-20Ã— over element-wise operations
   - 2-5Ã— over full-precision matmul for sparse ternary weights

4. **Integration Path:**
   - Add `tmatmul()` to `ternary_simd_engine.cpp`
   - Create SIMD kernel in `src/core/simd/ternary_matmul_kernels.h`
   - Expose to Python via pybind11
   - Update TritNet to use ternary matmul

**Files to Create/Modify:**
- `src/core/simd/ternary_matmul_kernels.h` (new)
- `src/engine/ternary_simd_engine.cpp` (add tmatmul binding)
- `models/tritnet/src/ternary_layers.py` (use ternary matmul)

---

### 5. Dense243 SIMD Pack Operation Inefficiency

**Problem:**
`dense243_pack_simd()` uses ~30 addition operations to compute base-243 encoding.

**Location:** `src/engine/experimental/dense243/ternary_dense243_simd.h:143-211`

**Current Implementation:**
```cpp
// Lines 180-201: Multiplication by powers of 3 using repeated addition
__m256i o1_times_3 = _mm256_add_epi8(o1, _mm256_add_epi8(o1, o1));
__m256i o2_times_9 = _mm256_add_epi8(o2_times_3, _mm256_add_epi8(o2_times_3, o2_times_3));
__m256i o3_times_27 = _mm256_add_epi8(o3_times_9, _mm256_add_epi8(o3_times_9, o3_times_9));
__m256i o4_times_81 = _mm256_add_epi8(o4_times_27, _mm256_add_epi8(o4_times_27, o4_times_27));
```

**Performance:**
- Extraction: 5 shuffles (~5 cycles)
- Packing: 30+ additions (~30 cycles)
- **Total overhead: 45Ã— vs direct 2-bit SIMD**

**Optimization Idea:**
Pre-compute packing LUT (similar to extraction LUTs):
```cpp
// All 243 possible 5-trit combinations â†’ packed byte
constexpr auto DENSE243_PACK_LUT = make_dense243_pack_lut();
// Then use 5D shuffle or iterative lookup
```

**Challenge:**
5D LUT doesn't fit in single shuffle (need sequential lookups or different strategy).

**Recommendation:**
Dense243 is memory-optimized, not compute-optimized. Current implementation is acceptable for storage/transmission use cases.

---

## ðŸŸ¡ MEDIUM Priority

### 6. Platform Validation Gap

**Problem:**
Only Windows x64 has production validation. Linux/macOS builds are untested.

**Current Status:**
```
Platform           Build System    Tests    Status
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Windows x64 (MSVC) build.py        65/65    âœ… Validated
Linux x64 (GCC)    build.py        ?        âš ï¸ Untested
macOS Intel        build.py        ?        âš ï¸ Untested
macOS ARM (M1/M2)  build.py        0        âœ… Detects, skips
```

**Impact:**
Cannot make production claims for Linux/macOS deployments.

**Resolution:**
1. Enable GitHub Actions CI for Linux/macOS
2. Run full test suite on both platforms
3. Validate benchmark results match Windows performance
4. Document any platform-specific issues

---

### 7. OpenMP Re-enablement

**Problem:**
OpenMP disabled in CI due to documented crash (root cause fixed but needs validation).

**Status:**
- âœ… Builds with `/openmp` (MSVC) or `-fopenmp` (GCC)
- âœ… Works in manual builds
- âŒ Disabled in automated tests (`tests/test_omp.py`)
- âœ… Root cause identified and fixed (alignment issues)

**Impact:**
Missing 2-8Ã— speedup for large arrays (â‰¥100K elements).

**Resolution:**
1. Re-enable `test_omp.py` in CI
2. Run extensive stability tests (1000+ iterations)
3. Validate no crashes or data corruption
4. Update documentation to reflect OpenMP as production-ready

---

### 8. Inconsistent Naming Conventions

**Problem:**
LUT naming lacks `TERNARY_` prefix, unlike other constants.

**Examples:**
```cpp
// Good: Core API constants
#define TERNARY_CORE_HAS_FUSION_POC 1

// Inconsistent: LUTs
TADD_LUT          // Should be: TERNARY_TADD_LUT
TNOT_LUT_SIMD     // Should be: TERNARY_TNOT_SIMD_LUT
DENSE243_EXTRACT_T0_LUT  // Good (has prefix)
```

**Impact:**
Potential symbol collisions in global namespace.

**Recommendation:**
Add `TERNARY_` prefix to all LUT constants for consistency and namespace hygiene.

---

## ðŸŸ¢ LOW Priority

### 9. Multi-Dimensional Array Support

**Problem:**
Current implementation only supports 1D arrays.

**Workaround:**
Users can reshape to 1D, process, then reshape back.

**Future Enhancement:**
Add n-dimensional array support with proper stride handling.

---

### 10. ARM NEON/SVE Support

**Problem:**
Only x86-64 AVX2 SIMD supported. ARM processors (mobile/edge) not optimized.

**Status:**
- âœ… CPU detection framework exists (`ternary_cpu_detect.h`)
- âœ… ISA abstraction layer designed (`ternary_simd_config.h`)
- âŒ No ARM NEON implementation yet

**Impact:**
Missed opportunity for mobile/edge AI deployment (Raspberry Pi, smartphones, etc.).

**Future Work:**
Implement NEON kernels (128-bit, 16 trits/op) for ARM v8.

---

### 11. GPU/TPU Acceleration (TritNet Phase 4)

**Problem:**
TritNet uses CPU matmul. GPU/TPU would accelerate training and inference.

**Current:**
- PyTorch CPU tensors only
- No CUDA support

**Future:**
- Phase 4 (TritNet roadmap): GPU acceleration
- Export ternary weights for custom CUDA kernels
- Batched inference on GPU

---

### 12. Profiler Framework Integration

**Problem:**
VTune profiler hooks exist but not integrated into main workflow.

**Status:**
```cpp
// src/core/profiling/ternary_profiler.h exists
// Hooks in ternary_simd_engine.cpp (lines 102-106, 165, 200, etc.)
// But: No documentation on how to use, no build script integration
```

**Resolution:**
Document profiler usage and add build flag for VTune integration.

---

## Optimization Plan Summary

### Phase 1: Critical Fixes (Week 1)
1. âœ… Fix fusion benchmark imports (update to use `ternary_simd_engine`)
2. âœ… Verify all build scripts work correctly
3. âœ… Document current module structure

### Phase 2: Code Quality (Week 2)
4. âœ… Refactor `process_binary_array` / `process_unary_array` duplication
5. âœ… Add consistent naming for LUTs
6. âœ… Improve code documentation

### Phase 3: Performance Optimizations (Week 3-4)
7. âœ… Implement proper ternary matrix multiplication SIMD kernels
8. âœ… Integrate tmatmul into TritNet
9. âœ… Benchmark and validate speedup

### Phase 4: Platform Expansion (Week 5-6)
10. âœ… Enable Linux/macOS CI
11. âœ… Validate cross-platform performance
12. âœ… Re-enable and validate OpenMP

### Phase 5: Future Enhancements (Post-MVP)
13. ARM NEON support
14. GPU/TPU acceleration
15. Multi-dimensional arrays
16. Profiler integration documentation

---

## Detailed File-by-File Analysis

### Build Scripts (`build/`)

| Script | Status | Issues | Priority |
|--------|--------|--------|----------|
| `build.py` | âœ… Working | None | N/A |
| `build_dense243.py` | âœ… Working | None | N/A |
| `build_pgo_unified.py` | âœ… Working | None | N/A |
| `build_pgo.py` | âœ… Legacy | Superseded by unified | ðŸŸ¢ LOW |
| `build_competitive.py` | âœ… Working | None | N/A |
| `build_all.py` | âœ… Working | Doesn't include dense243 | ðŸŸ¡ MEDIUM |
| `build_fusion.py` | âŒ Missing | Need to create OR fix benchmarks | ðŸ”´ CRITICAL |
| `clean_all.py` | âœ… Working | None | N/A |

**Recommendations:**
1. **Option A:** Update `build_all.py` to include `build_dense243.py`
2. **Option B:** Update benchmarks to import from `ternary_simd_engine` (no build_fusion needed)

---

### Benchmark Scripts (`benchmarks/`)

| Script | Module Imports | Status | Fix Needed |
|--------|----------------|--------|------------|
| `bench_phase0.py` | ternary_simd_engine | âœ… Working | None |
| `bench_dense243.py` | ternary_dense243_module | âœ… Working | None |
| `bench_competitive.py` | ternary_simd_engine | âœ… Working | None |
| `bench_fusion.py` | ternary_fusion_engine | âŒ Broken | Change to ternary_simd_engine |
| `bench_model_quantization.py` | ternary_simd_engine | âœ… Working | None |
| `bench_power_consumption.py` | ternary_simd_engine | âœ… Working | None |
| `micro/bench_fusion_*.py` (4) | ternary_fusion_engine | âŒ Broken | Change to ternary_simd_engine |
| `macro/bench_neural_layer.py` | ternary_fusion_engine | âŒ Broken | Change to ternary_simd_engine |
| `macro/bench_image_pipeline.py` | ternary_fusion_engine | âŒ Broken | Change to ternary_simd_engine |

**Total Broken:** 7 files
**Fix:** Global search-replace `ternary_fusion_engine` â†’ `ternary_simd_engine`

---

## Matmul Optimization Detailed Design

### Current State

**TritNet Forward Pass (ternary_layers.py:145-163):**
```python
def forward(self, input: torch.Tensor) -> torch.Tensor:
    if self.quantize_weights:
        weight_to_use = StraightThroughEstimator.apply(self.weight, self.threshold)
    else:
        weight_to_use = self.weight

    return F.linear(input, weight_to_use, self.bias)
    # â†‘ Uses PyTorch's full-precision matmul (BLAS/MKL)
```

### Proposed Optimization

**Ternary Matrix Multiplication SIMD Kernel:**

```cpp
// src/core/simd/ternary_matmul_kernels.h

// Compute C = A @ B where A, B contain ternary values {-1, 0, +1}
// A: [M, K] matrix (row-major)
// B: [K, N] matrix (row-major)
// C: [M, N] output (int32 accumulators)

template <bool Sanitize = true>
void tmatmul_simd(
    const uint8_t* A,  // M Ã— K matrix (2-bit trits)
    const uint8_t* B,  // K Ã— N matrix (2-bit trits)
    int32_t* C,        // M Ã— N output (32-bit accumulators)
    size_t M,
    size_t K,
    size_t N
) {
    // For each output element C[i,j]:
    for (size_t i = 0; i < M; ++i) {
        for (size_t j = 0; j < N; ++j) {
            // Dot product: Î£ A[i,k] * B[k,j]

            // SIMD: Process 32 elements at a time
            __m256i acc = _mm256_setzero_si256();
            size_t k = 0;

            for (; k + 32 <= K; k += 32) {
                __m256i a_vec = _mm256_loadu_si256((__m256i*)(A + i*K + k));
                __m256i b_vec = _mm256_loadu_si256((__m256i*)(B + k*N + j));

                // Ternary multiply + accumulate
                __m256i prod = tmul_simd<Sanitize>(a_vec, b_vec);

                // Convert to int32 and accumulate
                // (need horizontal sum across vector)
                // ... implementation details ...
            }

            // Scalar tail for remaining elements
            for (; k < K; ++k) {
                int8_t a_val = trit_to_int(A[i*K + k]);
                int8_t b_val = trit_to_int(B[k*N + j]);
                C[i*N + j] += a_val * b_val;
            }
        }
    }
}
```

**Key Optimizations:**
1. **SIMD Horizontal Sum:** Use `_mm256_hadd_epi32` to sum 32-wide vectors
2. **Sparse Zero Handling:** Skip k where A[i,k]=0 or B[k,j]=0
3. **Tiling:** Cache-friendly blocking for large matrices
4. **INT32 Accumulation:** Prevent overflow (243 ops max = 8-bit sufficient, but use int32 for safety)

**Expected Performance:**
- **Small matrices (K=32):** 5-10Ã— vs PyTorch FP32
- **Medium matrices (K=256):** 10-20Ã— vs PyTorch FP32
- **Large matrices (K=1024+):** 2-5Ã— (memory-bound)

---

## Measurement & Validation Checklist

Before marking any optimization as complete, validate:

### Performance Validation
- [ ] Baseline benchmark (3 runs, report median + variance)
- [ ] Optimized benchmark (3 runs, report median + variance)
- [ ] Speedup calculation with confidence intervals
- [ ] Regression check (<5% slowdown on any workload)
- [ ] Document validation date and platform

### Correctness Validation
- [ ] Unit tests pass (65/65 for Windows x64)
- [ ] Correctness tests for new features
- [ ] Regression tests for modified code
- [ ] Cross-platform validation (Linux/macOS)

### Code Quality
- [ ] No new compiler warnings
- [ ] Consistent naming conventions
- [ ] Documentation updated
- [ ] Code comments explain "why" not "what"

---

## Conclusion

The Ternary Engine codebase is **production-quality** with well-designed architecture and validated performance. The identified technical debt is **manageable** and primarily consists of:

1. Missing benchmark module (fusion) - easily fixed via import updates
2. Code duplication - refactoring opportunity for cleaner code
3. Matmul optimization - significant performance gain potential
4. Platform validation - extend testing to Linux/macOS

**Recommended immediate actions:**
1. Fix fusion benchmark imports (1 hour)
2. Refactor process templates (2 hours)
3. Implement ternary matmul SIMD kernel (1-2 weeks)
4. Enable cross-platform CI (3-4 days)

**Total effort:** ~2-3 weeks for Phase 1-2, additional 2-4 weeks for Phase 3-4.
