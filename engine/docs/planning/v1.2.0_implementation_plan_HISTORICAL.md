# Ternary Engine v1.2.0 Implementation Plan

**Version:** v1.2.0 "Encoding-Aware Pipeline"
**Goal:** Add Sixtet/Octet encoding layers and backend interface while maintaining portability
**Current:** v1.1.0 "ktr"
**Target Performance:** 30 Gops/s sustained, 45 Gops/s peak

---

## Overview

v1.2.0 introduces a layered architecture separating:
1. **Encoding layers** (Sixtet, Octet, Dense243) - I/O and cache optimization
2. **Scalar core** (portable reference) - mathematical correctness
3. **Backend interface** (TCBI) - clean separation of SIMD backends
4. **Safe SIMD optimizations** - portable performance improvements

**Key Principle:** SIMD backends NEVER operate on packed formats directly. They operate on canonical 2-bit internal format only.

---

## Phase 1: Encoding Layer Implementation

### **Step 1.1: Sixtet Pack/Unpack**

**Purpose:** Pack 3 trits into 6 bits for cache optimization

**Implementation:**
- Create `src/core/packing/sixtet_pack.h`
- Canonical 64-state LUT (3^3 = 27 valid states)
- Branchless pack/unpack functions
- Support for alignment (32-byte boundaries)

**Interface:**
```cpp
// Pack 3 trits {-1, 0, +1} → 6-bit value (0-63)
uint8_t sixtet_pack(int8_t t0, int8_t t1, int8_t t2);

// Unpack 6-bit value → 3 trits
void sixtet_unpack(uint8_t packed, int8_t* t0, int8_t* t1, int8_t* t2);

// Batch operations (AVX2-friendly)
void sixtet_pack_array(const uint8_t* trits, uint8_t* packed, size_t n_trits);
void sixtet_unpack_array(const uint8_t* packed, uint8_t* trits, size_t n_trits);
```

**LUT Generation:**
```cpp
// Compile-time LUT generation
constexpr uint8_t sixtet_encode_table[3][3][3] = {
    // [t0=-1][t1=-1][t2=-1] = 0
    // [t0=-1][t1=-1][t2= 0] = 1
    // ...
};

constexpr struct {
    int8_t t0, t1, t2;
} sixtet_decode_table[64] = {
    {-1, -1, -1},  // index 0
    {-1, -1,  0},  // index 1
    // ...
};
```

**Optimization:**
- Use `_mm256_shuffle_epi8` for batch decode (32 sixtet values → 96 trits)
- Cache-align LUTs
- SIMD gather for non-contiguous access

**Validation:**
- Round-trip test: `unpack(pack(t)) == t`
- All 27 valid states tested
- Performance: <1 cycle per 3-trit group

---

### **Step 1.2: Octet Pack/Unpack**

**Purpose:** Pack 2 trits into 3 bits for byte-aligned I/O

**Implementation:**
- Create `src/core/packing/octet_pack.h`
- 9-state mapping (3^2) into 3 bits (8 states + 1 canonical fix)
- Sentinel detection for invalid states
- Error-resilient decoding

**Interface:**
```cpp
// Pack 2 trits → 3-bit value (0-7)
uint8_t octet_pack(int8_t t0, int8_t t1);

// Unpack 3-bit value → 2 trits (with error detection)
bool octet_unpack(uint8_t packed, int8_t* t0, int8_t* t1);

// Batch operations
void octet_pack_array(const uint8_t* trits, uint8_t* packed, size_t n_trits);
bool octet_unpack_array(const uint8_t* packed, uint8_t* trits, size_t n_trits);
```

**Canonical Mapping:**
```
State | t0  | t1  | 3-bit value | Notes
------|-----|-----|-------------|-------
  0   | -1  | -1  |   0b000     | Valid
  1   | -1  |  0  |   0b001     | Valid
  2   | -1  | +1  |   0b010     | Valid
  3   |  0  | -1  |   0b011     | Valid
  4   |  0  |  0  |   0b100     | Valid
  5   |  0  | +1  |   0b101     | Valid
  6   | +1  | -1  |   0b110     | Valid
  7   | +1  |  0  |   0b111     | Valid
  8   | +1  | +1  |   0b111*    | Sentinel → canonical map
```

**Error Handling:**
- Sentinel state (0b111) maps to (+1, +1) via canonical LUT
- Invalid states return error flag
- Fallback to safe default

---

### **Step 1.3: Dense243 Integration**

**Purpose:** Maintain existing 5 trits/byte encoding

**Implementation:**
- Keep existing `src/engine/dense243/ternary_dense243.h`
- Integrate with new encoding layer API
- Ensure consistency across encodings

**No Changes Required:**
- Dense243 already validated
- 0.25 ns pack, 0.91 ns unpack (measured)
- Used for storage/network only

---

### **Step 1.4: Unified Encoding API**

**Purpose:** Single interface for all encodings

**Implementation:**
- Create `src/core/packing/pack.h` (unified header)
- Runtime encoding selection
- Automatic format detection

**Interface:**
```cpp
typedef enum {
    ENCODING_RAW_2BIT,     // Internal format (1 byte per trit)
    ENCODING_SIXTET,       // 3 trits per byte (6 bits used)
    ENCODING_OCTET,        // 2 trits per 3 bits
    ENCODING_DENSE243      // 5 trits per byte
} TernaryEncoding;

// Generic pack/unpack with encoding selection
void ternary_pack(const uint8_t* trits, uint8_t* packed,
                  size_t n_trits, TernaryEncoding encoding);

void ternary_unpack(const uint8_t* packed, uint8_t* trits,
                    size_t n_trits, TernaryEncoding encoding);
```

**Format Detection:**
```cpp
// Auto-detect encoding from metadata byte
TernaryEncoding ternary_detect_encoding(const uint8_t* data);

// Encoding header byte: [version:2][encoding:3][reserved:3]
// Example: 0b00_001_000 = version 0, SIXTET encoding
```

---

## Phase 2: Backend Interface

### **Step 2.1: Backend Interface Definition**

**Purpose:** Clean separation between scalar core and SIMD backends

**Implementation:**
- Create `src/core/backend/backend_interface.h`
- Define virtual function table for ternary operations
- Backend registration system

**Interface:**
```cpp
// Backend capability flags
typedef enum {
    BACKEND_CAP_SCALAR     = 0x01,
    BACKEND_CAP_SIMD_128   = 0x02,
    BACKEND_CAP_SIMD_256   = 0x04,
    BACKEND_CAP_SIMD_512   = 0x08,
    BACKEND_CAP_GPU        = 0x10,
    BACKEND_CAP_FUSION     = 0x20
} BackendCapabilities;

// Backend descriptor
typedef struct {
    const char* name;
    const char* description;
    BackendCapabilities capabilities;

    // Detection function
    bool (*detect)(void);

    // Unary operations
    void (*tnot)(const uint8_t* a, uint8_t* out, size_t n);

    // Binary operations
    void (*tadd)(const uint8_t* a, const uint8_t* b, uint8_t* out, size_t n);
    void (*tmul)(const uint8_t* a, const uint8_t* b, uint8_t* out, size_t n);
    void (*tmin)(const uint8_t* a, const uint8_t* b, uint8_t* out, size_t n);
    void (*tmax)(const uint8_t* a, const uint8_t* b, uint8_t* out, size_t n);

    // Fused operations (optional, NULL if not supported)
    void (*tnot_tadd)(const uint8_t* a, const uint8_t* b, uint8_t* out, size_t n);

} TernaryBackend;
```

**Backend Registry:**
```cpp
// Register a backend (called at module init)
void ternary_register_backend(const TernaryBackend* backend);

// Get best available backend
const TernaryBackend* ternary_get_backend(void);

// Override backend selection
void ternary_set_backend(const char* name);

// List available backends
void ternary_list_backends(void);
```

---

### **Step 2.2: Scalar Backend**

**Purpose:** Portable reference implementation

**Implementation:**
- Create `src/core/backend/backend_scalar.c`
- Pure C99 implementation
- No SIMD, no platform-specific code
- Reference for correctness testing

**Operations:**
```c
// Scalar reference (existing LUT-based logic)
void scalar_tadd(const uint8_t* a, const uint8_t* b, uint8_t* out, size_t n) {
    for (size_t i = 0; i < n; i++) {
        out[i] = TADD_LUT[a[i]][b[i]];
    }
}

void scalar_tmul(const uint8_t* a, const uint8_t* b, uint8_t* out, size_t n) {
    for (size_t i = 0; i < n; i++) {
        out[i] = TMUL_LUT[a[i]][b[i]];
    }
}

// ... similar for tmin, tmax, tnot
```

**Backend Descriptor:**
```c
static const TernaryBackend scalar_backend = {
    .name = "scalar",
    .description = "Portable C99 reference implementation",
    .capabilities = BACKEND_CAP_SCALAR,
    .detect = scalar_detect,  // Always returns true
    .tnot = scalar_tnot,
    .tadd = scalar_tadd,
    .tmul = scalar_tmul,
    .tmin = scalar_tmin,
    .tmax = scalar_tmax,
    .tnot_tadd = NULL  // No fusion support
};
```

---

### **Step 2.3: AVX2 Backend (v1)**

**Purpose:** Existing SIMD implementation as backend

**Implementation:**
- Refactor `src/engine/bindings_core_ops.cpp` kernels
- Move SIMD functions to `src/core/backend/backend_avx2_v1.cpp`
- Maintain current performance (28-35 Gops/s)

**Detection:**
```cpp
bool avx2_v1_detect(void) {
    // Use existing CPU detection
    #ifdef __AVX2__
    return has_avx2();
    #else
    return false;
    #endif
}
```

**Backend Descriptor:**
```cpp
static const TernaryBackend avx2_v1_backend = {
    .name = "avx2_v1",
    .description = "AVX2 SIMD (v1.0 kernels)",
    .capabilities = BACKEND_CAP_SIMD_256 | BACKEND_CAP_FUSION,
    .detect = avx2_v1_detect,
    .tnot = avx2_v1_tnot,
    .tadd = avx2_v1_tadd,
    .tmul = avx2_v1_tmul,
    .tmin = avx2_v1_tmin,
    .tmax = avx2_v1_tmax,
    .tnot_tadd = avx2_v1_tnot_tadd  // Fusion support
};
```

**No Performance Regression:**
- Keep existing `_mm256_shuffle_epi8` implementation
- Maintain OpenMP parallelization
- Preserve fusion operations

---

### **Step 2.4: Backend Dispatcher**

**Purpose:** Automatic backend selection at runtime

**Implementation:**
- Create `src/core/backend/backend_registry.c`
- Backend priority system (AVX2 > Scalar)
- Thread-safe selection

**Registry:**
```cpp
#define MAX_BACKENDS 16

static struct {
    const TernaryBackend* backends[MAX_BACKENDS];
    size_t count;
    const TernaryBackend* active;
    bool manual_override;
} backend_registry = {0};

void ternary_init_backends(void) {
    // Register backends in priority order
    ternary_register_backend(&avx2_v1_backend);
    ternary_register_backend(&scalar_backend);

    // Select best available
    backend_registry.active = ternary_auto_select_backend();
}

const TernaryBackend* ternary_auto_select_backend(void) {
    // Try backends in order until one succeeds detection
    for (size_t i = 0; i < backend_registry.count; i++) {
        if (backend_registry.backends[i]->detect()) {
            return backend_registry.backends[i];
        }
    }
    return NULL;  // Should never happen (scalar always available)
}
```

**Python Binding:**
```python
import ternary_simd_engine as tc

# List available backends
print(tc.list_backends())
# Output: ['avx2_v1', 'scalar']

# Get current backend
print(tc.get_backend())
# Output: 'avx2_v1'

# Override backend selection
tc.set_backend('scalar')
```

---

## Phase 3: Safe SIMD Optimizations

### **Step 3.1: Canonical Index LUT**

**Purpose:** Eliminate `(a << 2) | b` integer arithmetic

**Implementation:**
- Create `src/core/simd/ternary_canonical_index.h`
- Pre-compute all index mappings at compile time
- 16-byte or 256-byte LUT options

**Concept:**
```cpp
// Instead of: index = (a << 2) | b
// Use: index = CANONICAL_INDEX[a][b]

// 16-byte version (4x4 for 2-bit trits)
constexpr uint8_t CANONICAL_INDEX_16[4][4] = {
    {0,  1,  2,  3},   // a = 0b00 (-1)
    {4,  5,  6,  7},   // a = 0b01 (0)
    {8,  9, 10, 11},   // a = 0b10 (+1)
    {12, 13, 14, 15}   // a = 0b11 (invalid/sentinel)
};

// 256-byte version (expanded for direct lookup)
constexpr uint8_t CANONICAL_INDEX_256[256] = {
    // Pre-expanded for all byte combinations
    // Allows: index = CANONICAL_INDEX_256[byte_value]
};
```

**SIMD Usage:**
```cpp
// Load canonical index LUT into register
__m256i canon_lut = _mm256_load_si256((__m256i*)CANONICAL_INDEX_256);

// Direct lookup (no shift/OR)
__m256i indices = _mm256_shuffle_epi8(canon_lut, input_bytes);

// Apply to operation LUT
__m256i result = _mm256_shuffle_epi8(operation_lut, indices);
```

**Benefit:**
- Eliminates 2 instructions (shift + OR)
- Removes dependency chain
- Reduces INT ALU pressure
- Expected: +12-18% throughput

---

### **Step 3.2: LUT-256B Expansion**

**Purpose:** Use 256-byte lookup tables for direct indexing

**Implementation:**
- Create `src/core/algebra/ternary_lut_256.h`
- Expand 16-byte LUTs to 256 bytes
- Cache-align for L1 residency

**Concept:**
```cpp
// Original: 16-byte LUT (4-bit index)
constexpr uint8_t TADD_LUT_16[16] = {
    /* 0=(-1,-1) */ ZERO,
    /* 1=(-1, 0) */ MINUS_ONE,
    /* 2=(-1,+1) */ ZERO,
    /* 3=( 0,-1) */ MINUS_ONE,
    // ...
};

// Expanded: 256-byte LUT (8-bit index)
alignas(256) constexpr uint8_t TADD_LUT_256[256] = {
    // Copy pattern from LUT_16 across all 256 entries
    // Allows byte-level direct indexing
    // Indices 0-15 are valid, 16-255 are duplicates or sentinels
};
```

**Generation:**
```cpp
// Compile-time LUT expansion
template<size_t N>
constexpr auto expand_lut_256(const uint8_t (&lut16)[16]) {
    std::array<uint8_t, 256> lut256{};
    for (size_t i = 0; i < 256; i++) {
        lut256[i] = lut16[i & 0x0F];  // Wrap to 16-entry pattern
    }
    return lut256;
}

constexpr auto TADD_LUT_256 = expand_lut_256(TADD_LUT_16);
```

**Benefit:**
- Direct byte indexing (no masking)
- Fits in 4 L1 cache lines
- Predictable memory access
- Expected: +25-35% throughput

---

### **Step 3.3: Dual-Shuffle XOR**

**Purpose:** Use two parallel shuffles + XOR to saturate multiple execution ports

**Implementation:**
- Create `src/core/simd/ternary_dual_shuffle.h`
- Split LUTs into HI and LO planes
- XOR-based result combination

**Concept:**
```cpp
// Instead of: result = shuffle(LUT, index)
// Use: result = shuffle(LUT_LO, a) XOR shuffle(LUT_HI, b)

// Split operation into two independent shuffles
__m256i lo = _mm256_shuffle_epi8(LUT_LO, input_a);  // Port 5
__m256i hi = _mm256_shuffle_epi8(LUT_HI, input_b);  // Port 5 (next cycle)
__m256i result = _mm256_xor_si256(lo, hi);          // Port 0 (parallel)
```

**LUT Splitting:**
```cpp
// Decompose operation LUT into XOR-compatible planes
// For tadd(a, b):
//   LUT_LO[a] XOR LUT_HI[b] = TADD_LUT[a][b]

// Example generation:
void generate_dual_shuffle_luts(
    const uint8_t* operation_lut,
    uint8_t* lut_lo_out,
    uint8_t* lut_hi_out
) {
    // Use Karnaugh map or exhaustive search
    // to find LO and HI such that:
    // operation_lut[i] = lut_lo[i & 0xF] ^ lut_hi[(i >> 4) & 0xF]
}
```

**Port Utilization:**
- **Shuffle 1:** Port 5 (Intel) or Port 3 (AMD)
- **Shuffle 2:** Port 5 (Intel) or Port 3 (AMD) - next cycle
- **XOR:** Port 0 (Intel/AMD) - runs in parallel
- **Result:** 2 shuffles + free XOR = 2 cycle latency, 1 cycle throughput

**Benefit:**
- Doubles shuffle throughput
- Removes shift/OR dependency
- Better port distribution
- Expected: 1.5-1.7× speedup on Zen CPUs

---

### **Step 3.4: Selective Interleaving**

**Purpose:** Interleave operations to hide latency

**Implementation:**
- Unroll loops by 2× or 4×
- Process multiple vectors in flight
- Hide shuffle latency with independent operations

**Concept:**
```cpp
// Instead of: for (i = 0; i < n; i += 32) { process_one_vector(i); }
// Use: for (i = 0; i < n; i += 64) { process_two_vectors(i, i+32); }

void tadd_interleaved(const uint8_t* a, const uint8_t* b, uint8_t* out, size_t n) {
    for (size_t i = 0; i + 64 <= n; i += 64) {
        // Load 2 vectors
        __m256i a1 = _mm256_load_si256((__m256i*)(a + i));
        __m256i b1 = _mm256_load_si256((__m256i*)(b + i));
        __m256i a2 = _mm256_load_si256((__m256i*)(a + i + 32));
        __m256i b2 = _mm256_load_si256((__m256i*)(b + i + 32));

        // Process vector 1
        __m256i lo1 = _mm256_shuffle_epi8(LUT_LO, a1);
        __m256i hi1 = _mm256_shuffle_epi8(LUT_HI, b1);

        // Process vector 2 (hides latency of vector 1)
        __m256i lo2 = _mm256_shuffle_epi8(LUT_LO, a2);
        __m256i hi2 = _mm256_shuffle_epi8(LUT_HI, b2);

        // Combine
        __m256i out1 = _mm256_xor_si256(lo1, hi1);
        __m256i out2 = _mm256_xor_si256(lo2, hi2);

        // Store
        _mm256_store_si256((__m256i*)(out + i), out1);
        _mm256_store_si256((__m256i*)(out + i + 32), out2);
    }
    // Handle remainder
}
```

**Benefit:**
- Hides shuffle latency
- Better instruction-level parallelism
- More stable performance
- Expected: +10-15% throughput

---

## Phase 4: Pipeline Integration

### **Step 4.1: Sixtet Strip-Mining**

**Purpose:** Process large arrays in Sixtet-encoded chunks for cache efficiency

**Implementation:**
- Strip-mine large arrays into Sixtet-sized blocks
- Decode to 2-bit, process with SIMD, re-encode to Sixtet

**Pipeline:**
```
Input: Sixtet array (1M trits = 333K bytes)
  ↓
Decode strip (96 trits → 96 bytes) [stays in L1]
  ↓
SIMD process (32 bytes × 3 iterations) [hot]
  ↓
Encode strip (96 bytes → 32 bytes Sixtet)
  ↓
Output: Sixtet array
```

**Strip Size Selection:**
```cpp
// Optimal strip size for L1 cache (32KB)
// Trits in 2-bit: 96 trits = 96 bytes
// Trits in Sixtet: 96 trits = 32 bytes (3× compression)
#define SIXTET_STRIP_SIZE 96

void tadd_sixtet_striped(
    const uint8_t* a_sixtet,
    const uint8_t* b_sixtet,
    uint8_t* out_sixtet,
    size_t n_trits
) {
    uint8_t strip_a[SIXTET_STRIP_SIZE];
    uint8_t strip_b[SIXTET_STRIP_SIZE];
    uint8_t strip_out[SIXTET_STRIP_SIZE];

    for (size_t i = 0; i < n_trits; i += SIXTET_STRIP_SIZE) {
        size_t strip_len = min(SIXTET_STRIP_SIZE, n_trits - i);

        // Decode Sixtet → 2-bit
        sixtet_unpack_array(a_sixtet + i/3, strip_a, strip_len);
        sixtet_unpack_array(b_sixtet + i/3, strip_b, strip_len);

        // SIMD process (stays in L1)
        backend->tadd(strip_a, strip_b, strip_out, strip_len);

        // Encode 2-bit → Sixtet
        sixtet_pack_array(strip_out, out_sixtet + i/3, strip_len);
    }
}
```

**Benefit:**
- 3× reduction in memory traffic
- Better L1 cache utilization
- More stable performance under load
- Expected: +15-25% sustained throughput

---

### **Step 4.2: Hybrid Format Pipeline**

**Purpose:** Automatic format conversion based on operation context

**Implementation:**
- Input detection (Sixtet/Octet/Dense243/Raw)
- Automatic decode to 2-bit internal format
- SIMD processing
- Automatic encode to output format

**Pipeline Manager:**
```cpp
void ternary_operation(
    const void* input_a,
    const void* input_b,
    void* output,
    size_t n_trits,
    TernaryEncoding input_encoding,
    TernaryEncoding output_encoding,
    TernaryOperation operation
) {
    // Allocate internal 2-bit buffers
    uint8_t* a_2bit = allocate_aligned(n_trits, 32);
    uint8_t* b_2bit = allocate_aligned(n_trits, 32);
    uint8_t* out_2bit = allocate_aligned(n_trits, 32);

    // Decode to internal format
    ternary_unpack(input_a, a_2bit, n_trits, input_encoding);
    ternary_unpack(input_b, b_2bit, n_trits, input_encoding);

    // Execute operation using backend
    const TernaryBackend* backend = ternary_get_backend();
    switch (operation) {
        case OP_TADD: backend->tadd(a_2bit, b_2bit, out_2bit, n_trits); break;
        case OP_TMUL: backend->tmul(a_2bit, b_2bit, out_2bit, n_trits); break;
        // ...
    }

    // Encode to output format
    ternary_pack(out_2bit, output, n_trits, output_encoding);

    // Free buffers
    free_aligned(a_2bit);
    free_aligned(b_2bit);
    free_aligned(out_2bit);
}
```

**Optimization:**
- Cache decoded buffers for repeated operations
- Skip encode/decode for internal pipelines
- Use Sixtet for intermediate results

---

## Phase 5: Testing & Validation

### **Step 5.1: Encoding Round-Trip Tests**

**Tests:**
- Sixtet pack/unpack correctness (all 27 states)
- Octet pack/unpack correctness (all 9 states)
- Dense243 consistency
- Format conversion accuracy

**Files:**
- `tests/cpp/test_sixtet.cpp`
- `tests/cpp/test_octet.cpp`
- `tests/cpp/test_encoding_roundtrip.cpp`

**Test Cases:**
```cpp
TEST(Sixtet, PackUnpack) {
    for (int t0 = -1; t0 <= 1; t0++) {
        for (int t1 = -1; t1 <= 1; t1++) {
            for (int t2 = -1; t2 <= 1; t2++) {
                uint8_t packed = sixtet_pack(t0, t1, t2);
                int8_t u0, u1, u2;
                sixtet_unpack(packed, &u0, &u1, &u2);
                ASSERT_EQ(t0, u0);
                ASSERT_EQ(t1, u1);
                ASSERT_EQ(t2, u2);
            }
        }
    }
}

TEST(Sixtet, ArrayRoundTrip) {
    constexpr size_t N = 1024;
    uint8_t trits[N], packed[N/3+1], unpacked[N];

    // Generate random trits
    for (size_t i = 0; i < N; i++) {
        trits[i] = (rand() % 3) - 1;  // {-1, 0, +1}
    }

    // Pack and unpack
    sixtet_pack_array(trits, packed, N);
    sixtet_unpack_array(packed, unpacked, N);

    // Verify
    for (size_t i = 0; i < N; i++) {
        ASSERT_EQ(trits[i], unpacked[i]);
    }
}
```

---

### **Step 5.2: Backend Equivalence Tests**

**Tests:**
- Scalar vs AVX2 output equivalence
- All operations (tadd, tmul, tmin, tmax, tnot)
- Fusion operation consistency
- Edge cases (size=0, size=1, non-aligned)

**Files:**
- `tests/python/test_backends.py`
- `tests/cpp/test_backend_equivalence.cpp`

**Test Cases:**
```python
def test_backend_equivalence():
    sizes = [0, 1, 31, 32, 33, 100, 1000, 10000]

    for size in sizes:
        a = np.random.randint(0, 3, size, dtype=np.uint8)
        b = np.random.randint(0, 3, size, dtype=np.uint8)

        # Test with scalar backend
        tc.set_backend('scalar')
        result_scalar = tc.tadd(a, b)

        # Test with AVX2 backend
        tc.set_backend('avx2_v1')
        result_avx2 = tc.tadd(a, b)

        # Results must be identical
        assert np.array_equal(result_scalar, result_avx2)
```

---

### **Step 5.3: Performance Benchmarks**

**Benchmarks:**
- Encoding overhead (pack/unpack cost)
- Backend dispatch overhead
- Sixtet strip-mining gains
- LUT-256B speedup
- Dual-shuffle XOR speedup

**Files:**
- `benchmarks/bench_encoding.py`
- `benchmarks/bench_backends.py`
- `benchmarks/bench_v1_2_improvements.py`

**Metrics:**
```python
# Encoding overhead
def bench_encoding_overhead():
    data = np.random.randint(0, 3, 1_000_000, dtype=np.uint8)

    # Measure pack time
    t0 = time.perf_counter_ns()
    packed = sixtet_pack_array(data)
    t1 = time.perf_counter_ns()
    pack_time = t1 - t0

    # Measure unpack time
    t0 = time.perf_counter_ns()
    unpacked = sixtet_unpack_array(packed)
    t1 = time.perf_counter_ns()
    unpack_time = t1 - t0

    print(f"Pack:   {pack_time / 1e6:.2f} ms ({pack_time / 1_000_000:.2f} ns/trit)")
    print(f"Unpack: {unpack_time / 1e6:.2f} ms ({unpack_time / 1_000_000:.2f} ns/trit)")
```

**Success Criteria:**
- No performance regression vs v1.1.0
- +15% sustained throughput from Sixtet
- +20% peak throughput from SIMD optimizations
- Encoding overhead <5% of total operation time

---

## Phase 6: Documentation

### **Step 6.1: API Documentation**

**Files:**
- `docs/api/encoding-layer.md` - Sixtet/Octet/Dense243 usage
- `docs/api/backend-interface.md` - Backend selection and registration
- `docs/api/v1.2-migration.md` - Upgrading from v1.1.0

**Content:**
- Usage examples
- Performance characteristics
- Best practices
- Common pitfalls

---

### **Step 6.2: Architecture Documentation**

**Files:**
- `docs/architecture/layered-design.md` - Three-layer architecture
- `docs/architecture/encoding-layer.md` - Encoding format details
- `docs/architecture/backend-system.md` - Backend dispatch mechanism
- `docs/performance/simd-optimizations.md` - Canonical index, LUT-256, dual-shuffle

**Diagrams:**
- Data flow pipeline
- Backend selection flowchart
- Cache hierarchy optimization
- SIMD instruction scheduling

---

### **Step 6.3: Performance Guide**

**Files:**
- `docs/performance/tuning-guide.md`
- `docs/performance/encoding-selection.md`
- `docs/performance/backend-selection.md`

**Topics:**
- When to use Sixtet vs Octet vs Dense243
- Backend selection strategies
- Cache optimization techniques
- Performance measurement methodology

---

## Phase 7: Platform Validation

### **Step 7.1: Windows x64 Validation**

**Platform:** Primary development platform
- MSVC compiler
- AVX2 support required
- OpenMP enabled

**Validation:**
- All tests passing (100+ test cases)
- Benchmarks meeting targets
- No regressions from v1.1.0

---

### **Step 7.2: Linux x64 Validation**

**Platform:** GCC/Clang builds
- Ubuntu 20.04+, Debian 11+
- GCC 9+ or Clang 10+
- AVX2 detection

**Validation:**
- Build succeeds
- Tests passing
- Performance comparable to Windows

---

### **Step 7.3: macOS ARM64 Validation**

**Platform:** Apple Silicon (M1/M2/M3)
- Clang with Apple extensions
- NEON backend (future work)
- Scalar backend (immediate)

**Validation:**
- Build succeeds with scalar backend
- Tests passing
- NEON backend placeholder

---

## Success Criteria

### **Functional Requirements** ✅
- All encoding layers implemented and tested
- Backend interface working with 2+ backends
- Scalar backend as reference
- AVX2 backend maintaining performance
- Round-trip encoding correctness
- Backend equivalence verified

### **Performance Requirements** ✅
- No regression from v1.1.0 (28 Gops/s sustained)
- +15% sustained throughput from Sixtet optimization
- +20% peak throughput from SIMD improvements
- Target: 30 Gops/s sustained, 45 Gops/s peak
- Encoding overhead <5% of operation time
- Backend dispatch overhead <1% of operation time

### **Platform Requirements** ✅
- Windows x64 fully validated
- Linux x64 builds and tests pass
- macOS ARM64 builds (scalar backend)
- No ISA lock-in (architecture portable)

### **Quality Requirements** ✅
- 100+ tests passing
- Zero known correctness bugs
- Comprehensive documentation
- API backwards compatible (external interface unchanged)

---

## Implementation Order

**Sequential Dependencies:**
1. Encoding Layer (Phase 1) - no dependencies
2. Backend Interface (Phase 2) - depends on Phase 1
3. SIMD Optimizations (Phase 3) - depends on Phase 2
4. Pipeline Integration (Phase 4) - depends on Phases 1-3
5. Testing (Phase 5) - depends on Phases 1-4
6. Documentation (Phase 6) - depends on Phases 1-5
7. Platform Validation (Phase 7) - depends on all phases

**Parallelizable Work:**
- Sixtet and Octet can be developed in parallel
- Backend interface and scalar backend can be done concurrently
- Documentation can start early with spec documents

---

## Risk Mitigation

### **Risk: Performance Regression**
- Mitigation: Comprehensive benchmarking at each step
- Fallback: Keep v1.1.0 backend as "avx2_v1"
- Validation: Performance gate in CI

### **Risk: Platform Compatibility**
- Mitigation: Multi-platform CI (Windows, Linux, macOS)
- Fallback: Scalar backend always available
- Validation: Test on diverse hardware

### **Risk: API Breaking Changes**
- Mitigation: Backend system is internal implementation detail
- Fallback: Python API unchanged
- Validation: External API compatibility tests

### **Risk: Complexity**
- Mitigation: Clear separation of concerns
- Documentation: Comprehensive architecture docs
- Code review: Peer review for all phases

---

## Future Work (Post v1.2.0)

**v1.3.0:**
- Ternary IR (intermediate representation)
- Full backend dispatcher with cost model
- ARM NEON backend

**v2.0:**
- Advanced AVX2 optimizations
- Multi-LUT fusion
- Permute pipeline

**v2.5:**
- AVX-512 backend
- ARM SVE backend
- RISC-V Vector backend

**v3.0:**
- TritNet GPU acceleration
- BitNet integration
- Model quantization

---

## Appendix: Code Structure

```
src/
├── core/
│   ├── algebra/
│   │   ├── ternary_algebra.h           # Existing
│   │   ├── ternary_lut_gen.h           # Existing
│   │   └── ternary_lut_256.h           # NEW (Phase 3.2)
│   ├── backend/
│   │   ├── backend_interface.h         # NEW (Phase 2.1)
│   │   ├── backend_registry.c          # NEW (Phase 2.4)
│   │   ├── backend_scalar.c            # NEW (Phase 2.2)
│   │   └── backend_avx2_v1.cpp         # NEW (Phase 2.3)
│   ├── packing/
│   │   ├── pack.h                      # NEW (Phase 1.4)
│   │   ├── sixtet_pack.h               # NEW (Phase 1.1)
│   │   ├── octet_pack.h                # NEW (Phase 1.2)
│   │   └── dense243_integration.h      # NEW (Phase 1.3)
│   └── simd/
│       ├── ternary_simd_kernels.h      # Existing
│       ├── ternary_canonical_index.h   # NEW (Phase 3.1)
│       ├── ternary_dual_shuffle.h      # NEW (Phase 3.3)
│       └── ternary_cpu_detect.h        # Existing
├── engine/
│   ├── bindings_core_ops.cpp           # Refactored (Phase 2.3)
│   └── dense243/
│       └── ternary_dense243.h          # Existing
tests/
├── cpp/
│   ├── test_sixtet.cpp                 # NEW (Phase 5.1)
│   ├── test_octet.cpp                  # NEW (Phase 5.1)
│   ├── test_encoding_roundtrip.cpp     # NEW (Phase 5.1)
│   └── test_backend_equivalence.cpp    # NEW (Phase 5.2)
└── python/
    └── test_backends.py                # NEW (Phase 5.2)
benchmarks/
├── bench_encoding.py                   # NEW (Phase 5.3)
├── bench_backends.py                   # NEW (Phase 5.3)
└── bench_v1_2_improvements.py          # NEW (Phase 5.3)
docs/
├── ROADMAP.md                          # Updated
├── api/
│   ├── encoding-layer.md               # NEW (Phase 6.1)
│   ├── backend-interface.md            # NEW (Phase 6.1)
│   └── v1.2-migration.md               # NEW (Phase 6.1)
├── architecture/
│   ├── layered-design.md               # NEW (Phase 6.2)
│   ├── encoding-layer.md               # NEW (Phase 6.2)
│   └── backend-system.md               # NEW (Phase 6.2)
└── performance/
    ├── tuning-guide.md                 # NEW (Phase 6.3)
    └── simd-optimizations.md           # NEW (Phase 6.3)
```

---

**Document Version:** 1.0
**Last Updated:** 2025-11-24
**Status:** Ready for Implementation
