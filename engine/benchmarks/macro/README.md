# Macro Benchmarks: Real-World Application Performance

**Purpose:** Measure actual end-to-end speedup in realistic workloads, not isolated micro-kernels.

---

## Why Macro Benchmarks?

**Micro benchmarks** (in `benchmarks/micro/`) measure isolated operation speedup:
- `fused_tnot_tadd` vs `tnot(tadd)` on fresh arrays
- Result: **1.75-1.78√ó speedup** (validated)

**But real applications don't work this way:**
- Multiple operations in sequence
- Python overhead (loops, function calls)
- Memory allocation/deallocation
- Non-ternary operations (I/O, preprocessing)
- Mixed operation sizes

**Macro benchmarks measure what users actually care about:** End-to-end application speedup.

---

## Benchmark Scenarios

### 1. Image Processing Pipeline (`bench_image_pipeline.py`)

**Scenario:** Ternary image convolution + filters

```python
def process_image(img):
    # Convolution (many tmul operations)
    conv = convolve_ternary(img, kernel)

    # Bias add
    biased = ternary.tadd(conv, bias)

    # Activation (tnot)
    activated = ternary.tnot(biased)  # ‚Üê Fusion opportunity!

    # More filters...
    return result
```

**Fusion coverage:** ~20-30% of operations (tadd+tnot step)

**Expected macro speedup:** 1.10-1.15√ó (10-15% improvement)

---

### 2. Neural Network Layer (`bench_neural_layer.py`)

**Scenario:** Ternary matrix multiply + activation

```python
def ternary_layer_forward(x, W, b):
    # Matrix multiply (many operations)
    z = ternary_matmul(x, W)

    # Add bias + activation
    z = ternary.tadd(z, b)
    a = ternary.tnot(z)  # ‚Üê Fusion opportunity!

    return a
```

**Fusion coverage:** ~15-25% of operations

**Expected macro speedup:** 1.08-1.12√ó (8-12% improvement)

---

### 3. Batch Data Processing (`bench_batch_processing.py`)

**Scenario:** Large dataset transformations

```python
def process_batch(data_a, data_b):
    # Multiple ternary operations
    temp1 = ternary.tmul(data_a, data_b)
    temp2 = ternary.tmax(temp1, data_a)
    result = ternary.tnot(temp2)  # ‚Üê Could fuse tnot+tmax if we had it
    return result
```

**Fusion coverage:** Depends on operation patterns

**Expected macro speedup:** 1.05-1.20√ó (5-20% improvement)

---

### 4. Mixed Workload (`bench_mixed_workload.py`)

**Scenario:** Realistic combination with Python overhead

```python
for batch in dataloader:  # I/O overhead
    x = quantize_to_ternary(batch)  # Conversion

    # Ternary operations (some fusible)
    y = ternary_pipeline(x)

    # Post-processing
    result = dequantize(y)
    save(result)  # I/O
```

**Fusion coverage:** ~10-20% of total runtime

**Expected macro speedup:** 1.02-1.08√ó (2-8% improvement)

---

## Validation Criteria

### Success Metrics

**If macro speedup ‚â• 1.10√ó (10% improvement):**
- ‚úì Fusion is worthwhile
- ‚Üí Deploy Phase 4.1 to production
- ‚Üí Consider Phase 4.2/4.3 (more fusion patterns)

**If macro speedup = 1.05-1.10√ó (5-10% improvement):**
- ‚ö† Modest gains
- ‚Üí Deploy Phase 4.1 (still useful)
- ‚Üí Phase 4.2/4.3 likely not worth engineering cost

**If macro speedup < 1.05√ó (< 5% improvement):**
- ‚úó Fusion benefits too small
- ‚Üí Re-evaluate: Maybe Python overhead dominates?
- ‚Üí Phase 4.2/4.3 definitely not worth it

---

## Honest Expectations (Based on micro-vs-macro analysis)

**Micro speedup:** 1.77√ó (validated)

**Realistic macro estimates:**

| Workload Type | Fusible % | Expected Macro | Measured |
|---------------|-----------|----------------|----------|
| Image pipeline | 20-30% | 1.10-1.15√ó | TBD |
| Neural layer | 15-25% | 1.08-1.12√ó | TBD |
| Batch processing | 10-30% | 1.05-1.20√ó | TBD |
| Mixed workload | 10-20% | 1.02-1.08√ó | TBD |

**If measured < expected:** Python overhead or I/O dominates (fusion can't help)

**If measured ‚âà expected:** Theory validated, fusion works as predicted

**If measured > expected:** Bonus! Cache effects or other synergies

---

## Decision Framework

```
IF avg_macro_speedup >= 1.10:
    VERDICT: "Deploy Phase 4.1, consider expanding"
    NEXT: Phase 4.2/4.3 investigation

ELIF avg_macro_speedup >= 1.05:
    VERDICT: "Deploy Phase 4.1, STOP fusion expansion"
    NEXT: Focus on other optimizations (lower abstractions, JIT)

ELSE:
    VERDICT: "Fusion ROI too low for deployment"
    NEXT: Investigate why (profiling, Python overhead analysis)
```

---

## Running Macro Benchmarks

```bash
# Run all macro benchmarks
python benchmarks/macro/bench_image_pipeline.py
python benchmarks/macro/bench_neural_layer.py
python benchmarks/macro/bench_batch_processing.py
python benchmarks/macro/bench_mixed_workload.py

# Or run the suite
python benchmarks/macro/run_all_macro_benchmarks.py
```

**Each benchmark reports:**
- Baseline time (unfused)
- Fused time (Phase 4.1 operations)
- Macro speedup (end-to-end)
- Comparison to micro predictions

---

## Truth-First Commitment

**We will:**
- ‚úì Measure real workloads (not synthetic micro-kernels)
- ‚úì Report actual speedups (with variance)
- ‚úì Compare to theoretical predictions
- ‚úì Make honest go/no-go decision on Phase 4.2/4.3

**We will NOT:**
- ‚ùå Cherry-pick favorable workloads
- ‚ùå Claim micro speedups are macro speedups
- ‚ùå Build more fusion if macro gains < 5%

**Goal:** Data-driven decision on whether to continue fusion expansion or deploy Phase 4.1 and move on.

---

**Micro benchmarks tell us what's possible. Macro benchmarks tell us what's real.** üî¨
