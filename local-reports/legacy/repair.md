# Ternary dual-VAEs current broken status

**Status = trained but poorly measured.**
The errors are in **metrics and testing**, not in the architecture or data.
â†’ Thatâ€™s gold: it means the model *already learns*, but the system evaluating it lies.

---

### ðŸ§© 2ï¸âƒ£ Repair Plan (exact tasks)

**Total time:** â‰ˆ 3â€“5 focused hours.

| Step | File                                    | Action                                                                                                     | Result                                       |
| ---- | --------------------------------------- | ---------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| âœ… 1  | `train_ternary_v5_5.py` L357            | Replace `softmax(logits)` â†’ `torch.distributions.Categorical(logits=logits).sample()` or `.argmax(dim=-1)` | True coverage (~86 %) measured correctly     |
| âœ… 2  | `run_benchmark.py` L24â€“49               | Require `--checkpoint`; abort if missing; feed `latent = model.encode(data)` into entropy computation      | Valid and reproducible metrics               |
| âœ… 3  | `tests/test_generalization.py` L116â€“215 | Remove prints; add asserts like `acc > 0.9`, `unique_ops >= 0.8 * total_ops`                               | Automated tests that truly validate learning |
| âœ… 4  | Choose checkpoint                       | Keep `epoch 106`; document SHA hash + true coverage                                                        | Certified baseline                           |
| âš™ï¸ 5 | Log outputs                             | Save `benchmarks/coverage_vs_entropy.json` with timestamp                                                  | Traceable benchmark                          |

---

### ðŸ§  3ï¸âƒ£ After the Repair

* **Honest benchmark:** will report ~86â€“95 % real coverage, coherent entropy.
* **Reproducible tests:** valid CI.
* **Usable checkpoint:** `latest.pt (epoch 106)` with signed hash.
* **Stable fluxttice:** ready to abstract as *foundation branch* (`dev/fluxttice-foundation`).

---

### ðŸš€ 4ï¸âƒ£ Minimal Viable Production

Once repaired:

1. Package the VAE as a module (`fluxttice_core/` with `encode`, `decode`, `coverage_report()`).
2. Publish reproducible benchmark â†’ first *scientific release* or API.
3. **Short-term monetization:** â€œfluxttice explorerâ€ / embedding API / technical paper.

---

### ðŸ’¡ 5ï¸âƒ£ Final Diagnosis

ðŸ‘‰ The *model* is healthy.
ðŸ‘‰ The *metrics* are broken.
ðŸ‘‰ The *fix* is cheap and linear.

---

# DIAGNOSIS MADE BY CODEX

â€¢ Key Findings

  - Coverage numbers are overstated because sample returns the expectation
    of softmax(logits) instead of drawing categorical samples, and both the
    trainer and benchmark round those expectations to integers (src/models/
    ternary_vae_v5_5.py:614, scripts/train/train_ternary_v5_5.py:357, scripts/
    benchmark/run_benchmark.py:117). When I decoded directly with argmax from
    the v5_5 latest.pt checkpoint (epoch 106) the model produced 16â€¯976â€“16â€¯921
    unique operationsâ€”only â‰ˆ86% of the ternary spaceâ€”versus the reported â‰¥99%.
  - The benchmark suite currently gives misleading results: it happily
    runs on random weights if --checkpoint is omitted (scripts/benchmark/
    run_benchmark.py:24-49), and its latent-entropy metric is computed on
    fresh Gaussian noise rather than latent codes from data (scripts/benchmark/
    run_benchmark.py:184-214), so those numbers say nothing about the trained
    model.
  - The â€œgeneralizationâ€ tests donâ€™t validate successâ€”they instantiate a brand-
    new, untrained model and only assert trivial conditions like accuracy
    >= 0 (tests/test_generalization.py:116-215). They function as print-only
    diagnostics, so the automated test suite never verifies that training
    learned anything.
  - Utility check: loading sandbox-training/checkpoints/v5_5/latest.pt shows
    the model does memorize operations wellâ€”hold-out reconstruction on 10%
    of ternary ops hits 100% for both VAEs, but generation tops out around
    86% unique ops (hash-counted) and later checkpoints in checkpoints/
    ternary_v5_5_best.pt regress to â‰ˆ95% coverage, so the usable checkpoint
    needs to be chosen carefully.

  Next Steps

  1. Replace sample with categorical sampling (or argmax) and recalc coverage/
     benchmarks; store both sampled and expectation-based metrics so you can
     compare.
  2. Require a checkpoint in run_benchmark.py, feed real reconstructions
     into the entropy metric, and persist benchmark outputs alongside config/
     checkpoint hashes.
  3. Convert tests/test_generalization.py into real assertions that load the
     trained checkpoint; remove or relocate the print-only diagnostics.
  4. Decide which checkpoint is production-ready (epochâ€¯106 vs later epochs)
     and document the true hash-based coverage so downstream consumers know the
     actual capability.