# Appetitive Dual-VAE Configuration
# Bio-inspired training with emergent drives toward r>0.99 3-adic correlation
# Goal: Coverage -> Metric -> Hierarchy -> Symbiosis -> Closure -> Agency

# Model architecture (base DualNeuralVAEV5)
model:
  input_dim: 9
  latent_dim: 16

  # Adaptive latent permeability
  rho_min: 0.1
  rho_max: 0.7

  # Adaptive entropy alignment
  lambda3_base: 0.3
  lambda3_amplitude: 0.15

  # Collapse threshold
  eps_kl: 0.0005

  # Enable adaptive features
  gradient_balance: true
  adaptive_scheduling: true

  # StateNet configuration
  use_statenet: true
  statenet_lr_scale: 0.05
  statenet_lambda_scale: 0.01

# =============================================================================
# APPETITIVE MODULE CONFIGURATION
# =============================================================================

# Appetite 1: Adaptive Ranking Loss (ultrametric structure)
# Triplet loss with multi-scale margins based on 3-adic valuation gaps
appetite_ranking:
  base_margin: 0.1           # Base margin for triplet loss
  n_triplets: 1000           # Triplets sampled per batch
  initial_weight: 0.5        # Phase 1A weight (dominant)

# Appetite 2: Hierarchical Norm Loss (MSB/LSB variance)
# Enforces variance hierarchy across latent dimension groups
appetite_hierarchy:
  n_groups: 4                # Groups: MSB (0-3), MID (4-7), ... LSB (12-15)
  target_ratio: 2.0          # Target variance ratio between adjacent groups
  initial_weight: 0.1        # Phase 1A weight

# Appetite 3: Curiosity Module (density-based exploration)
# KDE-based density estimation driving exploration of rare regions
appetite_curiosity:
  bandwidth: 1.0             # KDE bandwidth
  max_history: 5000          # Size of latent history buffer
  initial_weight: 0.0        # Phase 1A weight (inactive initially)

# Appetite 4: Symbiotic Bridge (VAE-A/VAE-B coupling)
# InfoNCE mutual information estimation with cross-attention
appetite_symbiosis:
  hidden_dim: 32             # Hidden dim for MI estimator
  temperature: 0.1           # InfoNCE temperature
  initial_weight: 0.0        # Phase 1A weight (inactive initially)

# Appetite 5: Algebraic Closure (homomorphism constraint)
# z_a + z_b - z_0 = z_{a o b}
appetite_closure:
  num_pairs: 500             # Random pairs per batch
  initial_weight: 0.0        # Phase 1A weight (inactive initially)

# Violation buffer for hard negative mining
violation_buffer:
  capacity: 10000            # Max stored violations
  decay: 0.99                # Exponential decay factor

# =============================================================================
# PHASE GATES (Metric-based, not epoch-based)
# =============================================================================
# Transitions triggered when metrics exceed thresholds

phase_gates:
  # Phase 1A -> 1B: Metric Foundation -> Structural Consolidation
  phase_1a_to_1b: 0.75       # Requires r > 0.75

  # Phase 1B -> 2A: -> Symbiotic Coupling
  phase_1b_to_2a: 0.85       # Requires r > 0.85

  # Phase 2A -> 2B: -> Algebraic Awakening
  phase_2a_to_2b: 2.0        # Requires MI > 2.0 bits

  # Phase 2B -> 3: -> Algebraic Satiation
  phase_2b_to_3: 0.5         # Requires addition accuracy > 50%

# =============================================================================
# BASE TRAINING CONFIGURATION (inherited from v5.6)
# =============================================================================

# Dataset
ternary_dataset:
  num_samples: 19683
  exhaustive: true
  seed: 42

# Training
seed: 42
batch_size: 256
num_workers: 0
total_epochs: 300            # Extended for phase transitions

# Optimizer
optimizer:
  type: adamw
  lr_start: 0.001
  weight_decay: 0.0001
  lr_schedule:
    - epoch: 0
      lr: 0.001
    - epoch: 50
      lr: 0.0005
    - epoch: 120
      lr: 0.0003
    - epoch: 180
      lr: 0.0001
    - epoch: 250
      lr: 0.00005
    - epoch: 300
      lr: 0.00002

# VAE-A parameters
vae_a:
  beta_start: 0.3
  beta_end: 0.8
  beta_warmup_epochs: 50
  temp_start: 1.0
  temp_end: 0.3
  temp_cyclic: true
  temp_boost_amplitude: 0.5

# VAE-B parameters
vae_b:
  beta_start: 0.0
  beta_end: 0.5
  beta_warmup_epochs: 50
  temp_start: 0.9
  temp_end: 0.2
  temp_phase4: 0.3
  entropy_weight: 0.05
  repulsion_weight: 0.01

# Free bits
free_bits: 0.5

# p-Adic losses (base ranking loss, appetite modules extend this)
padic_losses:
  enable_metric_loss: false
  enable_ranking_loss: false  # Handled by appetite_ranking instead
  enable_norm_loss: false     # Handled by appetite_hierarchy instead

# Controller parameters
controller:
  temp_lag: 30
  beta_phase_lag: 0.785
  entropy_ema_alpha: 0.9
  dH_dt_threshold: 0.05

# Training dynamics
grad_clip: 1.0

# Early stopping
patience: 150                 # Extended for phase transitions
min_delta: 0.0001
coverage_plateau_patience: 100
min_coverage_delta: 0.001

# Logging and checkpointing
log_interval: 1
checkpoint_freq: 10
checkpoint_dir: sandbox-training/checkpoints/appetitive
eval_num_samples: 50000
eval_interval: 5
coverage_check_interval: 5

# TensorBoard
tensorboard_dir: runs
experiment_name: appetitive_vae

# TorchInductor (disabled on Windows)
torch_compile:
  enabled: false
  backend: inductor
  mode: default
  fullgraph: false

# Data splits
train_split: 0.8
val_split: 0.1
test_split: 0.1

# Success metrics
target_coverage_percent: 99.0
target_correlation: 0.99      # NEW: r > 0.99 target
target_kld_min: 0.3
target_kld_max: 1.0
target_recon_accuracy: 85.0

# Phase transition epochs (base model, for scheduler compatibility)
phase_transitions:
  entropy_expansion_end: 40
  consolidation_end: 120
  resonant_coupling_end: 250
  ultra_exploration_start: 250
  statenet_warm_start: 20

# =============================================================================
# APPETITIVE DUAL-VAE: Teleological Ladder
# =============================================================================
#
# The five appetites create a natural progression:
#
# 1. RANKING (Ordering)
#    - Triplet loss preserves 3-adic ordinal structure
#    - Multi-scale margins respect exponential hierarchy
#    - Target: r > 0.99 concordance
#
# 2. HIERARCHY (Proprioception)
#    - MSB/LSB variance structure emerges
#    - Self-awareness of latent dimension importance
#    - Target: Var[MSB] > Var[MID] > Var[LSB]
#
# 3. CURIOSITY (Exploration)
#    - Density-based drive toward rare regions
#    - Prevents collapse to common operations
#    - Target: Full 19683 operation coverage
#
# 4. SYMBIOSIS (Coupling)
#    - Mutual information between VAE-A and VAE-B
#    - Cross-attention for complementary learning
#    - Target: MI > 2.0 bits
#
# 5. CLOSURE (Algebraic)
#    - Homomorphism constraint: z_a + z_b - z_0 = z_{a o b}
#    - Enables emergent addition
#    - Target: > 50% addition accuracy
#
# =============================================================================
