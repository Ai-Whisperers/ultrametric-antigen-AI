# V5.12.4 FIXED Configuration: Proper Checkpoint Loading for Coverage
#
# This config is optimized for:
# 1. Proper checkpoint loading to achieve 100% coverage
# 2. V5.11+ architecture with frozen components working correctly
# 3. Fixed from earlier null path issue
# 4. Production-ready training with emergent phenomena observation
#
# Expected runtime: 2-4 hours on RTX 3050
# Target: Coverage=100%, Hierarchy_B=-0.83, Richness>0.006

# Device Configuration
device:
  name: "v5_12_4_fixed_checkpoint"
  cuda_device: 0
  use_amp: false
  pin_memory: true
  num_workers: 4
  empty_cache_freq: 25

# PyTorch Compilation (Phase 1.1 - torch.compile for 1.4-2.0x speedup)
torch_compile:
  enabled: false     # Temporarily disabled - .item() calls causing graph breaks
  backend: eager     # Options: inductor, eager, aot_eager (start with eager for compatibility)
  mode: default      # Options: default, reduce-overhead, max-autotune
  fullgraph: false   # Set to true for maximum optimization (may break with complex models)

# Mixed Precision Training (Phase 1.2 - 2.0x speedup + 20-30% VRAM reduction)
mixed_precision:
  enabled: true
  dtype: float16     # Options: float16, bfloat16 (bfloat16 better for newer GPUs)
  init_scale: 65536.0
  growth_factor: 2.0
  backoff_factor: 0.5
  growth_interval: 2000

# Model Architecture (V5.12.4)
model:
  name: TernaryVAEV5_11_PartialFreeze
  latent_dim: 16
  hidden_dim: 64
  max_radius: 0.95
  curvature: 1.0

  use_controller: true
  use_dual_projection: true
  learnable_curvature: true
  manifold_aware: true

  projection_layers: 2
  projection_dropout: 0.1

  # V5.12.4: Improved encoder/decoder
  encoder_type: improved
  decoder_type: improved
  encoder_dropout: 0.1
  decoder_dropout: 0.1
  logvar_min: -10.0
  logvar_max: 2.0

# Option C: Partial Freeze
option_c:
  enabled: true
  encoder_b_lr_scale: 0.1
  encoder_a_lr_scale: 0.05

# FIXED: Proper Checkpoint Loading (was null, causing 0% coverage)
frozen_checkpoint:
  path: sandbox-training/checkpoints/v5_12_4/best_Q.pt  # Compatible V5.12.4 checkpoint
  encoder_to_load: both  # Load both encoders for coverage
  decoder_to_load: decoder_A

# Homeostatic Control
homeostasis:
  enabled: true
  coverage_freeze_threshold: 0.995
  coverage_unfreeze_threshold: 1.0
  coverage_floor: 0.95
  warmup_epochs: 10
  hysteresis_epochs: 5
  enable_annealing: true
  annealing_step: 0.003
  hierarchy_plateau_threshold: 0.001
  hierarchy_plateau_patience: 15
  hierarchy_patience_ceiling: 30

# Loss Configuration
loss:
  # PRIMARY: RichHierarchyLoss
  rich_hierarchy:
    enabled: true
    hierarchy_weight: 5.0
    coverage_weight: 1.0
    richness_weight: 2.5
    separation_weight: 3.0
    min_richness_ratio: 0.5

  # AUXILIARY: Radial targets
  radial:
    enabled: true
    inner_radius: 0.08
    outer_radius: 0.90
    radial_weight: 1.0

  # Phase 2: Geodesic
  geodesic:
    enabled: true
    phase_start_epoch: 50
    curvature: 1.0
    max_target_distance: 3.0
    n_pairs: 2000
    weight: 0.5

  # Global rank loss
  rank:
    enabled: true
    weight: 0.5
    temperature: 0.1

  # Zero-structure loss
  zero_structure:
    enabled: true
    valuation_weight: 0.5
    sparsity_weight: 0.3

# Riemannian Optimization
riemannian:
  enabled: true
  optimizer: adam

# Training Configuration
training:
  epochs: 30   # Quick test to validate optimizations work
  batch_size: 512
  lr: 8.0e-4
  weight_decay: 1.0e-4
  max_grad_norm: 1.0

  use_stratified: true
  high_v_budget_ratio: 0.25

  use_adaptive: true
  hierarchy_threshold: -0.75
  patience: 50  # Increased patience for full training
  min_epochs: 30  # Increased minimum epochs

  # Learning rate scheduler
  scheduler:
    type: cosine_annealing
    T_max: 50
    eta_min: 1.0e-6

  # Evaluation
  eval_every: 2
  save_every: 10
  print_every: 2

# Data Configuration
data:
  use_full_dataset: true
  n_operations: 19683

# Logging
logging:
  tensorboard: true
  log_dir: runs/v5_12_4_fixed_checkpoint
  print_every: 2

# Memory Optimization (Phase 1 and 2 optimizations)
memory:
  gradient_checkpointing: true  # 30-40% VRAM reduction (Phase 2.1)
  empty_cache_freq: 25
  cudnn_benchmark: true

# Grokking Detection (Phase 1.4)
grokking_detection:
  enabled: true
  monitor_window: 20
  plateau_threshold: 0.0001
  plateau_patience: 15
  accuracy_jump_threshold: 0.02

# Checkpoints
checkpoints:
  save_dir: sandbox-training/checkpoints/v5_12_4_fixed
  save_best: true
  best_metric: composite_score
  checkpoint_name: v5_12_4_fixed

# Success Criteria (should now be achievable with proper checkpoint)
targets:
  coverage: 1.0       # Should achieve 100% with frozen components
  hierarchy_B: -0.83  # Ceiling target
  richness: 0.006     # Preserve geometric diversity
  r_v9: 0.15          # Good separation

# Version tracking
version:
  model: "5.12.4"
  config: "fixed_checkpoint_1.0"
  date: "2026-01-12"
  changes:
    - "FIXED: checkpoint path from null to v5_11_homeostasis/best.pt"
    - "Should now achieve 100% coverage with proper frozen components"
    - "Reduced epochs to 50 for quick validation of fix"