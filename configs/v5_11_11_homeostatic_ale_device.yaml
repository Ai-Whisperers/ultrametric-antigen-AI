# V5.11.11 Homeostatic Configuration - Optimized for RTX 2060 SUPER (8GB VRAM)
#
# Device: AMD Ryzen + NVIDIA GeForce RTX 2060 SUPER
# RAM: 16GB physical, 48GB virtual
# VRAM: 8GB dedicated
# CUDA: 12.6 (Driver 580.97)
#
# Architecture: Homeostatic Control with Manifold-Aware Training
# - Frozen encoder_A (coverage preserved)
# - Trainable encoder_B (learns 3-adic structure)
# - Q-gated annealing for threshold relaxation
# - Learnable curvature for geometry adaptation
# - Riemannian optimization on Poincare manifold

# Device Configuration
device:
  name: "ale_device_rtx2060s"
  cuda_device: 0
  use_amp: true                        # Mixed precision (FP16) to save VRAM
  pin_memory: true                     # Faster CPU-GPU transfer
  num_workers: 4                       # DataLoader workers

# Model Architecture (V5.11.11)
model:
  name: TernaryVAEV5_11_PartialFreeze
  latent_dim: 16
  hidden_dim: 64                       # Projection network hidden dim
  max_radius: 0.95                     # Poincare ball radius
  curvature: 1.0                       # Initial hyperbolic curvature

  # V5.11.11 Features
  use_controller: true                 # Differentiable controller
  use_dual_projection: true            # Separate projections for VAE-A/B
  learnable_curvature: true            # Manifold-aware curvature learning
  manifold_aware: true                 # ManifoldParameter for Riemannian gradients

  # Projection Network
  projection_hidden_dim: 64
  projection_layers: 2                 # 2-layer MLP for projection
  projection_dropout: 0.1

# Option C Configuration (encoder_B trainable)
option_c:
  enabled: true
  encoder_b_lr_scale: 0.1              # Lower LR for encoder_B

# Frozen Checkpoint (from v5.5 with 100% coverage)
frozen_checkpoint:
  path: sandbox-training/checkpoints/v5_5/latest.pt
  encoder_to_load: both
  decoder_to_load: decoder_A

# Homeostatic Control (V5.11.7 + V5.11.8 Q-gated annealing)
homeostasis:
  enabled: true
  coverage_freeze_threshold: 0.995     # Freeze encoder_A when coverage < this
  coverage_unfreeze_threshold: 1.0
  coverage_floor: 0.95                 # Minimum coverage (annealing floor)
  warmup_epochs: 5
  hysteresis_epochs: 3

  # Q-gated annealing
  enable_annealing: true
  annealing_step: 0.005

  # Hierarchy monitoring
  hierarchy_plateau_threshold: 0.001
  hierarchy_plateau_patience: 5
  hierarchy_patience_ceiling: 15

  # Controller monitoring
  controller_grad_threshold: 0.01
  controller_grad_patience: 3
  controller_patience_ceiling: 10

# Progressive Unfreezing (V5.11.6)
progressive_unfreeze:
  enabled: false                       # Disabled - using homeostasis instead
  start_epoch: 5
  warmup_epochs: 5
  encoder_a_lr_scale: 0.05

# Loss Configuration
loss:
  # Unified Geodesic Loss
  geodesic:
    enabled: true
    curvature: 1.0
    max_target_distance: 3.0
    n_pairs: 2000
    use_smooth_l1: true

  # Radial Hierarchy Loss
  radial:
    enabled: true
    inner_radius: 0.1                  # Target for high-valuation (center)
    outer_radius: 0.85                 # Target for low-valuation (boundary)
    radial_weight: 2.0
    margin_weight: 1.0

  # Global Rank Loss (V5.11.3)
  rank:
    enabled: true
    weight: 1.0
    temperature: 0.1
    n_pairs: 2000

  # Zero-Structure Loss (V5.11.9)
  zero_structure:
    enabled: true
    valuation_weight: 1.0
    sparsity_weight: 0.5

# Riemannian Optimization (V5.11.10)
riemannian:
  enabled: true
  optimizer: adam                      # RiemannianAdam from geoopt

# Training Configuration - Optimized for 8GB VRAM
training:
  epochs: 150
  batch_size: 512                      # Conservative for 8GB VRAM
  lr: 1.0e-3
  weight_decay: 1.0e-3

  # Gradient management
  max_grad_norm: 1.0

  # Stratified sampling
  use_stratified: true                 # Ensure all valuation levels in batches

  # Adaptive curriculum
  use_adaptive: true
  hierarchy_threshold: -0.70           # Freeze tau when hierarchy reaches this
  patience: 20                         # Early stopping patience
  min_epochs: 30                       # Minimum before early stopping

  # LR Scheduler
  scheduler:
    type: cosine_warmup_restart
    T_0: 20
    T_mult: 2

  # Evaluation and checkpointing
  eval_every: 5
  save_every: 20
  print_every: 5

# Data Configuration
data:
  use_full_dataset: true
  n_operations: 19683                  # 3^9 ternary operations

# Logging
logging:
  tensorboard: true
  log_dir: runs/v5_11_11_homeostatic_ale_device
  print_every: 5

# Checkpoints
checkpoints:
  save_dir: sandbox-training/checkpoints/v5_11_11_homeostatic_ale_device
  save_best: true
  best_metric: radial_corr_A           # More negative is better
  best_mode: min
  checkpoint_name: v5_11_11_homeostatic_ale_device

# Success Criteria
targets:
  coverage: 1.0                        # 100% (must maintain)
  radial_hierarchy: -0.70              # Target: strong negative correlation
  distance_correlation: 0.8            # Target: high geodesic correlation
  Q_target: 1.5                        # Structure capacity target

# Memory Optimization for 8GB VRAM
memory:
  gradient_checkpointing: false        # Enable if OOM occurs
  empty_cache_freq: 10                 # Clear CUDA cache every N batches
  cudnn_benchmark: true                # Optimize conv operations
