# Ternary VAE v5.8 - Hard Negative Mining + Hierarchical Margin + Two-Phase Training
# Key innovations:
#   1. PAdicRankingLossV2 with hard negative mining (50% hard, 50% random)
#   2. Hierarchical margin scaled by valuation difference
#   3. Two-phase training: Coverage first (Phase 1), then correlation (Phase 2)
# Goal: Achieve r>0.85 AND coverage>95% (stretch: r>0.90, coverage>99%)

# Model architecture (same as v5.6 backbone)
model:
  input_dim: 9
  latent_dim: 16

  # Adaptive latent permeability (phase-scheduled)
  rho_min: 0.1
  rho_max: 0.7

  # Adaptive entropy alignment (cyclic)
  lambda3_base: 0.3
  lambda3_amplitude: 0.15

  # Collapse threshold
  eps_kl: 0.0005

  # Enable adaptive features
  gradient_balance: true
  adaptive_scheduling: true

  # StateNet v2 (from v5.6)
  use_statenet: true
  statenet_lr_scale: 0.05
  statenet_lambda_scale: 0.01

# Dataset generation
ternary_dataset:
  num_samples: 19683
  exhaustive: true
  seed: 42

# Training configuration
seed: 42
batch_size: 256
num_workers: 0

# Total training epochs
total_epochs: 300

# Unified optimizer
optimizer:
  type: adamw
  lr_start: 0.001
  weight_decay: 0.0001
  lr_schedule:
    - epoch: 0
      lr: 0.001
    - epoch: 100
      lr: 0.0005
    - epoch: 200
      lr: 0.0003
    - epoch: 270
      lr: 0.0001

# VAE-A parameters (chaotic, stable, exploratory)
vae_a:
  beta_start: 0.3
  beta_end: 0.8
  beta_warmup_epochs: 50
  temp_start: 1.0
  temp_end: 0.3
  temp_cyclic: true
  temp_boost_amplitude: 0.5

# VAE-B parameters (frozen, precise, entropy-amplifying)
vae_b:
  beta_start: 0.0
  beta_end: 0.5
  beta_warmup_epochs: 50
  temp_start: 0.9
  temp_end: 0.2
  temp_phase4: 0.3
  entropy_weight: 0.05
  repulsion_weight: 0.01

# KL divergence free bits
free_bits: 0.5

# =============================================================================
# TWO-PHASE TRAINING (v5.8 innovation)
# =============================================================================
two_phase_training:
  enabled: true

  # Phase 1: Coverage Focus (no ranking loss)
  phase1:
    end_epoch: 100               # When to transition to Phase 2
    ranking_weight: 0.0          # No ranking loss in Phase 1
    coverage_target: 95.0        # Target coverage before Phase 2

  # Phase 2: Correlation Optimization (with ranking loss)
  phase2:
    ranking_weight_start: 0.3    # Start with moderate weight
    ranking_weight_end: 0.8      # Ramp up to full weight
    ranking_ramp_epochs: 50      # Epochs to ramp from start to end
    coverage_floor: 90.0         # Minimum coverage to maintain
    coverage_penalty_scale: 5.0  # Penalty multiplier if coverage drops

# =============================================================================
# p-ADIC LOSSES V2 (v5.8 innovation)
# =============================================================================
padic_losses:
  # Metric loss (MSE-based - kept disabled)
  metric_loss_weight: 0.1
  metric_loss_scale: 1.0
  metric_n_pairs: 1000
  enable_metric_loss: false

  # Ranking Loss V2: Hard Negative Mining + Hierarchical Margin
  enable_ranking_loss_v2: true   # Use V2 instead of V1
  ranking_v2:
    base_margin: 0.05            # Minimum margin for all triplets
    margin_scale: 0.15           # Scale factor for valuation-based adjustment
    n_triplets: 500              # Triplets per batch
    hard_negative_ratio: 0.5     # 50% hard negatives, 50% random
    semi_hard: true              # Use semi-hard negatives

  # Legacy V1 ranking (disabled when V2 is enabled)
  enable_ranking_loss: false
  ranking_loss_weight: 0.5
  ranking_margin: 0.1
  ranking_n_triplets: 500

  # Norm regularizer
  norm_loss_weight: 0.05
  enable_norm_loss: true

# Controller parameters
controller:
  temp_lag: 30
  beta_phase_lag: 0.785
  entropy_ema_alpha: 0.9
  dH_dt_threshold: 0.05

# Training dynamics
grad_clip: 1.0

# Early stopping (relaxed for two-phase training)
patience: 150
min_delta: 0.0001
coverage_plateau_patience: 150
min_coverage_delta: 0.001

# Logging and checkpointing
log_interval: 1
checkpoint_freq: 10
checkpoint_dir: sandbox-training/checkpoints/v5_8
eval_num_samples: 50000
eval_interval: 5
coverage_check_interval: 5

# TensorBoard
tensorboard_dir: runs
experiment_name: null

# TorchInductor compilation
torch_compile:
  enabled: false
  backend: inductor
  mode: default
  fullgraph: false

# Data splits
train_split: 0.8
val_split: 0.1
test_split: 0.1

# Success metrics (targets)
target_coverage_percent: 95.0
target_kld_min: 0.3
target_kld_max: 1.0
target_recon_accuracy: 85.0
target_ranking_correlation: 0.85  # Realistic target with Euclidean geometry

# Phase transition epochs
phase_transitions:
  entropy_expansion_end: 40
  consolidation_end: 120
  resonant_coupling_end: 250
  ultra_exploration_start: 250
  statenet_warm_start: 20

# =============================================================================
# v5.8 Key Improvements over v5.7:
# =============================================================================
#
# 1. PAdicRankingLossV2:
#    - Hard negative mining: Focus on triplets that violate ranking
#    - Semi-hard strategy: Select negatives close but in wrong order
#    - 50/50 split: Half hard negatives, half random (for diversity)
#
# 2. HIERARCHICAL MARGIN:
#    - margin = base_margin + margin_scale * |v_pos - v_neg|
#    - Larger valuation difference = easier distinction = larger margin
#    - Better gradient signal at all scales of 3-adic distance
#
# 3. TWO-PHASE TRAINING:
#    - Phase 1 (epochs 0-100): Focus on coverage, no ranking loss
#    - Phase 2 (epochs 100+): Add ranking loss, maintain coverage floor
#    - Prevents coverage-correlation tradeoff from early training
#
# 4. COVERAGE PROTECTION:
#    - Coverage floor of 90% in Phase 2
#    - Strong penalty if coverage drops below floor
#    - Ensures we don't sacrifice coverage for correlation
#
# Expected Results:
#    - Phase 1: Coverage should reach 95%+ with baseline correlation (~0.55)
#    - Phase 2: Correlation should improve to 0.85+ while maintaining 90%+ coverage
#
# =============================================================================
