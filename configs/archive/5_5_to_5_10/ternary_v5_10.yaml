# Ternary VAE v5.10+ - Radial-First Curriculum Learning
# =============================================================================
config_version: "5.10.1"
# Key innovations (StateNet v5 + curriculum-controlled radial stratification):
#   1. RadialStratificationLoss: Enforces 3-adic tree structure via radial hierarchy
#   2. ContinuousCurriculum: Differentiable tau blends radial -> ranking losses
#   3. StateNet v5: 20D input (+radial_loss, tau), 8D output (+delta_curriculum)
#   4. Emergent curriculum: StateNet learns optimal radial->ranking transition
#   5. HyperbolicPrior: Wrapped Normal on Poincare ball (replaces Gaussian KL)
#   6. HomeostaticAdaptation: Both VAEs self-regulate for algebraic convergence
#
# Philosophy: "Learn radial hierarchy first, then angular discrimination"
#   - tau=0: Pure radial learning (coarse tree structure)
#   - tau=1: Pure ranking learning (fine angular discrimination)
#   - StateNet controls curriculum via delta_curriculum output
#   - No hard thresholds - curriculum pacing emerges from learning
#
# Goal: r > 0.95, coverage > 95% via radial-first curriculum
# =============================================================================

# Model architecture (same backbone)
model:
  input_dim: 9
  latent_dim: 16

  # Adaptive latent permeability (phase-scheduled)
  rho_min: 0.1
  rho_max: 0.7

  # Adaptive entropy alignment (cyclic)
  lambda3_base: 0.3
  lambda3_amplitude: 0.15

  # Collapse threshold
  eps_kl: 0.0005

  # Enable adaptive features
  gradient_balance: true
  adaptive_scheduling: true

  # StateNet v5 with curriculum control (20D input, 8D output)
  # Inherits v5.6 (12D: H, KL, grad, rho, lambda, coverage)
  # Inherits v5.7 (+2D: r_A, r_B ranking correlations, +1D: delta_ranking)
  # v5.10 adds (+4D: mean_radius_A/B, prior_sigma, curvature, +2D: delta_sigma, delta_curvature)
  # v5 adds (+2D: radial_loss, curriculum_tau, +1D: delta_curriculum)
  use_statenet: true
  statenet_version: 5                 # v5 enables curriculum control
  statenet_lr_scale: 0.1              # Learning rate correction scale
  statenet_lambda_scale: 0.02         # Lambda (1,2,3) correction scale
  statenet_ranking_scale: 0.3         # v5.7: ranking weight correction scale
  statenet_hyp_sigma_scale: 0.05      # v5.10: prior_sigma correction scale
  statenet_hyp_curvature_scale: 0.02  # v5.10: curvature correction scale
  statenet_curriculum_scale: 0.1      # v5: curriculum advancement scale

# =============================================================================
# RADIAL STRATIFICATION (v5.10+ - enforces 3-adic tree structure)
# =============================================================================
# Maps 3-adic valuation to radial position in latent space:
#   - High valuation (e.g., 81 = 3^4) -> small radius (near origin = tree root)
#   - Low valuation (e.g., 1, 2, 4, 5) -> large radius (near boundary = leaves)
# This creates concentric shells encoding tree depth.
radial_stratification:
  enabled: true
  inner_radius: 0.1          # Target radius for highest valuation (tree root)
  outer_radius: 0.85         # Target radius for lowest valuation (tree leaves)
  max_valuation: 9           # log_3(19683) - maximum depth in 3-adic tree
  valuation_weighting: true  # Weight high-valuation points more (rarer, important)
  loss_type: smooth_l1       # 'smooth_l1' (robust) or 'mse' (sharp gradients)
  base_weight: 0.3           # Base contribution to total loss

# =============================================================================
# CONTINUOUS CURRICULUM (v5.10+ - StateNet-driven radial->ranking transition)
# =============================================================================
# tau = 0: Pure radial learning (coarse tree structure)
# tau = 1: Pure ranking learning (fine angular discrimination)
# StateNet v5 learns optimal curriculum trajectory via delta_curriculum output.
# No hard thresholds - fully emergent curriculum pacing.
curriculum:
  enabled: true
  initial_tau: 0.0           # Start with pure radial focus
  tau_min: 0.0               # Minimum tau bound
  tau_max: 1.0               # Maximum tau bound
  tau_scale: 0.1             # How much delta_curriculum affects tau per step
  tau_momentum: 0.95         # EMA momentum for tau history tracking

# Dataset generation
ternary_dataset:
  num_samples: 19683
  exhaustive: true
  seed: 42

# Training configuration
seed: 42
batch_size: 256
num_workers: 0
total_epochs: 50  # P2 FIX: Increased for exploration boost validation (was 30)
# P2 FIX: GPU-resident dataset (eliminates CPUâ†’GPU transfers)
# All 19,683 samples loaded to GPU once (~865 KB) for zero-transfer training
gpu_resident: true

# Unified optimizer
optimizer:
  type: adamw
  lr_start: 0.001
  weight_decay: 0.0001
  lr_schedule:
    - epoch: 0
      lr: 0.001
    - epoch: 100
      lr: 0.0005
    - epoch: 200
      lr: 0.0003
    - epoch: 270
      lr: 0.0001

# VAE-A parameters (chaotic, explores boundary of Poincare ball)
vae_a:
  beta_start: 0.3
  beta_end: 0.8
  beta_warmup_epochs: 50
  temp_start: 1.0
  temp_end: 0.3
  temp_cyclic: true
  temp_boost_amplitude: 0.5

# VAE-B parameters (stable, anchors near origin of Poincare ball)
vae_b:
  beta_start: 0.0
  beta_end: 0.5
  beta_warmup_epochs: 50
  temp_start: 0.9
  temp_end: 0.2
  temp_phase4: 0.3
  entropy_weight: 0.05
  repulsion_weight: 0.01

# KL divergence free bits (lower with hyperbolic prior)
free_bits: 0.3

# =============================================================================
# CONTINUOUS FEEDBACK (v5.10 - integrated with hyperbolic homeostasis)
# =============================================================================
continuous_feedback:
  enabled: true
  base_ranking_weight: 0.4
  coverage_threshold: 92.0
  coverage_sensitivity: 0.05
  coverage_trend_sensitivity: 1.0
  min_ranking_weight: 0.1
  max_ranking_weight: 0.8
  coverage_ema_alpha: 0.95

# =============================================================================
# PURE HYPERBOLIC p-ADIC LOSSES (v5.10 - complete geometric consistency)
# =============================================================================
padic_losses:
  # DISABLED: Euclidean metric loss (conflicts with hyperbolic structure)
  enable_metric_loss: false
  metric_loss_weight: 0.0

  # DISABLED: Hyperbolic Ranking Loss - hard negative mining too slow on RTX 3050
  enable_ranking_loss_hyperbolic: false
  ranking_hyperbolic:
    base_margin: 0.05
    margin_scale: 0.15
    n_triplets: 100
    hard_negative_ratio: 0.2
    curvature: 2.0
    radial_weight: 0.4
    max_norm: 0.95
    weight: 0.5

  # DISABLED: Euclidean norm loss (radial hierarchy replaces this)
  enable_norm_loss: false
  norm_loss_weight: 0.0

  # ENABLED: Simple ranking loss (V1) - random triplet sampling, no hard mining
  # P1 FIX: Increased weight from 0.5 to 2.0 to compete with reconstruction (~14)
  # This provides actual gradient signal for preserving 3-adic structure
  enable_ranking_loss: true
  ranking_loss_weight: 2.0         # P1 FIX: Increased from 0.5 to 2.0 (best=0.7465 correlation)
  ranking_margin: 0.1              # Top-level key (code reads this)
  ranking_n_triplets: 500          # Top-level key (code reads this)
  ranking_loss:                    # Nested config (for reference)
    margin: 0.1
    n_triplets: 500
    weight: 2.0

  # ==========================================================================
  # v5.10 PURE HYPERBOLIC MODULES (new)
  # ==========================================================================
  hyperbolic_v10:
    # Hyperbolic Prior: Wrapped Normal on Poincare ball
    # Replaces KL(q || N(0,I)) with KL(q || WrappedNormal)
    # P1 FIX: Enabled to utilize StateNet's delta_sigma/delta_curvature outputs
    use_hyperbolic_prior: true
    prior:
      homeostatic: true        # Enable adaptive sigma/curvature
      latent_dim: 16
      curvature: 2.2           # P0 FIX: Optimal from epoch 8 analysis
      prior_sigma: 1.0         # Spread in tangent space (homeostatic adapts)
      max_norm: 0.95
      # P0 FIX: Realistic KL target (actual hyperbolic KL runs at 80-90)
      kl_target: 50.0          # Was hardcoded 1.0, caused curvature runaway
      target_radius: 0.5       # Target mean radius in Poincare ball
      # P0 FIX: Tighter homeostatic bounds (prevent runaway)
      sigma_min: 0.8
      sigma_max: 1.2
      curvature_min: 2.0       # Lock to stable range (was 0.5)
      curvature_max: 2.5       # Prevent runaway (was 4.0)
      adaptation_rate: 0.005   # Slower adaptation (was 0.01)
      ema_alpha: 0.05          # More smoothing (was 0.1)

    # Hyperbolic Reconstruction: Radius-weighted cross-entropy
    # Points near origin (high valuation) get higher weight
    # DISABLED: Causes training divergence, needs debugging
    use_hyperbolic_recon: false
    recon:
      homeostatic: true
      mode: weighted_ce        # 'geodesic', 'weighted_ce', or 'hybrid'
      curvature: 2.0
      max_norm: 0.95
      geodesic_weight: 0.3     # For hybrid mode
      radius_weighting: true   # Weight by hyperbolic position
      radius_power: 2.0        # Higher = more emphasis on origin
      weight: 0.5              # Weight in total loss
      # Homeostatic bounds
      geodesic_weight_min: 0.1
      geodesic_weight_max: 0.8
      radius_power_min: 1.0
      radius_power_max: 4.0
      adaptation_rate: 0.01

    # Hyperbolic Centroid Loss: Enforce tree structure via Frechet means
    # Points with same 3-adic prefix cluster around hyperbolic centroids
    # DISABLED: O(n^2) computation too slow for RTX 3050, needs vectorization
    use_centroid_loss: false
    centroid:
      max_level: 4             # Tree depth to enforce (3^4 = 81 clusters)
      curvature: 2.0
      max_norm: 0.95
      weight: 0.2              # Weight in total loss (lighter touch)
      # Level weights: higher levels (finer clusters) get lower weight
      # Default: exponential decay [0.5, 0.25, 0.125, 0.0625]

# Controller parameters
controller:
  temp_lag: 30
  beta_phase_lag: 0.785
  entropy_ema_alpha: 0.9
  dH_dt_threshold: 0.05

# Training dynamics
grad_clip: 1.0

# Early stopping (relaxed for homeostatic exploration)
patience: 150
min_delta: 0.0001
coverage_plateau_patience: 150
min_coverage_delta: 0.001

# =============================================================================
# P1 FIXES: Correlation-Aware Training
# =============================================================================
correlation_feedback:
  enabled: true
  # Correlation loss: penalize deviation from target correlation
  correlation_loss_weight: 0.1    # Weight of -correlation term in loss
  target_correlation: 0.95        # Target correlation for loss computation
  # Early stopping on correlation drop
  correlation_drop_threshold: 0.05  # Stop if corr drops more than this from best
  correlation_patience: 10          # Epochs to wait before stopping on corr drop

# =============================================================================
# P2 FIXES: Coverage-Triggered Exploration + Correlation Loss
# =============================================================================
exploration_boost:
  enabled: true
  # Coverage stall detection
  coverage_stall_threshold: 0.5   # Min coverage delta to not be "stalled" (%)
  coverage_stall_patience: 5      # Epochs of stall before boosting
  # Temperature boost when stalled
  temp_boost_factor: 1.15         # Multiply temperature by this when stalled
  temp_boost_max: 2.0             # Maximum temperature multiplier
  # Ranking weight reduction when stalled (less structure constraint)
  ranking_reduction_factor: 0.9   # Multiply ranking weight by this when stalled
  ranking_reduction_min: 0.05     # Minimum ranking weight

correlation_loss:
  enabled: true
  # Add -weight * correlation directly to loss (rewards higher correlation)
  # P1 FIX: Increased weight from 0.5 to 3.0 to be significant vs total loss (~14)
  # P1 FIX: Removed warmup to activate immediately (correlation drops in first 3 epochs)
  weight: 3.0                     # Weight of correlation bonus in loss (was 0.5)
  warmup_epochs: 0                # Start immediately (was 5)
  use_cached: true                # Use cached correlation (computed at eval_interval)

# Logging and checkpointing (OPTIMIZED FOR SPEED)
log_interval: 20                 # Batch logging every N batches (was 1)
log_dir: logs
checkpoint_freq: 10              # Save checkpoint every 10 epochs
checkpoint_dir: sandbox-training/checkpoints/v5_10
eval_num_samples: 500            # Reduced for faster evaluation
eval_interval: 3                 # THREE-BODY FIX: Frequent correlation check (was 5)
coverage_check_interval: 3       # THREE-BODY FIX: Frequent coverage eval (was 10)

# TensorBoard (MINIMAL - focus on checkpoints)
tensorboard_dir: runs
experiment_name: null
histogram_interval: 50           # Reduced frequency
embedding_interval: 0            # DISABLED - use checkpoints for analysis
embedding_n_samples: 0           # Not used when interval=0

# TorchInductor compilation (DISABLED - triton not supported on Windows)
torch_compile:
  enabled: false
  backend: inductor
  mode: reduce-overhead
  fullgraph: false

# Data splits
train_split: 0.8
val_split: 0.1
test_split: 0.1

# Success metrics (ambitious targets)
target_coverage_percent: 99.7
target_kld_min: 0.3
target_kld_max: 1.0
target_recon_accuracy: 85.0
target_ranking_correlation: 0.99

# Phase transition epochs
phase_transitions:
  entropy_expansion_end: 40
  consolidation_end: 120
  resonant_coupling_end: 250
  ultra_exploration_start: 250
  statenet_warm_start: 20

# =============================================================================
# v5.10.1 Key Innovations (over v5.10):
# =============================================================================
#
# 1. RADIAL STRATIFICATION LOSS:
#    - Explicit loss enforcing 3-adic tree structure via radial position
#    - High valuation (e.g., 81 = 3^4) -> target radius near origin
#    - Low valuation (e.g., 1, 2, 4, 5) -> target radius near boundary
#    - Solves the core problem: all embeddings clustering at similar radii
#
# 2. CONTINUOUS CURRICULUM:
#    - Differentiable tau in [0,1] blends radial and ranking losses
#    - tau=0: Pure radial learning (learn tree structure first)
#    - tau=1: Pure ranking learning (refine angular discrimination)
#    - Loss = (1-tau)*L_radial + tau*L_ranking
#
# 3. STATENET V5 (extends v4):
#    - Input: 20D (18D + radial_loss_norm + curriculum_tau)
#    - Output: 8D (7D + delta_curriculum)
#    - Curriculum attention head: learns what affects curriculum decisions
#    - Fully emergent curriculum pacing - no hard thresholds
#
# 4. EMERGENT CURRICULUM TRAJECTORY:
#    - StateNet observes radial_loss and outputs delta_curriculum
#    - Early training: keeps tau low while radial structure is learned
#    - Transition: increases tau as radial correlation improves
#    - Late training: tau near 1.0 for angular refinement
#    - Can retreat (negative delta) if radial structure degrades
#
# Expected Behavior:
#    - Epoch 0-50: tau stays near 0, radial structure forms
#    - Epoch 50-150: tau gradually increases as radial loss drops
#    - Epoch 150+: tau approaches 1.0, fine-grained angular learning
#    - StateNet can adapt this trajectory based on actual training dynamics
#
# Target Metrics:
#    - Radial correlation > 0.90 (valuation predicts radius)
#    - Ranking correlation > 0.95 (angular discrimination)
#    - Coverage > 95% (manifold coverage)
#
# =============================================================================
#
# v5.10 Base Features (still active):
# =============================================================================
#
# 1. HYPERBOLIC PRIOR (optional):
#    - Wrapped Normal distribution on Poincare ball
#    - Eliminates Euclidean/hyperbolic conflict
#
# 2. HOMEOSTATIC EMERGENCE:
#    - Both VAEs self-regulate hyperbolic parameters
#    - prior_sigma and curvature adapt dynamically
#
# 3. REMOVED EUCLIDEAN CONTAMINATION:
#    - norm_loss: DISABLED (radial stratification handles hierarchy)
#    - metric_loss: DISABLED (Poincare distance in ranking)
#
# =============================================================================
