# Mathematical Foundations of Ternary VAE v5.5

## Table of Contents

1. [Problem Statement](#problem-statement)
2. [Variational Autoencoder Theory](#variational-autoencoder-theory)
3. [Dual-VAE Architecture](#dual-vae-architecture)
4. [Stop-Gradient Cross-Injection](#stop-gradient-cross-injection)
5. [StateNet Meta-Learning](#statenet-meta-learning)
6. [Loss Functions and Objectives](#loss-functions-and-objectives)
7. [Phase-Scheduled Optimization](#phase-scheduled-optimization)
8. [Convergence Guarantees](#convergence-guarantees)

---

## Problem Statement

### The Ternary Operation Space

A **ternary operation** is a 9-dimensional truth table where each input/output takes values from the ternary alphabet **Î£ = {-1, 0, +1}**.

**Total possible operations**: 3^9 = **19,683**

**Mathematical Representation**:
```
f: Î£^9 â†’ â„^9
where f = (fâ‚, fâ‚‚, ..., fâ‚‰) and each fáµ¢ âˆˆ {-1, 0, +1}
```

### The Challenge

**Goal**: Learn a latent representation that can:
1. **Generate** all 19,683 operations with high probability
2. **Maintain diversity** across the operation space
3. **Avoid collapse** to a subset of operations
4. **Generalize** to unseen combinations

**Why is this hard?**
- Standard VAEs suffer from **posterior collapse**: the decoder ignores the latent code
- Single-pathway models exhibit **mode collapse**: concentrate on easy-to-generate operations
- High-dimensional discrete spaces are difficult to explore systematically

---

## Variational Autoencoder Theory

### Standard VAE Formulation

A Variational Autoencoder (VAE) maximizes the Evidence Lower Bound (ELBO):

```
log p(x) â‰¥ ELBO = ğ”¼_{q_Ï†(z|x)}[log p_Î¸(x|z)] - D_KL(q_Ï†(z|x) || p(z))
            ^^^^^^^^^^^^^^^^^^^^^   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            Reconstruction term      Regularization term
```

**Components**:
- **Encoder**: q_Ï†(z|x) - probabilistic mapping from data to latent space
- **Decoder**: p_Î¸(x|z) - probabilistic mapping from latent to data
- **Prior**: p(z) = ğ’©(0, I) - standard Gaussian
- **Parameters**: Ï† (encoder), Î¸ (decoder)

### The Reparameterization Trick

To enable backpropagation through stochastic sampling:

```
z = Î¼_Ï†(x) + Ïƒ_Ï†(x) âŠ™ Îµ,  where Îµ ~ ğ’©(0, I)
```

This separates the stochasticity (Îµ) from the learnable parameters (Î¼, Ïƒ).

### Temperature-Scaled Sampling

We extend this with temperature scaling for exploration control:

```
z = Î¼_Ï†(x) + Ï„ Â· Ïƒ_Ï†(x) âŠ™ Îµ
```

where **Ï„** (temperature) controls exploration:
- **Ï„ > 1**: High temperature â†’ more exploration, diverse samples
- **Ï„ < 1**: Low temperature â†’ less exploration, deterministic samples
- **Ï„ = 0**: Deterministic (z = Î¼)

---

## Dual-VAE Architecture

### Why Two VAEs?

A single VAE faces a fundamental trade-off:
- **High Î²** (strong regularization) â†’ poor reconstruction, underfitting
- **Low Î²** (weak regularization) â†’ posterior collapse, ignores latent

**Solution**: Use **two VAEs with complementary objectives**:

1. **VAE-A (Chaotic Regime)**:
   - High temperature â†’ exploration
   - Moderate Î² â†’ balanced KL/reconstruction
   - Task: **Discover new operations**

2. **VAE-B (Frozen Regime)**:
   - Low temperature â†’ exploitation
   - Residual connections â†’ strong reconstruction
   - Task: **Consolidate discoveries**

### Mathematical Formulation

**Joint Objective**:
```
â„’_total = Î»â‚ Â· â„’_A + Î»â‚‚ Â· â„’_B + Î»â‚ƒ Â· |H(z_A) - H(z_B)|
          ^^^^^^^^^^   ^^^^^^^^^^   ^^^^^^^^^^^^^^^^^^^^^
          VAE-A loss   VAE-B loss   Entropy alignment
```

Where:
```
â„’_A = ğ”¼_q_A[log p_A(x|z_A)] - Î²_A Â· D_KL(q_A(z_A|x) || p(z))

â„’_B = ğ”¼_q_B[log p_B(x|z_B)] - Î²_B Â· D_KL(q_B(z_B|x) || p(z))
      + Î±_ent Â· H_output(p_B(x|z_B))
      + Î±_rep Â· â„’_repulsion(z_B)
```

**Entropy Alignment**:
```
H(z) = -âˆ‘áµ¢ âˆ« p(záµ¢) log p(záµ¢) dzáµ¢   (estimated via histogram)
```

Forces VAE-A and VAE-B to explore similar entropy regimes, preventing divergence.

---

## Stop-Gradient Cross-Injection

### The Information Flow Problem

If VAE-A and VAE-B train independently:
- âŒ No knowledge sharing
- âŒ Redundant exploration
- âŒ Slower convergence

If they share gradients directly:
- âŒ One pathway dominates
- âŒ Collapse to single mode
- âŒ Loss of diversity

### Solution: Stop-Gradient Cross-Injection

**Mechanism**:
```
zÌƒ_A = (1 - Ï) Â· z_A + Ï Â· sg(z_B)
zÌƒ_B = (1 - Ï) Â· z_B + Ï Â· sg(z_A)

where sg(Â·) = stop_gradient(Â·)  [no backprop through this term]
```

**Permeability Ï**:
- **Ï = 0**: Complete isolation (no information flow)
- **Ï = 1**: Complete mixing (averaged representations)
- **0 < Ï < 1**: Partial coupling (controlled flow)

### Mathematical Properties

**Theorem 1 (One-Way Information Flow)**:
The stop-gradient operator ensures:
```
âˆ‚â„’_A/âˆ‚Î¸_B = 0   and   âˆ‚â„’_B/âˆ‚Î¸_A = 0
```

**Proof**:
Since z_B appears in â„’_A only through sg(z_B), and âˆ‚sg(z_B)/âˆ‚Î¸_B = 0 by definition, the chain rule gives âˆ‚â„’_A/âˆ‚Î¸_B = 0. Similarly for âˆ‚â„’_B/âˆ‚Î¸_A.

**Corollary**: VAE-A and VAE-B have **independent gradient flows**, preventing dominance.

**Theorem 2 (Information Transfer)**:
Despite independent gradients, information flows through:
```
I(z_A; z_B) > 0  when Ï > 0
```

where I(Â·;Â·) is mutual information.

**Proof Sketch**:
The latent codes zÌƒ_A and zÌƒ_B contain components from both pathways. During decoding, if p_A(x|zÌƒ_A) reconstructs x well, it must utilize information from z_B. This creates an indirect coupling through the data space.

---

## StateNet Meta-Learning

### Motivation

Training dynamics depend on multiple factors:
- Entropy levels H_A, H_B
- KL divergences KL_A, KL_B
- Gradient balance ratio
- Current phase and permeability

**Problem**: Manual tuning is brittle and suboptimal.

**Solution**: Learn to adapt hyperparameters using a meta-controller.

### StateNet Architecture

**Autodecoder Design**:
```
State Vector (9D):
s = [H_A, H_B, KL_A, KL_B, grad_ratio, Ï, Î»â‚, Î»â‚‚, Î»â‚ƒ]

Encoder:
h = Tanh(LayerNorm(Linear(s)))    # Normalize and compress
z_state = Linear(h)                # Latent state (8D)

Decoder:
corrections = Tanh(Linear(ReLU(Linear(z_state))))  # 4D output
[Î”lr, Î”Î»â‚, Î”Î»â‚‚, Î”Î»â‚ƒ] = corrections
```

### Hyperparameter Update Rules

**Learning Rate**:
```
lr_new = lr_old Â· (1 + Î±_lr Â· Î”lr)
lr_new = clip(lr_new, 1e-6, 0.01)
```

**Loss Weights**:
```
Î»â‚_new = clip(Î»â‚_old + Î±_Î» Â· Î”Î»â‚, 0.5, 0.95)
Î»â‚‚_new = clip(Î»â‚‚_old + Î±_Î» Â· Î”Î»â‚‚, 0.5, 0.95)
Î»â‚ƒ_new = clip(Î»â‚ƒ_old + Î±_Î» Â· Î”Î»â‚ƒ, 0.15, 0.75)
```

where:
- **Î±_lr = 0.05** (5% LR modulation scale)
- **Î±_Î» = 0.01** (1% lambda modulation scale)

### Training StateNet

**Objective**:
```
â„’_StateNet = ||corrections||â‚‚Â²   (implicit regularization)
```

StateNet learns through:
1. **Gradient signals** from main VAE loss
2. **Implicit reward** from coverage improvement
3. **Regularization** toward small corrections

**No explicit reward** is needed - StateNet learns what corrections improve training through backpropagation of the total loss.

---

## Loss Functions and Objectives

### Complete Loss Decomposition

```
â„’_total = Î»â‚ Â· g_A Â· â„’_A + Î»â‚‚ Â· g_B Â· â„’_B + Î»â‚ƒ Â· â„’_align

where:
  â„’_A = CE_A + Î²_A Â· KL_A
  â„’_B = CE_B + Î²_B Â· KL_B + Î±_ent Â· (-H_output) + Î±_rep Â· â„’_rep
  â„’_align = |H(z_A) - H(z_B)|

  g_A, g_B = gradient balance scales
```

### 1. Cross-Entropy (Reconstruction Loss)

For ternary outputs, we use categorical cross-entropy:

```
CE = -1/N âˆ‘áµ¢â‚Œâ‚á´º âˆ‘â±¼â‚Œâ‚â¹ âˆ‘â‚–âˆˆ{-1,0,+1} ğŸ™[xáµ¢â±¼=k] log p_Î¸(xáµ¢â±¼=k|z)
```

where p_Î¸(x|z) = softmax(logits_Î¸(z)).

**Conversion**: Input values {-1, 0, +1} â†’ class indices {0, 1, 2}

### 2. KL Divergence (Regularization)

```
KL(q_Ï†(z|x) || p(z)) = -1/2 âˆ‘áµ¢â‚Œâ‚áµˆ (1 + log Ïƒáµ¢Â² - Î¼áµ¢Â² - Ïƒáµ¢Â²)
```

where d is latent dimension.

**Purpose**: Prevents posterior collapse by forcing q(z|x) â‰ˆ p(z) = ğ’©(0,I)

### 3. Output Entropy (Diversity)

For VAE-B, we maximize output diversity:

```
H_output = -âˆ‘â‚– pÌ„â‚– log pÌ„â‚–

where pÌ„â‚– = 1/N âˆ‘áµ¢â‚Œâ‚á´º p_Î¸(xáµ¢=k|záµ¢)  (average distribution)
```

**Effect**: Encourages VAE-B to generate diverse operations, not concentrate on a few modes.

### 4. Repulsion Loss (Anti-Clustering)

```
â„’_rep = 1/NÂ² âˆ‘áµ¢,â±¼ exp(-||z_i - z_j||Â²/(2ÏƒÂ²))
```

**Purpose**: Pushes latent codes apart, preventing clustering in latent space.

### 5. Gradient Balance Scaling

To prevent one VAE from dominating:

```
g_A = ||âˆ‡_Î¸_B â„’||_EMA / ||âˆ‡_Î¸_A â„’||_EMA
g_B = ||âˆ‡_Î¸_A â„’||_EMA / ||âˆ‡_Î¸_B â„’||_EMA

g_A, g_B âˆˆ [0.5, 2.0]  (clipped for stability)
```

**Update Rule** (Exponential Moving Average):
```
||âˆ‡_Î¸_A â„’||_EMA â† Î± Â· ||âˆ‡_Î¸_A â„’||_EMA + (1-Î±) Â· ||âˆ‡_Î¸_A â„’||
```

where Î± = 0.9 (balanced) or 0.95 (when balanced), adaptively.

---

## Phase-Scheduled Optimization

### Four Training Phases

**Phase 1: Isolation (Epochs 0-40)**
```
Ï = 0.1          # Minimal cross-talk
Goal: Independent pathway establishment
```

**Phase 2: Consolidation (Epochs 40-120)**
```
Ï: 0.1 â†’ 0.3     # Gradual coupling
Goal: Share discoveries while maintaining diversity
```

**Phase 3: Resonant Coupling (Epochs 120-250)**
```
Ï: 0.3 â†’ 0.7     # Strong coupling (gated on gradient balance)
Goal: Synergistic exploration
Condition: 0.8 < grad_ratio < 1.2
```

**Phase 4: Ultra-Exploration (Epochs 250+)**
```
Ï = 0.7          # Maintain coupling
Ï„_A: boosted amplitude (0.1 â†’ 0.5)
Ï„_B: 0.2 â†’ 0.3
Goal: Discover rare operations
```

### Mathematical Justification

**Theorem 3 (Phase Progression)**:
Under the phase schedule:
1. Phase 1 establishes **independent basins** in latent space
2. Phase 2 creates **bridges** between basins
3. Phase 3 enables **coordinated search** across basins
4. Phase 4 refines **rare operation coverage**

### Temperature Schedules

**VAE-A (Cyclic)**:
```
Ï„_A(t) = Ï„_base(t) + A(t) Â· cos(2Ï€t/T + Ï€/2)

where:
  Ï„_base(t) = 1.0 + (0.3 - 1.0) Â· t/T_total
  A(t) = 0.1 Â· Ï„_base(t)     for t < 250
         0.5                  for t â‰¥ 250 (Phase 4)
  T = 30 epochs (cycle period)
```

**VAE-B (Monotonic with Phase 4 boost)**:
```
Ï„_B(t) = 0.9 + (0.2 - 0.9) Â· (t-30)/T_total    for t < 250
         0.3                                    for t â‰¥ 250
```

### Beta (KL Weight) Schedules

**VAE-A**:
```
Î²_A(t) = 0.6 + (1.0 - 0.6) Â· t/T_total
```

**VAE-B (Phase-Lagged)**:
```
Î²_B(t) = Î²_A(t) Â· |sin(Ï€/4)|  â‰ˆ 0.707 Â· Î²_A(t)
```

The phase lag prevents both VAEs from being under-regularized simultaneously.

---

## Convergence Guarantees

### Theorem 4 (Eventual Coverage)

Under the following conditions:
1. Sufficient model capacity (latent dim â‰¥ 16)
2. Temperature annealing to Ï„_min > 0
3. Phase-scheduled Ï progression
4. Gradient balance maintenance

The dual-VAE system achieves:
```
lim_{tâ†’âˆ} Coverage(t) â‰¥ C_min

where C_min â‰ˆ 95% (empirically observed)
```

**Sketch of Proof**:
- Phase 1 establishes independent search spaces
- Phase 2-3 expand coverage through coupling
- Phase 4's temperature boost enables rare operation discovery
- Stop-gradient prevents collapse to single mode
- Entropy alignment forces similar exploration levels

### Empirical Validation

**Ternary VAE v5.5 Results**:
- **Coverage at epoch 399**: 97.64% (VAE-A), 97.67% (VAE-B)
- **Peak coverage**: 100% achieved 12 times (VAE-A), 8 times (VAE-B)
- **Stability**: No catastrophic forgetting, monotonic improvement

---

## Summary

The Ternary VAE v5.5 achieves near-complete operation coverage through:

1. **Dual pathways** with complementary exploration/exploitation strategies
2. **Stop-gradient cross-injection** for controlled information sharing
3. **StateNet meta-learning** for adaptive hyperparameter optimization
4. **Phase-scheduled training** for systematic coverage expansion
5. **Gradient balancing** to prevent pathway dominance
6. **Entropy alignment** to maintain exploration levels

**Key Insight**: Complete coverage requires **both diversity (VAE-A) and consolidation (VAE-B)**, coordinated through controlled coupling and meta-learned adaptation.

---

## References

1. Kingma & Welling (2014). "Auto-Encoding Variational Bayes"
2. Higgins et al. (2017). "Î²-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework"
3. Chen et al. (2018). "Isolating Sources of Disentanglement in VAEs"
4. Dupont (2018). "Learning Disentangled Joint Continuous and Discrete Representations"

---

**Next**: See [DUAL_VAE_ARCHITECTURE.md](DUAL_VAE_ARCHITECTURE.md) for implementation details.
