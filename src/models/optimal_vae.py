# Copyright 2024-2025 AI Whisperers (https://github.com/Ai-Whisperers)
#
# Licensed under the PolyForm Noncommercial License 1.0.0
# See LICENSE file in the repository root for full license text.

"""Optimal VAE Configuration - Based on Comprehensive Analysis (150+ experiments).

This module provides the optimal VAE configuration discovered through
comprehensive parallel testing. Key findings (REVISED 2025-12-27):

1. **Training matters most**: beta=0.1 with cyclical schedule is critical
2. **P-adic + radial losses synergize**: triplet + monotonic gives +0.55 correlation
3. **TropicalHyperbolicVAE**: Best hybrid architecture (+0.47 correlation, 88% accuracy)
4. **Higher padic_weight (0.5)**: Significantly improves structure preservation

Results (Spearman correlation with 3-adic distance):
- Previous best (hyperbolic only): +0.02
- NEW best (triplet + monotonic + cyclical): +0.5465 (+27x improvement!)
- TropicalHyperbolicVAE: +0.4678

The key insight is that cyclical beta schedule allows both good reconstruction
AND structure learning by alternating between exploration and refinement.

Usage:
    from src.models.optimal_vae import OptimalVAE, OptimalVAEConfig, get_optimal_config

    # For best correlation:
    config = get_optimal_config(mode="correlation")
    model = OptimalVAE(config)

    # For best accuracy:
    config = get_optimal_config(mode="accuracy")
    model = OptimalVAE(config)

    # For balanced performance:
    config = get_optimal_config(mode="balanced")
    model = OptimalVAE(config)
"""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Any, Dict, List, Literal, Optional

import numpy as np
import torch
import torch.nn as nn

from .simple_vae import SimpleVAEWithHyperbolic


@dataclass
class OptimalVAEConfig:
    """Configuration for the optimal VAE based on comprehensive analysis.

    These settings were determined through 150+ experiments on the ternary
    operations dataset (19,683 samples).

    REVISED findings (2025-12-27):
    - beta=0.1 with cyclical schedule: Essential for high correlation
    - padic_ranking + radial_loss: Synergistic combination (+0.55 correlation)
    - padic_weight=0.5, radial_weight=0.3: Optimal loss weights
    - learning_rate=0.005: Higher than default, works better
    - TropicalHyperbolic architecture: Best for combining accuracy + correlation
    """

    # Model architecture
    model_type: str = "tropical_hyperbolic"  # "simple", "simple_hyperbolic", "tropical_hyperbolic"
    input_dim: int = 9
    latent_dim: int = 16
    hidden_dims: List[int] = field(default_factory=lambda: [64, 32])
    dropout: float = 0.0

    # Hyperbolic geometry
    enable_hyperbolic: bool = True
    curvature: float = 1.0

    # Tropical geometry (for tropical_hyperbolic model)
    temperature: float = 0.05  # Lower = harder max (better structure)

    # P-adic ranking loss (RECOMMENDED: True with triplet)
    enable_padic_ranking: bool = True
    padic_loss_type: str = "triplet"  # "triplet", "soft_ranking", "geodesic"
    padic_weight: float = 0.5  # Higher than default - helps correlation
    padic_margin: float = 0.1
    padic_n_triplets: int = 500

    # Radial loss (RECOMMENDED: True with monotonic)
    enable_radial_loss: bool = True
    radial_loss_type: str = "monotonic"  # "monotonic", "hierarchy", "global_rank"
    radial_weight: float = 0.3

    # Training settings (REVISED)
    learning_rate: float = 0.005  # Higher than default
    weight_decay: float = 0.0  # Not needed
    batch_size: int = 256
    epochs: int = 100

    # Beta schedule (CRITICAL)
    beta: float = 0.1  # Higher than previous (was 0.01)
    beta_schedule: str = "cyclical"  # "constant", "warmup", "cyclical"
    beta_cycle_period: int = 50  # Epochs per cycle

    # Early stopping
    early_stopping: bool = False
    patience: int = 15


class OptimalVAE(nn.Module):
    """Optimal VAE with configurable architecture and losses.

    This is the recommended model configuration based on comprehensive testing.
    Achieves up to +0.55 Spearman correlation with 3-adic distance.

    The model supports multiple architectures:
    - simple: Basic VAE (baseline)
    - simple_hyperbolic: VAE with hyperbolic projection
    - tropical_hyperbolic: Hybrid tropical + hyperbolic (best)

    Args:
        config: OptimalVAEConfig with model settings
    """

    def __init__(self, config: Optional[OptimalVAEConfig] = None):
        super().__init__()

        if config is None:
            config = OptimalVAEConfig()

        self.config = config

        # Create model based on type
        if config.model_type == "tropical_hyperbolic":
            from .tropical_hyperbolic_vae import TropicalHyperbolicVAE

            self.vae = TropicalHyperbolicVAE(
                input_dim=config.input_dim,
                latent_dim=config.latent_dim,
                hidden_dims=config.hidden_dims,
                curvature=config.curvature,
                temperature=config.temperature,
            )
        elif config.model_type == "simple_hyperbolic":
            self.vae = SimpleVAEWithHyperbolic(
                input_dim=config.input_dim,
                latent_dim=config.latent_dim,
                hidden_dims=config.hidden_dims,
                dropout=config.dropout,
                curvature=config.curvature,
            )
        else:
            from .simple_vae import SimpleVAE

            self.vae = SimpleVAE(
                input_dim=config.input_dim,
                latent_dim=config.latent_dim,
                hidden_dims=config.hidden_dims,
                dropout=config.dropout,
            )

        # P-adic ranking loss
        self.padic_loss = None
        if config.enable_padic_ranking:
            if config.padic_loss_type == "triplet":
                from src.losses.padic import PAdicRankingLoss

                self.padic_loss = PAdicRankingLoss(
                    margin=config.padic_margin,
                    n_triplets=config.padic_n_triplets,
                )
            elif config.padic_loss_type == "soft_ranking":
                self.padic_loss = SoftPadicRankingLoss(temperature=0.5)
            elif config.padic_loss_type == "geodesic":
                from src.losses.padic_geodesic import PAdicGeodesicLoss

                self.padic_loss = PAdicGeodesicLoss(curvature=config.curvature)

        # Radial loss
        self.radial_loss = None
        if config.enable_radial_loss:
            if config.radial_loss_type == "monotonic":
                from src.losses.padic_geodesic import MonotonicRadialLoss

                self.radial_loss = MonotonicRadialLoss()
            elif config.radial_loss_type == "hierarchy":
                from src.losses.padic_geodesic import RadialHierarchyLoss

                self.radial_loss = RadialHierarchyLoss()
            elif config.radial_loss_type == "global_rank":
                from src.losses.padic_geodesic import GlobalRankLoss

                self.radial_loss = GlobalRankLoss()

    def forward(self, x: torch.Tensor) -> Dict[str, Any]:
        """Forward pass through the VAE."""
        return self.vae(x)

    def get_beta(self, epoch: int) -> float:
        """Get beta value based on schedule."""
        c = self.config
        if c.beta_schedule == "warmup":
            return c.beta * min(1.0, epoch / 20)
        elif c.beta_schedule == "cyclical":
            return c.beta * (0.5 + 0.5 * np.sin(2 * np.pi * epoch / c.beta_cycle_period))
        else:
            return c.beta

    def compute_loss(
        self,
        x: torch.Tensor,
        batch_indices: torch.Tensor,
        recon_loss_fn,
        kl_loss_fn,
        epoch: int = 0,
    ) -> tuple:
        """Compute total loss including p-adic and radial losses.

        Args:
            x: Input tensor (batch, input_dim)
            batch_indices: Operation indices for p-adic distance
            recon_loss_fn: Reconstruction loss function
            kl_loss_fn: KL divergence loss function
            epoch: Current epoch (for beta schedule)

        Returns:
            Tuple of (total_loss, loss_dict)
        """
        outputs = self.forward(x)

        logits = outputs["logits"]
        mu = outputs["mu"]
        logvar = outputs["logvar"]
        z = outputs.get("z_euc", outputs["z"])

        # Base VAE loss
        recon_loss = recon_loss_fn(logits, x)
        kl_loss = kl_loss_fn(mu, logvar)
        beta = self.get_beta(epoch)
        total_loss = recon_loss + beta * kl_loss

        loss_dict = {
            "recon": recon_loss.item(),
            "kl": kl_loss.item(),
            "beta": beta,
        }

        # P-adic ranking loss
        if self.padic_loss is not None:
            padic_out = self.padic_loss(z, batch_indices)
            if isinstance(padic_out, tuple):
                padic_loss = padic_out[0]
            else:
                padic_loss = padic_out
            total_loss = total_loss + self.config.padic_weight * padic_loss
            loss_dict["padic"] = padic_loss.item()

        # Radial loss
        if self.radial_loss is not None:
            radial_out = self.radial_loss(z, batch_indices)
            if isinstance(radial_out, tuple):
                radial_loss = radial_out[0]
            else:
                radial_loss = radial_out
            total_loss = total_loss + self.config.radial_weight * radial_loss
            loss_dict["radial"] = radial_loss.item()

        loss_dict["total"] = total_loss.item()

        return total_loss, loss_dict

    def encode(self, x: torch.Tensor) -> torch.Tensor:
        """Encode input to latent space."""
        return self.vae.encode(x)

    def reconstruct(self, x: torch.Tensor) -> torch.Tensor:
        """Reconstruct input."""
        return self.vae.reconstruct(x)

    def count_parameters(self) -> dict:
        """Count model parameters."""
        total = sum(p.numel() for p in self.parameters())
        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)
        return {"total": total, "trainable": trainable}


class SoftPadicRankingLoss(nn.Module):
    """Soft p-adic ranking using KL divergence between rank distributions."""

    def __init__(self, temperature: float = 0.5, n_samples: int = 200):
        super().__init__()
        self.temperature = temperature
        self.n_samples = n_samples

    def compute_padic_distance(self, i: int, j: int) -> float:
        if i == j:
            return 0.0
        diff = abs(i - j)
        k = 0
        while diff % 3 == 0:
            diff //= 3
            k += 1
        return 3.0 ** (-k)

    def forward(self, z: torch.Tensor, indices: torch.Tensor) -> torch.Tensor:
        n = z.size(0)
        if n < 3:
            return torch.tensor(0.0, device=z.device)

        if n > self.n_samples:
            idx = torch.randperm(n)[: self.n_samples]
            z = z[idx]
            indices = indices[idx]
            n = self.n_samples

        latent_dist = torch.cdist(z, z)
        padic_dist = torch.zeros(n, n, device=z.device)
        for i in range(n):
            for j in range(n):
                padic_dist[i, j] = self.compute_padic_distance(indices[i].item(), indices[j].item())

        import torch.nn.functional as F

        latent_ranks = F.softmax(-latent_dist / self.temperature, dim=1)
        padic_ranks = F.softmax(-padic_dist / self.temperature, dim=1)

        return F.kl_div(latent_ranks.log(), padic_ranks, reduction="batchmean")


def get_optimal_config(mode: Literal["correlation", "accuracy", "balanced"] = "balanced") -> OptimalVAEConfig:
    """Get the recommended optimal configuration.

    This configuration was determined through comprehensive analysis
    (150+ experiments) achieving up to +0.55 Spearman correlation.

    Args:
        mode: Configuration mode
            - "correlation": Maximize structure preservation (+0.55 correlation)
            - "accuracy": Maximize reconstruction (96%+ accuracy)
            - "balanced": Good balance of both (~89% acc, +0.42 corr)

    Returns:
        OptimalVAEConfig for the specified mode
    """
    if mode == "correlation":
        # Best for structure preservation
        return OptimalVAEConfig(
            model_type="simple_hyperbolic",
            enable_padic_ranking=True,
            padic_loss_type="triplet",
            padic_weight=0.5,
            enable_radial_loss=True,
            radial_loss_type="monotonic",
            radial_weight=0.3,
            beta=0.1,
            beta_schedule="cyclical",
            learning_rate=0.005,
            epochs=100,
        )

    elif mode == "accuracy":
        # Best for reconstruction accuracy
        return OptimalVAEConfig(
            model_type="tropical_hyperbolic",
            temperature=0.05,
            enable_padic_ranking=False,
            enable_radial_loss=False,
            beta=0.1,
            beta_schedule="cyclical",
            learning_rate=0.005,
            epochs=100,
        )

    else:  # balanced (default)
        # Best balance of accuracy and correlation
        return OptimalVAEConfig(
            model_type="tropical_hyperbolic",
            temperature=0.05,
            enable_padic_ranking=True,
            padic_loss_type="triplet",
            padic_weight=0.5,
            enable_radial_loss=True,
            radial_loss_type="monotonic",
            radial_weight=0.3,
            beta=0.1,
            beta_schedule="cyclical",
            learning_rate=0.005,
            epochs=100,
        )


__all__ = [
    "OptimalVAE",
    "OptimalVAEConfig",
    "SoftPadicRankingLoss",
    "get_optimal_config",
]
