# Copyright 2024-2025 AI Whisperers (https://github.com/Ai-Whisperers)
#
# Licensed under the PolyForm Noncommercial License 1.0.0
# See LICENSE file in the repository root for full license text.
#
# For commercial licensing inquiries: support@aiwhisperers.com

"""Geometric Loss for Nanoparticle Scaffolding.

This module implements the `GeometricAlignmentLoss` (Proposal 1), which
enforces symmetries in the latent space corresponding to vaccine scaffolds
(e.g., Ferritin 24-mer, mi3 60-mer).

Key Concepts:
- Target Geometries: Tetrahedral (4), Octahedral (6), Icosahedral (12/20/60).
- Latent Alignment: Minimizing RMSD between latent configuration and target vertices.
- Poincare Compatibility: Can operate in Euclidean or Hyperbolic space (via LogMap).

Usage:
    from src.losses.geometric_loss import GeometricAlignmentLoss
    criterion = GeometricAlignmentLoss(symmetry_group='icosahedral')
    loss, metrics = criterion(z_latent)
"""

import math
from typing import Dict, Optional, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F


class GeometricAlignmentLoss(nn.Module):
    """Enforces geometric symmetry constraints on latent representations.

    Designed to align vaccine antigens with nanoparticle scaffolds.
    """

    def __init__(
        self,
        symmetry_group: str = "point_24",  # ferritin-like
        scale: float = 1.0,
        alignment_weight: float = 1.0,
        regularization_weight: float = 0.1,
    ):
        """Initialize GeometricAlignmentLoss.

        Args:
            symmetry_group: Type of symmetry to enforce ('tetrahedral', 'octahedral', 'icosahedral', 'point_24')
            scale: Scaling factor for the target geometry radius
            alignment_weight: Weight for the main RMSD alignment loss
            regularization_weight: Weight for spacing/repulsion regularization
        """
        super().__init__()
        self.symmetry_group = symmetry_group
        self.scale = scale
        self.alignment_weight = alignment_weight
        self.regularization_weight = regularization_weight

        # Precompute target vertices
        self.register_buffer("target_vertices", self._generate_target_vertices(symmetry_group))

    def _generate_target_vertices(self, group: str) -> torch.Tensor:
        """Generate normalized vertices for standard polyhedra."""
        if group == "tetrahedral":
            # 4 vertices
            vertices = torch.tensor(
                [[1, 1, 1], [1, -1, -1], [-1, 1, -1], [-1, -1, 1]],
                dtype=torch.float32,
            )

        elif group == "octahedral":
            # 6 vertices
            vertices = torch.tensor(
                [
                    [1, 0, 0],
                    [-1, 0, 0],
                    [0, 1, 0],
                    [0, -1, 0],
                    [0, 0, 1],
                    [0, 0, -1],
                ],
                dtype=torch.float32,
            )

        elif group == "icosahedral":
            # 12 vertices (standard)
            phi = (1 + math.sqrt(5)) / 2
            vertices = torch.tensor(
                [
                    [-1, phi, 0],
                    [1, phi, 0],
                    [-1, -phi, 0],
                    [1, -phi, 0],
                    [0, -1, phi],
                    [0, 1, phi],
                    [0, -1, -phi],
                    [0, 1, -phi],
                    [phi, 0, -1],
                    [phi, 0, 1],
                    [-phi, 0, -1],
                    [-phi, 0, 1],
                ],
                dtype=torch.float32,
            )

        elif group == "point_24":
            # 24 points (e.g. Ferritin-like cubic symmetry variants or refined spherical code)
            # For simplicity, we use a spherical Fibonacci lattice for N=24 as a generic "even spacing" target
            # if specific crystallographic coords are not provided.
            # Here we implement a flexible spherical code approach or hardcoded approximate.
            # Let's use the vertices of a truncated octahedron or similar?
            # For this MVP, we'll use a Spherical Fibonacci Lattice generator for arbitrary N.
            return self._spherical_fibonacci_lattice(24)

        else:
            raise ValueError(f"Unknown symmetry group: {group}")

        return F.normalize(vertices, p=2, dim=1)

    def _spherical_fibonacci_lattice(self, n: int) -> torch.Tensor:
        """Generate N evenly spaced points on a sphere (Fibonacci Lattice)."""
        idx = torch.arange(0, n, dtype=torch.float32) + 0.5
        phi = math.pi * (1 + math.sqrt(5))

        y = 1 - 2 * idx / n
        radius = torch.sqrt(1 - y * y)
        theta = phi * idx

        # Generate points on sphere using Fibonacci lattice
        points = torch.stack([torch.cos(theta) * radius, y, torch.sin(theta) * radius], dim=1)

        return points

    def forward(self, z: torch.Tensor, target_indices: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, Dict]:
        """Compute alignment loss.

        Args:
            z: Latent batch (Batch, Dim). If Dim > 3, uses first 3 dims or projects?
               Assumption: We project to 3D for geometric alignment, or z is ALREADY 3D structural embedding.
               For VAEs with dim=16, we typically assume a specific subspace is the "geometric" one.
               For this MVP, we'll assume we align the first 3 dimensions.
            target_indices: Optional explicit mapping of batch items to target vertices.

        Returns:
            loss, metrics
        """
        z_3d = z[:, :3]  # Take first 3 dimensions
        z_3d = F.normalize(z_3d, dim=1) * self.scale  # Project to sphere of radius `scale`

        # If we don't have explicit targets, we solve the assignment problem (Soft Assign or Chamfer)
        # For 'scaffolding', we usually want the BATCH to cover the SCAFFOLD.
        # Simple Chamfer Distance:
        # For each target vertex, find closest latent point.
        # For each latent point, find closest target vertex.

        # Dist matrix: (Batch, N_Targets)
        dists = torch.cdist(z_3d, self.target_vertices * self.scale)

        # 1. Coverage Loss: Every target vertex must be close to SOME latent point
        min_dist_to_target, _ = torch.min(dists, dim=0)  # (N_Targets,)
        coverage_loss = torch.mean(min_dist_to_target**2)

        # 2. Alignment Loss: Every latent point must represent SOME target vertex
        min_dist_to_latent, _ = torch.min(dists, dim=1)  # (Batch,)
        alignment_loss = torch.mean(min_dist_to_latent**2)

        # 3. Regularization: Spread (maximize pairwise distance between batch items to prevent collapse)
        # Only needed if we want to fill the space.
        # Inverse pairwise distance?

        total_loss = self.alignment_weight * alignment_loss + self.alignment_weight * coverage_loss  # Weight both equally for now

        metrics = {
            "geo_coverage_loss": coverage_loss.item(),
            "geo_alignment_loss": alignment_loss.item(),
            "mean_rmsd": torch.sqrt(alignment_loss).item(),
        }

        return total_loss, metrics
