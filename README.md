# Ternary VAE v5.11: Pure Hyperbolic Geometry

> **Unified 3-Adic Embedding with Frozen Encoder Coverage**

![Version](https://img.shields.io/badge/version-5.11.0-blue)
![Coverage](https://img.shields.io/badge/coverage-100%25-green)
![Geometry](https://img.shields.io/badge/geometry-hyperbolic-purple)

This repository implements **Ternary VAE v5.11**, a variational autoencoder that learns the structure of ternary logic operations (3^9 space).
It uses a **Frozen Encoder** (v5.5 weights) to guarantee 100% coverage, while learning a **Hyperbolic (Poincar√©) Projection** to capture the hierarchical nature of 3-adic numbers.

---

## What's New in v5.11

**Frozen Encoder & Unified 3-Adic Embedding** - Focus on stable, 100% coverage with hyperbolic projection:

- **FrozenEncoderVAETrainer**: Specialized trainer for Poincare ball embedding with a fixed encoder
- **UnifiedHyperbolicPrior**: Self-regulating sigma based on centroid drift, now unified for both pathways
- **StateNet v4**: Enhanced meta-controller with orbital tracking (unchanged from v5.10)
- **3-adic Ranking Loss**: Explicit preservation of ultrametric structure (unchanged from v5.10)
- **DualNeuralVAEV5_11**: Full inheritance chain (v5.6 ‚Üí v5.7 ‚Üí v5.10 ‚Üí v5.11)

---

## Overview

The Ternary VAE v5.10.1 is a **dual-pathway variational autoencoder** that embeds all possible 19,683 ternary operations into a **16-dimensional Poincare ball** (hyperbolic space). It achieves this through:

1. **Hyperbolic geometry**: Native embedding in constant negative curvature space
2. **3-adic ultrametric preservation**: Ranking loss for tree-structured distances
3. **Homeostatic prior**: Self-regulating sigma based on manifold dynamics
4. **StateNet v4 meta-controller**: Adaptive hyperparameter optimization

### What Problem Does This Solve?

**Problem**: How can a neural network learn to represent **all possible** ternary logic operations without collapsing to a subset or losing diversity?

**Solution**: Dual-pathway architecture where:

- **VAE-A** explores chaotically with high temperature and entropy
- **VAE-B** consolidates discoveries with residual connections
- **StateNet** adapts training dynamics based on system state
- **Phase scheduling** guides progression from isolation ‚Üí coupling ‚Üí ultra-exploration

**Result**: 97.6%+ coverage of the entire 19,683-operation space with stable, reproducible training.

---

## Mathematical Definition: 3^9 Operations and the Ternary Manifold

### The 3^9 Operation Space

The **3^9 = 19,683 operations** are the complete set of binary functions over the ternary field:

```text
f: Z‚ÇÉ √ó Z‚ÇÉ ‚Üí Z‚ÇÉ   where Z‚ÇÉ = {-1, 0, +1}
```

Each operation is a **lookup table (LUT)** of 9 values‚Äîone output for each of the 9 possible input pairs `(a, b)` where `a, b ‚àà {-1, 0, +1}`. The canonical ordering:

| Input (a,b) | (-1,-1) | (-1,0) | (-1,+1) | (0,-1) | (0,0) | (0,+1) | (+1,-1) | (+1,0) | (+1,+1) |
| ----------- | ------- | ------ | ------- | ------ | ----- | ------ | ------- | ------ | ------- |
| LUT index   | 0       | 1      | 2       | 3      | 4     | 5      | 6       | 7      | 8       |

Each operation is indexed by `i ‚àà [0, 19682]` via its **base-3 representation**:

```text
i = Œ£‚Çñ (digit_k + 1) √ó 3^k   where digit_k ‚àà {-1, 0, +1}
```

### 3-Adic Ultrametric Structure

This indexing naturally endows the operation space with a **3-adic ultrametric**:

```text
d(i, j) = 3^(-v‚ÇÉ(|i-j|))
```

where `v‚ÇÉ(n)` is the **3-adic valuation**‚Äîthe largest power of 3 dividing `n`. Operations sharing `k` leading digits in base-3 are exactly `3^(-k)` apart.

| Shared Digits | Distance | Cluster Size | Interpretation          |
| ------------- | -------- | ------------ | ----------------------- |
| 0             | 1        | 19,683       | Different root branches |
| 1             | 1/3      | 6,561        | Same first output       |
| 2             | 1/9      | 2,187        | Same first row          |
| 3             | 1/27     | 729          | Same first 3 outputs    |
| ...           | ...      | ...          | ...                     |
| 9             | 0        | 1            | Identical operations    |

This makes the space **isomorphic to Z/3‚ÅπZ** with p-adic topology‚Äîa **9-level ternary tree** where leaves are individual operations and internal nodes are prefix-sharing clusters (**fibers**).

### The Ternary Manifold

The **ternary manifold** is the image of this discrete ultrametric space under the VAE encoder:

```text
Encoder: Z‚ÇÉ‚Åπ ‚Üí B¬π‚Å∂_Poincare   (19,683 points ‚Üí 16D Poincare ball)
```

The **Poincare ball** is hyperbolic space of constant negative curvature, where:

- **Geodesic distance** grows exponentially toward the boundary
- **Hierarchical structures** embed naturally (root at origin, leaves near boundary)
- **Ultrametric distances** are preserved isometrically when the embedding is correct

The training objective is for **Poincare geodesic distance to match 3-adic distance**, making the learned manifold an **isometric embedding** of the ternary operation tree.

---

## Quick Start

### Installation

```bash
cd ternary-vaes
pip install -r requirements.txt
```

### Training (GPU-Optimized)

```bash
# Recommended: GPU-resident dataset (zero CPU-GPU transfers)
python scripts/train/train_purposeful.py --config configs/ternary_v5_10.yaml --model-version v5.10 --gpu-resident

# Or use config default (gpu_resident: true in v5.10 config)
python scripts/train/train_purposeful.py --config configs/ternary_v5_10.yaml --model-version v5.10
```

### TensorBoard Visualization

```bash
# Launch dashboard (in separate terminal)
tensorboard --logdir runs

# Open http://localhost:6006 in browser
```

### Benchmarking

```bash
# Measure manifold resolution (discrete‚Üícontinuous mapping quality)
python scripts/benchmark/measure_manifold_resolution.py

# Results saved to: reports/benchmarks/manifold_resolution_{epoch}.json
```

---

## Project Structure (Refactored)

```text
ternary-vaes/
‚îú‚îÄ‚îÄ README.md                      # This file
‚îú‚îÄ‚îÄ DOCUMENTATION/                 # üìö KNOWLEDGE BASE
‚îú‚îÄ‚îÄ DOCUMENTATION/                 # üìö KNOWLEDGE BASE
‚îÇ   ‚îú‚îÄ‚îÄ 01_STAKEHOLDER_RESOURCES/  # Guides, Pitch Decks, & Technical Specs
‚îÇ   ‚îú‚îÄ‚îÄ 02_THEORY_AND_RESEARCH/    # Theory, Reports, Analysis
‚îÇ   ‚îú‚îÄ‚îÄ 03_EXPERIMENTS_AND_LABS/   # Experimental Code (Bioinformatics, Mathematics)
‚îÇ   ‚îú‚îÄ‚îÄ 04_PROJECT_MANAGEMENT/     # Plans & Archives
‚îÇ   ‚îî‚îÄ‚îÄ 06_LEGAL/                  # Legal
‚îÇ
‚îú‚îÄ‚îÄ src/                           # üß† LOGIC (Production Code)
‚îÇ   ‚îú‚îÄ‚îÄ training/                  # Training components
‚îÇ   ‚îú‚îÄ‚îÄ models/                    # Neural architectures
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ

‚îÇ
‚îú‚îÄ‚îÄ results/                       # üìä RESULTS & OUTPUTS
‚îÇ   ‚îú‚îÄ‚îÄ benchmarks/                # Performance metrics
‚îÇ   ‚îú‚îÄ‚îÄ discoveries/               # Key findings (e.g., HIV, Glycan Shield)
‚îÇ   ‚îî‚îÄ‚îÄ training_runs/             # Tensorboard logs
‚îÇ
‚îú‚îÄ‚îÄ configs/                       # ‚öôÔ∏è CONFIGURATION
‚îî‚îÄ‚îÄ tests/                         # Test suite
```

---

## Architecture Overview

### Modular Components

The refactored architecture follows Single Responsibility Principle:

| Module                | Responsibility            | Lines | Status        |
| --------------------- | ------------------------- | ----- | ------------- |
| **TernaryVAETrainer** | Orchestrate training loop | 350   | ‚úÖ Production |
| **DualVAELoss**       | Compute all losses        | 259   | ‚úÖ Production |
| **Schedulers**        | Schedule temp/beta/LR     | 211   | ‚úÖ Production |
| **TrainingMonitor**   | Log and track metrics     | 198   | ‚úÖ Production |
| **CheckpointManager** | Save/load checkpoints     | 136   | ‚úÖ Production |
| **Data Module**       | Generate/load data        | 141   | ‚úÖ Production |
| **Model**             | Define architecture       | 499   | ‚úÖ Production |

**Total**: ~2,000 lines of clean, testable, modular code

### Key Benefits

‚úÖ **Testability**: Each component can be tested independently
‚úÖ **Maintainability**: Easy to modify without breaking other components
‚úÖ **Reusability**: Modules can be used in other projects
‚úÖ **Extensibility**: Simple to add new features
‚úÖ **Clarity**: Clear dependencies and interfaces

---

## Key Features

### 1. Dual-Pathway Architecture

- **VAE-A (Chaotic Regime)**: 50,203 parameters, high temperature, exploratory
- **VAE-B (Frozen Regime)**: 117,499 parameters, residual connections, conservative
- **Stop-Gradient Cross-Injection**: Controlled information flow with permeability œÅ

### 2. StateNet Meta-Controller

- **1,068 parameters** (0.63% overhead)
- Learns to adapt learning rate and loss weights based on training state
- Input: [H_A, H_B, KL_A, KL_B, grad_ratio, œÅ, Œª‚ÇÅ, Œª‚ÇÇ, Œª‚ÇÉ]
- Output: Corrections [Œîlr, ŒîŒª‚ÇÅ, ŒîŒª‚ÇÇ, ŒîŒª‚ÇÉ]

### 3. Phase-Scheduled Training

- **Phase 1 (0-40)**: Isolation (œÅ=0.1)
- **Phase 2 (40-120)**: Consolidation (œÅ‚Üí0.3)
- **Phase 3 (120-250)**: Resonant Coupling (œÅ‚Üí0.7, gated on gradient balance)
- **Phase 4 (250+)**: Ultra-Exploration (œÅ=0.7, temperature boost)

### 4. Adaptive Gradient Balancing

- EMA tracking of gradient norms for VAE-A and VAE-B
- Dynamic scaling factors to maintain balance
- Momentum adaptation based on gradient ratio

### 5. Deterministic Reproducibility

- Fixed random seeds across ```text
  PyTorch: 2.x.x
  CUDA: True

```text`

- Deterministic CUDA operations
- Checkpoint includes full optimizer state
  -# 2. Configure
  cp configs/env.example .env
- Configuration-driven (no magic numbers in code)

### 6. Ensemble Prediction

- **100% reconstruction accuracy** by combining both VAEs
- Three strategies: voting, confidence-weighted, best-of-two
- Leverages complementary strengths: VAE-A (exploration), VAE-B (precision)
- Cross-injection increases coverage: 84.80% vs 77.55% (best isolated)
- Validated at epoch 3, maintains superiority throughout training

### 7. GPU-First Training Optimizations (v5.10.2)

**Zero-transfer architecture** for maximum GPU utilization:

| Optimization             | Before                         | After              | Speedup       |
| ------------------------ | ------------------------------ | ------------------ | ------------- |
| **GPU-Resident Dataset** | 77 CPU‚ÜíGPU transfers/epoch     | 0 transfers        | ~15% faster   |
| **Async Checkpoints**    | Blocking I/O (3x per interval) | Background thread  | Non-blocking  |
| **TensorBoard Flush**    | Per-metric flush               | Single epoch flush | -I/O overhead |
| **Valuation LUT**        | O(9) loop per index            | O(1) tensor lookup | ~10x faster   |
| **Coverage Eval**        | Python loop + CPU sync         | torch.unique (GPU) | ~100x faster  |

**Key components:**

- `src/core/ternary.py` - TERNARY singleton with precomputed LUTs
- `src/data/gpu_resident.py` - GPU-resident dataset (~865 KB on GPU)
- `src/artifacts/checkpoint_manager.py` - AsyncCheckpointSaver
- `src/observability/` - Decoupled metrics layer

**Enable via config:**

```yaml
gpu_resident: true # All 19,683 samples on GPU
```

---

## Usage Examples

### Basic Training (Refactored API)

```python
import torch
import yaml
from pathlib import Path

# Import modular components
from src.models.ternary_vae_v5_6 import DualNeuralVAEV5
from src.training import TernaryVAETrainer
from src.data import generate_all_ternary_operations, TernaryOperationDataset
from torch.utils.data import DataLoader, random_split

# Load configuration
with open('configs/ternary_v5_6.yaml') as f:
    config = yaml.safe_load(f)

# Generate dataset
operations = generate_all_ternary_operations()
dataset = TernaryOperationDataset(operations)

# Split data
train_size = int(0.8 * len(dataset))
val_size = int(0.1 * len(dataset))
test_size = len(dataset) - train_size - val_size
train_dataset, val_dataset, test_dataset = random_split(
    dataset, [train_size, val_size, test_size]
)

# Create data loaders
train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)

# Initialize model
model = DualNeuralVAEV5(
    input_dim=9,
    latent_dim=16,
    rho_min=0.1,
    rho_max=0.9,
    lambda3_base=0.3,
    lambda3_amplitude=0.15,
    eps_kl=0.01,
    gradient_balance=True,
    adaptive_scheduling=True,
    use_statenet=True
)

# Initialize trainer (dependency injection)
trainer = TernaryVAETrainer(model, config, device='cuda')

# Train
trainer.train(train_loader, val_loader)
```

### Using Individual Components

```python
# Use loss computation independently
from src.losses import DualVAELoss, KLDivergenceLoss

loss_fn = DualVAELoss(free_bits=0.5, repulsion_sigma=0.5)
kl_loss = KLDivergenceLoss(free_bits=0.5)

# Use schedulers independently
from src.training import TemperatureScheduler, BetaScheduler

temp_scheduler = TemperatureScheduler(config, phase_4_start=200, temp_lag=5)
beta_scheduler = BetaScheduler(config, beta_phase_lag=1.5708)

# Use data generation independently
from src.data import generate_all_ternary_operations

operations = generate_all_ternary_operations()  # All 19,683 operations
```

### Loading Checkpoints

```python
from src.artifacts import CheckpointManager

checkpoint_manager = CheckpointManager(
    checkpoint_dir=Path('artifacts/raw/dual_vae_v5_5'),
    checkpoint_freq=10
)

# Load best checkpoint
checkpoint = checkpoint_manager.load_checkpoint(
    model=model,
    optimizer=optimizer,
    checkpoint_name='best',
    device='cuda'
)

print(f"Loaded epoch {checkpoint['epoch']}")
print(f"Best val loss: {checkpoint['best_val_loss']}")
```

---

## Performance Metrics

### Coverage (Best Checkpoint)

- **VAE-A**: 97.64% (19,218 / 19,683 operations)
- **VAE-B**: 97.67% (19,224 / 19,683 operations)
- **100% Epochs**: Reached 12 times (A), 8 times (B), 2 times (both)

### Training Stability

- **Epochs Trained**: 399/400 (complete run, no crashes)
- **Best Validation Loss**: -0.2562 (refactored), 1.814 (original)
- **Gradient Balance**: Achieved and maintained
- **No Catastrophic Forgetting**: Coverage increased monotonically

### Computational Efficiency

- **Training Time**: ~108s/epoch on CUDA (refactored)
- **Memory Usage**: ~2.1GB VRAM
- **Performance**: Zero regression vs original
- **Total Training Time**: ~2.5-3 hours for 400 epochs

### Code Quality

- **Model**: 632 ‚Üí 499 lines (-21%)
- **Trainer**: 398 ‚Üí 350 lines (-12%)
- **SRP Compliance**: 100%
- **Test Coverage**: 100% validation pass (15/15 tests)

### Manifold Resolution

**Critical Discovery**: The dual-VAE system achieves **100% perfect reconstruction** through ensemble prediction, despite individual VAE-A achieving only 14.87% accuracy at epoch 3.

#### Isolated VAE Performance (Epoch 3)

- **VAE-A**: 14.87% reconstruction | 77.55% coverage | 66.84% overall
- **VAE-B**: 100% reconstruction | 65.82% coverage | 88.87% overall
- **Combined**: 77.85% baseline resolution

#### Coupled System Performance (Epoch 3)

- **Ensemble Reconstruction**: **100%** (all strategies: voting, confidence-weighted, best-of-two)
- **Cross-Injected Sampling**: 84.80% coverage (rho=0.7)
- **Improvement**: +85.13pp reconstruction (vs VAE-A), +7.25pp coverage (vs best isolated)

#### Benchmarks Available

```bash
# Isolated VAE resolution (each VAE independently)
python scripts/benchmark/measure_manifold_resolution.py

# Coupled system resolution (both VAEs working together)
python scripts/benchmark/measure_coupled_resolution.py
```

#### Metrics Measured

- **Reconstruction Fidelity**: Exact match rate, bit-error distribution
- **Ensemble Strategies**: Voting, confidence-weighted, best-of-two
- **Sampling Coverage**: Unique operations from prior sampling (with cross-injection)
- **Latent Separation**: Pairwise distance statistics in latent space
- **Interpolation Quality**: Smoothness of latent-space interpolations (100% validity)
- **Nearest Neighbor**: Hamming distance preservation (perfect topology)
- **Complementarity**: VAE specialization analysis
- **Latent Coupling**: Cross-VAE correlation and alignment

**See**: `reports/benchmarks/RESOLUTION_COMPARISON.md` for detailed analysis.

---

## üìÇ Project Structure

This repository is organized to serve multiple professional audiences.

### `src/` - Production Code

The core Python package.

- `models/`: v5.11 Frozen Encoder + Hyperbolic Projection.
- `training/`: Specialized hyperbolic trainers.
- [Codebase Structure Guide](DOCUMENTATION/01_GUIDES/developers/CODEBASE_STRUCTURE.md)

### `DOCUMENTATION/` - Knowledge Base

## Jona's Research Roadmap

Detailed research notes and future directions can be found in the documentation:

- [Strategic Roadmap (Jona)](DOCUMENTATION/04_PROJECT_MANAGEMENT/active_plans/00_MASTER_ROADMAP_JONA.md)
- [Scientific Domains](DOCUMENTATION/02_THEORY_AND_RESEARCH/biology_context/SCIENTIFIC_DOMAINS.md)
- [Medical Frontiers](DOCUMENTATION/02_THEORY_AND_RESEARCH/biology_context/MEDICAL_FRONTIERS.md)
- [Relevant Repositories](DOCUMENTATION/02_THEORY_AND_RESEARCH/academic_output/RELEVANT_REPOS.md)
- [Suggested Libraries](DOCUMENTATION/01_STAKEHOLDER_RESOURCES/industry/technical_specs/DEPENDENCIES.md)

- **[01_STAKEHOLDER_RESOURCES](DOCUMENTATION/01_STAKEHOLDER_RESOURCES/)**: Guides & Presentations.

  - [Setup & Dependencies](DOCUMENTATION/01_STAKEHOLDER_RESOURCES/industry/technical_specs/SETUP.md)
  - [Workflows & Scripts](DOCUMENTATION/01_STAKEHOLDER_RESOURCES/industry/technical_specs/WORKFLOWS.md)
  - [Mathematical Foundations](DOCUMENTATION/01_STAKEHOLDER_RESOURCES/academic/theory_deep_dive/)
    - [Ternary VAE Formulation](DOCUMENTATION/01_STAKEHOLDER_RESOURCES/academic/theory_deep_dive/01_ternary_vae_formulation.md)
    - [StateNet Dynamics](DOCUMENTATION/01_STAKEHOLDER_RESOURCES/academic/theory_deep_dive/02_statenet_dynamics.md)
    - [Optimization & Convergence](DOCUMENTATION/01_STAKEHOLDER_RESOURCES/academic/theory_deep_dive/03_optimization_and_convergence.md)
  - [Dual VAE Architecture](DOCUMENTATION/01_STAKEHOLDER_RESOURCES/academic/code_walkthrough/DUAL_VAE_GUIDE.md)

- **[02_THEORY_AND_RESEARCH](DOCUMENTATION/02_THEORY_AND_RESEARCH/)**: Research & Context.
  - **Foundations**: [Conjectures](DOCUMENTATION/02_THEORY_AND_RESEARCH/foundations/CONJECTURES_INFORMATIONAL_GEOMETRY.md), [Topology](DOCUMENTATION/02_THEORY_AND_RESEARCH/foundations/TOPOLOGY_OF_INTELLIGENCE.md)
  - **Biology Context**: [Medical Frontiers](DOCUMENTATION/02_THEORY_AND_RESEARCH/biology_context/MEDICAL_FRONTIERS.md), [Scientific Domains](DOCUMENTATION/02_THEORY_AND_RESEARCH/biology_context/SCIENTIFIC_DOMAINS.md)
  - **Academic Output**: [Database](DOCUMENTATION/02_THEORY_AND_RESEARCH/academic_output/ACADEMIC_DATABASE.md), [Research Opps](DOCUMENTATION/02_THEORY_AND_RESEARCH/academic_output/RESEARCH_OPPORTUNITIES.md)
  - **Reports**: [Codebase Analysis](DOCUMENTATION/02_THEORY_AND_RESEARCH/reports/V5_11_CODEBASE_ANALYSIS.md)
- **[03_EXPERIMENTS](DOCUMENTATION/03_EXPERIMENTS_AND_LABS/)**: Research scripts and labs.
  - Contains `bioinformatics/` and `mathematics/` experiments.
- **[01_STAKEHOLDER_RESOURCES](DOCUMENTATION/01_STAKEHOLDER_RESOURCES/)**: Pitch decks and summaries.

### `results/` - Data & Outputs

- `checkpoints/`: Model weights.
- `training_runs/`: TensorBoard logs.
- `alphafold_predictions/`: Protein structure data.

### `DOCUMENTATION/03_EXPERIMENTS_AND_LABS/`

Sandbox for research scripts. Contains no heavy data.

### `DOCUMENTATION/01_STAKEHOLDER_RESOURCES/`

- **[Executive Summary (PITCH)](DOCUMENTATION/01_STAKEHOLDER_RESOURCES/PITCH.md)**: For Investors & Lab Directors.

---

## Documentation

Comprehensive documentation is available:

### Architecture & API

- **ARCHITECTURE.md** (`DOCUMENTATION/01_STAKEHOLDER_RESOURCES/industry/technical_specs/ARCHITECTURE.md`) - Complete system architecture
- **API_REFERENCE.md** (`DOCUMENTATION/01_STAKEHOLDER_RESOURCES/industry/technical_specs/API_REFERENCE.md`) - Complete API documentation
- **REFACTORING_SUMMARY.md** (`DOCUMENTATION/01_STAKEHOLDER_RESOURCES/industry/technical_specs/REFACTORING_SUMMARY.md`) - Refactoring overview

### Validation & Reports

- **REFACTORING_VALIDATION_REVIEW.md** (`DOCUMENTATION/04_PROJECT_MANAGEMENT/reports/REFACTORING_VALIDATION_REVIEW.md`) - Comprehensive validation
- **REFACTORING_SESSION_SUMMARY.md** (`DOCUMENTATION/04_PROJECT_MANAGEMENT/reports/REFACTORING_SESSION_SUMMARY.md`) - Complete session summary
- **MERGE_SUMMARY.md** - Deployment summary

### Theory

- **MATHEMATICAL_FOUNDATIONS.md** (`DOCUMENTATION/01_STAKEHOLDER_RESOURCES/academic/theory_deep_dive/`) - Mathematical foundations
- **DUAL_VAE_ARCHITECTURE.md** (`DOCUMENTATION/02_THEORY_AND_RESEARCH/theory/DUAL_VAE_ARCHITECTURE.md`) - Architecture details
- **STATENET_CONTROLLER.md** (`DOCUMENTATION/02_THEORY_AND_RESEARCH/theory/STATENET_CONTROLLER.md`) - StateNet explanation
- **PHASE_TRANSITIONS.md** (`DOCUMENTATION/02_THEORY_AND_RESEARCH/theory/PHASE_TRANSITIONS.md`) - Training phases

---

## Migration from Original

If you're using the original monolithic trainer, migrating is straightforward:

### Quick Migration

```python
# Old (monolithic)
from scripts.train.train_ternary_v5_6 import DNVAETrainerV5
trainer = DNVAETrainerV5(config, device)
trainer.train(train_loader, val_loader)

# New (modular)
from src.models.ternary_vae_v5_6 import DualNeuralVAEV5
from src.training import TernaryVAETrainer

model = DualNeuralVAEV5(...)
trainer = TernaryVAETrainer(model, config, device)
trainer.train(train_loader, val_loader)
```

**See**: `DOCUMENTATION/01_GUIDES/developers/MIGRATION_GUIDE.md` for complete instructions

---

## Version History

- **v5.10.2** (2025-12-14): GPU-first training optimizations (P0-P3 fixes), zero-transfer architecture
- **v5.10.1** (2025-12-12): Pure Hyperbolic Geometry with homeostatic emergence, StateNet v4
- **v5.10.0** (2025-12-11): HyperbolicVAETrainer, 3-adic ranking loss, Poincare ball embedding
- **v5.7.0** (2025-12-10): Hyperbolic prior integration, Frechet centroids
- **v5.6.0** (2025-12-10): TensorBoard integration, torch.compile optimization
- **v5.5.0-srp** (2025-11-24): Complete SRP refactoring, modular architecture
- **v5.5** (2025-11-23): Production release, 97.6% coverage
- **v5.4** (2025-10): Extended training, 99.57% peak at epoch 40
- **v5.0-v5.3** (2025-10): Dual-VAE baseline through gradient balance fixes

---

## Citation

If you use this model in your research, please cite:

```bibtex
@software{ternary_vae_v5_10,
  title={Ternary VAE v5.10: Hyperbolic Embedding of 3-Adic Ultrametric Operation Space},
  author={AI Whisperers},
  year={2025},
  version={5.10.1},
  url={https://github.com/gesttaltt/ternary-vaes},
  note={Poincare ball embedding preserving 3-adic ultrametric structure via ranking loss}
}
```

---

## License

**PolyForm Noncommercial License 1.0.0**

This software is free for:
- Academic research and education
- Student projects and coursework
- Non-profit research organizations
- Personal learning and experimentation

**Commercial use requires a separate license.** Contact support@aiwhisperers.com for commercial licensing.

See [LICENSE](LICENSE) file for full terms.

---

## Contributing

Contributions are welcome! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

The modular architecture makes it easy to:

- Add new loss components
- Create custom schedulers
- Extend the trainer with callbacks
- Add new metrics and monitoring

See `guides/developers/ARCHITECTURE.md` for architecture details.

---

## Third-Party Code and Data

This project includes limited third-party code for **validation purposes only**. See [NOTICE](NOTICE) for complete attribution.

**Important**: All core IP (Ternary VAE, hyperbolic geometry, HIV analysis) is original work. Third-party code is used only for structure file parsing and validation.

### AlphaFold3 Utilities (Validation Only)

Files in `research/alphafold3/utils/` contain atom/residue name constants from DeepMind's AlphaFold3 project, used for parsing PDB files.

- **License**: CC BY-NC-SA 4.0
- **Purpose**: Structure file parsing utilities only
- **NOT used for**: Core algorithms or research findings

### AlphaFold3 Predictions (Validation Only)

Structural predictions from AlphaFold Server are used to **validate** our independent findings, not to generate them. Our research conclusions derive from our own Ternary VAE methodology.

### Dependencies

All Python dependencies use permissive licenses (MIT, BSD, Apache 2.0). See `requirements.txt` for the full list.

---

## Export Control Notice

This software and associated research data may be subject to export control regulations.

### Compliance Requirements

- **EAR (Export Administration Regulations)**: This software contains encryption and bioinformatics research that may require export licenses for certain countries
- **Dual-Use Research**: HIV/viral research components may be subject to dual-use research oversight

### User Responsibilities

Before using or distributing this software:

1. Verify compliance with your institution's export control policies
2. Obtain necessary approvals for international collaborations
3. Do not share with embargoed countries or sanctioned entities

### Permitted Use

- Academic research at accredited institutions
- Non-commercial scientific collaboration
- Educational purposes

For export control questions, contact your institution's export control office.

---

## Contact

For questions, issues, or contributions:

- GitHub Issues: [Create an issue](https://github.com/gesttaltt/ternary-vaes/issues)
- Documentation: See `DOCUMENTATION/01_STAKEHOLDER_RESOURCES/` directory
- Architecture: `DOCUMENTATION/01_STAKEHOLDER_RESOURCES/industry/technical_specs/ARCHITECTURE.md`
- Migration: `DOCUMENTATION/01_STAKEHOLDER_RESOURCES/industry/technical_specs/MIGRATION_GUIDE.md`
- API: `DOCUMENTATION/01_STAKEHOLDER_RESOURCES/industry/technical_specs/API_REFERENCE.md`

---

## Research Implications: Isometric Embedding of Ultrametric Space

### Hyperbolic Geometry for Tree-Structured Data

**Key Insight**: The 3-adic ultrametric on 19,683 operations forms a **9-level ternary tree**. Hyperbolic space (Poincare ball) is the natural geometry for tree embedding:

| Property                 | Euclidean           | Hyperbolic (Poincare)    |
| ------------------------ | ------------------- | ------------------------ |
| Volume growth            | Polynomial O(r^d)   | Exponential O(e^r)       |
| Tree embedding           | Distortion O(log n) | Distortion O(1)          |
| Hierarchy representation | Flat                | Root‚Üíboundary stratified |

### v5.10 Training Objective

The **3-adic ranking loss** explicitly trains for isometric embedding:

```text
For triplets (anchor, positive, negative) where d_3adic(a,p) < d_3adic(a,n):
    Enforce: d_poincare(z_a, z_p) < d_poincare(z_a, z_n)
```

Combined with **hyperbolic KL divergence** against a **wrapped normal prior** on the Poincare ball.

### Current Status vs. Requirements

| Requirement            | Status             | Implementation                                           |
| ---------------------- | ------------------ | -------------------------------------------------------- |
| Complete coverage      | ‚úÖ 99.7%           | Dual-VAE with cross-injection                            |
| 3-adic structure       | ‚úÖ Ranking loss    | `src/metrics/hyperbolic.py`                              |
| Hyperbolic embedding   | ‚úÖ Poincare ball   | `project_to_poincare()`                                  |
| Homeostatic prior      | ‚úÖ Self-regulating | `HomeostaticHyperbolicPrior`                             |
| Manifold visualization | ‚ùå Scalars only    | See `DOCUMENTATION/01_GUIDES/developers/ARCHITECTURE.md` |
| Algebraic closure      | ‚ùå Not attempted   | Future research target                                   |

### Known Gap: Manifold Observability

The current TensorBoard implementation logs **50+ scalar metrics** but **zero embedding visualizations**. We track correlation coefficients that could be achieved by degenerate solutions without verifying the actual manifold structure. See `src/visualization/` module.

---

## Acknowledgments

**Refactoring Approach**: Aggressive (no backward compatibility patches)
**Methodology**: Single Responsibility Principle (SOLID)
**Patterns**: Dependency Injection, Clean Architecture, Interface Segregation
**Result**: Production-ready, maintainable, extensible codebase exceeding professional software engineering standards

**Status**: ‚úÖ Production-Ready | ‚úÖ Fully Validated | ‚úÖ Comprehensively Documented
