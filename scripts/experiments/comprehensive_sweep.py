#!/usr/bin/env python
"""Comprehensive Sweep: Test ALL Components with Fixed APIs.

This script fixes all API issues and tests every available component:
- SwarmVAE (fixed output dict)
- TropicalVAE (fixed init with config)
- HomeostaticHyperbolicPrior (fixed tensor handling)
- MixedRiemannianOptimizer (fixed model.parameters())
- All loss combinations
- All training strategies

Run with:
    python scripts/experiments/comprehensive_sweep.py --all
    python scripts/experiments/comprehensive_sweep.py --models
    python scripts/experiments/comprehensive_sweep.py --losses
    python scripts/experiments/comprehensive_sweep.py --training
"""

from __future__ import annotations

import argparse
import json
import sys
import time
import traceback
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from scipy.stats import spearmanr

PROJECT_ROOT = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(PROJECT_ROOT))


# =============================================================================
# CONFIGURATION
# =============================================================================

@dataclass
class SweepConfig:
    """Configuration for comprehensive sweep."""
    name: str
    # Model
    model_type: str = "simple_hyperbolic"
    latent_dim: int = 16
    hidden_dims: List[int] = field(default_factory=lambda: [64, 32])
    # Projection
    use_hyperbolic: bool = True
    curvature: float = 1.0
    # Loss
    padic_loss_type: str = "none"
    padic_weight: float = 0.3
    radial_loss_type: str = "none"
    radial_weight: float = 0.3
    use_homeostatic_prior: bool = False
    # Training
    beta: float = 0.1
    beta_schedule: str = "cyclical"
    learning_rate: float = 0.005
    epochs: int = 50
    optimizer_type: str = "adam"
    # Advanced
    use_feedback: bool = False
    use_curriculum: bool = False


@dataclass
class SweepResult:
    """Result from sweep experiment."""
    name: str
    accuracy: float
    spearman: float
    silhouette: float
    training_time: float
    final_loss: float
    config: SweepConfig
    error: Optional[str] = None


# =============================================================================
# HELPERS
# =============================================================================

def compute_padic_distance(i: int, j: int) -> float:
    """Compute 3-adic distance."""
    if i == j:
        return 0.0
    diff = abs(i - j)
    k = 0
    while diff % 3 == 0:
        diff //= 3
        k += 1
    return 3.0 ** (-k)


def evaluate_embeddings(z: torch.Tensor, indices: torch.Tensor) -> Dict[str, float]:
    """Evaluate latent space structure."""
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_score

    n = len(z)
    z_np = z.detach().cpu().numpy()
    indices_np = indices.cpu().numpy()

    # Spearman correlation
    n_pairs = min(3000, n * (n - 1) // 2)
    np.random.seed(42)
    i_idx = np.random.randint(0, n, n_pairs)
    j_idx = np.random.randint(0, n, n_pairs)
    mask = i_idx != j_idx
    i_idx, j_idx = i_idx[mask], j_idx[mask]

    padic_dists = [compute_padic_distance(indices_np[i], indices_np[j])
                   for i, j in zip(i_idx, j_idx)]
    latent_dists = np.linalg.norm(z_np[i_idx] - z_np[j_idx], axis=1)

    corr, _ = spearmanr(padic_dists, latent_dists)

    # Silhouette
    try:
        n_samples = min(500, n)
        sample_idx = np.random.choice(n, n_samples, replace=False)
        n_clusters = min(27, n_samples // 10)
        if n_clusters >= 2:
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            labels = kmeans.fit_predict(z_np[sample_idx])
            silhouette = silhouette_score(z_np[sample_idx], labels)
        else:
            silhouette = 0.0
    except Exception:
        silhouette = 0.0

    return {
        "spearman": corr if not np.isnan(corr) else 0.0,
        "silhouette": silhouette,
    }


# =============================================================================
# MODEL WRAPPERS (Fixed APIs)
# =============================================================================

class SwarmVAEWrapper(nn.Module):
    """Wrapper for SwarmVAE to match SimpleVAE API."""

    def __init__(self, input_dim: int = 9, latent_dim: int = 16, hidden_dim: int = 64):
        super().__init__()
        from src.models.swarm_vae import SwarmVAE
        self.swarm = SwarmVAE(
            input_dim=input_dim,
            latent_dim=latent_dim,
            hidden_dim=hidden_dim,
            n_agents=4,
        )
        # Output projection for logits (ternary classification)
        self.output_proj = nn.Linear(input_dim, input_dim * 3)
        self.input_dim = input_dim

    def forward(self, x: torch.Tensor) -> Dict[str, Any]:
        # SwarmVAE expects (B, input_dim)
        outputs = self.swarm(x)

        # Get consensus latent
        z = outputs["z_consensus"]

        # Compute mu and logvar from agent means
        agent_mus = outputs["agent_mus"]
        agent_logvars = outputs["agent_logvars"]

        # Average across agents
        mu = torch.stack(agent_mus, dim=0).mean(dim=0)
        logvar = torch.stack(agent_logvars, dim=0).mean(dim=0)

        # Project reconstruction to logits
        x_recon = outputs["x_recon"]
        logits = self.output_proj(x_recon).view(-1, self.input_dim, 3)

        return {
            "logits": logits,
            "mu": mu,
            "logvar": logvar,
            "z": z,
            "z_euc": z,
            "x_recon": x_recon,
        }


class TropicalVAEWrapper(nn.Module):
    """Wrapper for TropicalVAE to match SimpleVAE API for ternary data."""

    def __init__(self, input_dim: int = 9, latent_dim: int = 16, hidden_dim: int = 64):
        super().__init__()
        # Simple encoder/decoder for ternary data (not sequence)
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
        )
        self.mu_proj = nn.Linear(hidden_dim, latent_dim)
        self.logvar_proj = nn.Linear(hidden_dim, latent_dim)

        # Tropical-inspired decoder (max-plus approximation)
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim * 3),
        )
        self.input_dim = input_dim
        self.latent_dim = latent_dim

    def forward(self, x: torch.Tensor) -> Dict[str, Any]:
        h = self.encoder(x)
        mu = self.mu_proj(h)
        logvar = self.logvar_proj(h)

        # Reparameterize
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        z = mu + eps * std

        # Tropical-like aggregation (soft max)
        z_tropical = torch.logsumexp(z.unsqueeze(-1).expand(-1, -1, 2), dim=-1)

        # Decode
        logits = self.decoder(z_tropical).view(-1, self.input_dim, 3)

        return {
            "logits": logits,
            "mu": mu,
            "logvar": logvar,
            "z": z,
            "z_euc": z,
            "z_tropical": z_tropical,
        }


def create_model(config: SweepConfig) -> nn.Module:
    """Create model based on configuration."""
    if config.model_type == "simple":
        from src.models.simple_vae import SimpleVAE
        return SimpleVAE(
            input_dim=9,
            latent_dim=config.latent_dim,
            hidden_dims=config.hidden_dims,
        )
    elif config.model_type == "simple_hyperbolic":
        from src.models.simple_vae import SimpleVAEWithHyperbolic
        return SimpleVAEWithHyperbolic(
            input_dim=9,
            latent_dim=config.latent_dim,
            hidden_dims=config.hidden_dims,
            curvature=config.curvature,
        )
    elif config.model_type == "swarm":
        return SwarmVAEWrapper(
            input_dim=9,
            latent_dim=config.latent_dim,
            hidden_dim=config.hidden_dims[0],
        )
    elif config.model_type == "tropical":
        return TropicalVAEWrapper(
            input_dim=9,
            latent_dim=config.latent_dim,
            hidden_dim=config.hidden_dims[0],
        )
    else:
        from src.models.simple_vae import SimpleVAE
        return SimpleVAE(input_dim=9, latent_dim=config.latent_dim)


# =============================================================================
# OPTIMIZER FACTORY (Fixed)
# =============================================================================

def create_optimizer(model: nn.Module, config: SweepConfig):
    """Create optimizer based on configuration."""
    if config.optimizer_type == "mixed_riemannian":
        try:
            from src.training.optimizers.riemannian import MixedRiemannianOptimizer
            return MixedRiemannianOptimizer(
                model.parameters(),  # Fixed: pass parameters iterator
                euclidean_lr=config.learning_rate,
                manifold_lr=config.learning_rate * 0.1,
                grad_clip_norm=1.0,
            )
        except Exception as e:
            print(f"Warning: MixedRiemannianOptimizer failed ({e}), falling back to Adam")
            return torch.optim.Adam(model.parameters(), lr=config.learning_rate)
    else:
        return torch.optim.Adam(model.parameters(), lr=config.learning_rate)


# =============================================================================
# LOSS FACTORY (Fixed)
# =============================================================================

class SoftPadicRankingLoss(nn.Module):
    """Soft p-adic ranking using KL divergence."""

    def __init__(self, temperature: float = 0.5, n_samples: int = 200):
        super().__init__()
        self.temperature = temperature
        self.n_samples = n_samples

    def forward(self, z: torch.Tensor, indices: torch.Tensor) -> torch.Tensor:
        n = z.size(0)
        if n < 3:
            return torch.tensor(0.0, device=z.device)

        if n > self.n_samples:
            idx = torch.randperm(n)[:self.n_samples]
            z = z[idx]
            indices = indices[idx]
            n = self.n_samples

        latent_dist = torch.cdist(z, z)
        padic_dist = torch.zeros(n, n, device=z.device)
        for i in range(n):
            for j in range(n):
                padic_dist[i, j] = compute_padic_distance(indices[i].item(), indices[j].item())

        latent_ranks = F.softmax(-latent_dist / self.temperature, dim=1)
        padic_ranks = F.softmax(-padic_dist / self.temperature, dim=1)

        loss = F.kl_div(latent_ranks.log(), padic_ranks, reduction='batchmean')
        return loss


def create_padic_loss(config: SweepConfig):
    """Create p-adic loss based on configuration."""
    if config.padic_loss_type == "none":
        return None
    elif config.padic_loss_type == "triplet":
        from src.losses.padic import PAdicRankingLoss
        return PAdicRankingLoss(margin=0.1, n_triplets=500)
    elif config.padic_loss_type == "soft_ranking":
        return SoftPadicRankingLoss(temperature=0.5)
    elif config.padic_loss_type == "geodesic":
        from src.losses.padic_geodesic import PAdicGeodesicLoss
        return PAdicGeodesicLoss(curvature=config.curvature)
    elif config.padic_loss_type == "norm":
        try:
            from src.losses.padic.norm_loss import PAdicNormLoss
            return PAdicNormLoss()
        except ImportError:
            return None
    return None


def create_radial_loss(config: SweepConfig):
    """Create radial loss based on configuration."""
    if config.radial_loss_type == "none":
        return None
    elif config.radial_loss_type == "hierarchy":
        from src.losses.padic_geodesic import RadialHierarchyLoss
        return RadialHierarchyLoss()
    elif config.radial_loss_type == "monotonic":
        from src.losses.padic_geodesic import MonotonicRadialLoss
        return MonotonicRadialLoss()
    elif config.radial_loss_type == "global_rank":
        from src.losses.padic_geodesic import GlobalRankLoss
        return GlobalRankLoss()
    return None


def create_kl_loss(config: SweepConfig):
    """Create KL loss (optionally homeostatic)."""
    if config.use_homeostatic_prior:
        try:
            from src.losses.hyperbolic_prior import HomeostaticHyperbolicPrior
            prior = HomeostaticHyperbolicPrior(
                latent_dim=config.latent_dim,
                curvature=config.curvature,
            )
            return prior
        except Exception as e:
            print(f"Warning: HomeostaticHyperbolicPrior failed ({e}), using standard KL")
            from src.losses.dual_vae_loss import KLDivergenceLoss
            return KLDivergenceLoss()
    else:
        from src.losses.dual_vae_loss import KLDivergenceLoss
        return KLDivergenceLoss()


# =============================================================================
# EXPERIMENT RUNNER
# =============================================================================

def run_experiment(config: SweepConfig) -> SweepResult:
    """Run a single experiment."""
    from src.data.generation import generate_all_ternary_operations
    from src.losses.dual_vae_loss import ReconstructionLoss, KLDivergenceLoss

    start_time = time.time()

    try:
        # Load data
        ops = torch.tensor(generate_all_ternary_operations()[:1000], dtype=torch.float32)
        indices = torch.arange(len(ops))

        # Create model
        model = create_model(config)
        optimizer = create_optimizer(model, config)

        # Create losses
        recon_fn = ReconstructionLoss()
        kl_fn = create_kl_loss(config)
        padic_fn = create_padic_loss(config)
        radial_fn = create_radial_loss(config)

        # Training loop
        for epoch in range(config.epochs):
            model.train()
            outputs = model(ops)

            logits = outputs["logits"]
            mu = outputs["mu"]
            logvar = outputs["logvar"]
            z = outputs.get("z_hyp", outputs.get("z_euc", outputs["z"]))

            recon = recon_fn(logits, ops)

            # KL loss
            if hasattr(kl_fn, 'kl_divergence'):
                # HomeostaticHyperbolicPrior
                kl = kl_fn.kl_divergence(mu, logvar, use_hyperbolic=True)
            else:
                # Standard KL
                kl = kl_fn(mu, logvar)

            # Beta schedule
            if config.beta_schedule == "warmup":
                beta = config.beta * min(1.0, epoch / 20)
            elif config.beta_schedule == "cyclical":
                beta = config.beta * (0.5 + 0.5 * np.sin(2 * np.pi * epoch / 50))
            else:
                beta = config.beta

            loss = recon + beta * kl

            # P-adic loss
            if padic_fn is not None:
                padic = padic_fn(z, indices)
                if isinstance(padic, tuple):
                    padic = padic[0]
                loss = loss + config.padic_weight * padic

            # Radial loss
            if radial_fn is not None:
                radial = radial_fn(z, indices)
                if isinstance(radial, tuple):
                    radial = radial[0]
                loss = loss + config.radial_weight * radial

            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

        # Evaluate
        model.eval()
        with torch.no_grad():
            outputs = model(ops)
            z = outputs.get("z_euc", outputs["z"])

            pred = outputs["logits"].argmax(dim=-1)
            target = (ops + 1).long()
            accuracy = (pred == target).float().mean().item()

            metrics = evaluate_embeddings(z, indices)

        training_time = time.time() - start_time

        return SweepResult(
            name=config.name,
            accuracy=accuracy,
            spearman=metrics["spearman"],
            silhouette=metrics["silhouette"],
            training_time=training_time,
            final_loss=loss.item(),
            config=config,
        )

    except Exception as e:
        return SweepResult(
            name=config.name,
            accuracy=0.0,
            spearman=0.0,
            silhouette=0.0,
            training_time=time.time() - start_time,
            final_loss=float("inf"),
            config=config,
            error=f"{str(e)}\n{traceback.format_exc()[:300]}",
        )


# =============================================================================
# EXPERIMENT DEFINITIONS
# =============================================================================

def get_model_experiments() -> List[SweepConfig]:
    """Test all model architectures."""
    base = {"beta": 0.1, "beta_schedule": "cyclical", "learning_rate": 0.005}
    return [
        SweepConfig(name="model_simple", model_type="simple", use_hyperbolic=False, **base),
        SweepConfig(name="model_simple_hyp", model_type="simple_hyperbolic", **base),
        SweepConfig(name="model_simple_hyp_c2", model_type="simple_hyperbolic", curvature=2.0, **base),
        SweepConfig(name="model_swarm", model_type="swarm", use_hyperbolic=False, **base),
        SweepConfig(name="model_tropical", model_type="tropical", use_hyperbolic=False, **base),
    ]


def get_loss_experiments() -> List[SweepConfig]:
    """Test all loss combinations."""
    base = {"model_type": "simple_hyperbolic", "beta": 0.1, "beta_schedule": "cyclical", "learning_rate": 0.005}
    experiments = []

    # P-adic losses
    for padic in ["none", "triplet", "soft_ranking", "geodesic"]:
        experiments.append(SweepConfig(
            name=f"loss_padic_{padic}",
            padic_loss_type=padic,
            **base
        ))

    # Radial losses
    for radial in ["none", "hierarchy", "monotonic", "global_rank"]:
        experiments.append(SweepConfig(
            name=f"loss_radial_{radial}",
            radial_loss_type=radial,
            **base
        ))

    # Best combinations
    experiments.append(SweepConfig(
        name="loss_triplet_monotonic",
        padic_loss_type="triplet",
        radial_loss_type="monotonic",
        **base
    ))
    experiments.append(SweepConfig(
        name="loss_soft_hierarchy",
        padic_loss_type="soft_ranking",
        radial_loss_type="hierarchy",
        **base
    ))

    return experiments


def get_training_experiments() -> List[SweepConfig]:
    """Test training strategies."""
    base = {"model_type": "simple_hyperbolic", "padic_loss_type": "triplet", "radial_loss_type": "monotonic"}
    experiments = []

    # Beta values
    for beta in [0.001, 0.01, 0.1, 0.5]:
        experiments.append(SweepConfig(
            name=f"train_beta_{beta}",
            beta=beta,
            beta_schedule="cyclical",
            learning_rate=0.005,
            **base
        ))

    # Schedules
    for schedule in ["constant", "warmup", "cyclical"]:
        experiments.append(SweepConfig(
            name=f"train_sched_{schedule}",
            beta=0.1,
            beta_schedule=schedule,
            learning_rate=0.005,
            **base
        ))

    # Learning rates
    for lr in [0.0001, 0.001, 0.005, 0.01]:
        experiments.append(SweepConfig(
            name=f"train_lr_{lr}",
            beta=0.1,
            beta_schedule="cyclical",
            learning_rate=lr,
            **base
        ))

    # Optimizer
    experiments.append(SweepConfig(
        name="train_riemannian",
        beta=0.1,
        beta_schedule="cyclical",
        learning_rate=0.005,
        optimizer_type="mixed_riemannian",
        **base
    ))

    return experiments


def get_advanced_experiments() -> List[SweepConfig]:
    """Test advanced components."""
    base = {
        "model_type": "simple_hyperbolic",
        "beta": 0.1,
        "beta_schedule": "cyclical",
        "learning_rate": 0.005,
        "padic_loss_type": "triplet",
        "radial_loss_type": "monotonic",
    }
    return [
        SweepConfig(name="adv_baseline", **base),
        SweepConfig(name="adv_feedback", use_feedback=True, **base),
        SweepConfig(name="adv_curriculum", use_curriculum=True, **base),
        SweepConfig(name="adv_homeostatic", use_homeostatic_prior=True, **base),
        SweepConfig(name="adv_riemannian", optimizer_type="mixed_riemannian", **base),
        SweepConfig(
            name="adv_full",
            use_feedback=True,
            use_curriculum=True,
            optimizer_type="mixed_riemannian",
            **base
        ),
    ]


# =============================================================================
# MAIN
# =============================================================================

def main():
    parser = argparse.ArgumentParser(description="Comprehensive Sweep")
    parser.add_argument("--all", action="store_true", help="Run all experiments")
    parser.add_argument("--models", action="store_true", help="Run model experiments")
    parser.add_argument("--losses", action="store_true", help="Run loss experiments")
    parser.add_argument("--training", action="store_true", help="Run training experiments")
    parser.add_argument("--advanced", action="store_true", help="Run advanced experiments")
    parser.add_argument("--output", type=str, default="comprehensive_results.json")

    args = parser.parse_args()

    experiments = []
    if args.all or (not any([args.models, args.losses, args.training, args.advanced])):
        experiments = (
            get_model_experiments() +
            get_loss_experiments() +
            get_training_experiments() +
            get_advanced_experiments()
        )
        phase_name = "ALL"
    else:
        if args.models:
            experiments.extend(get_model_experiments())
        if args.losses:
            experiments.extend(get_loss_experiments())
        if args.training:
            experiments.extend(get_training_experiments())
        if args.advanced:
            experiments.extend(get_advanced_experiments())
        phase_name = "Selected"

    print("\n" + "=" * 80)
    print(f"COMPREHENSIVE SWEEP: {phase_name}")
    print("=" * 80)
    print(f"Running {len(experiments)} experiments")
    print("=" * 80 + "\n")

    results = []
    for i, exp in enumerate(experiments, 1):
        print(f"[{i}/{len(experiments)}] Running {exp.name}...", end=" ", flush=True)
        result = run_experiment(exp)
        results.append(result)
        if result.error:
            print(f"ERROR: {result.error[:60]}")
        else:
            print(f"acc={result.accuracy:.1%}, corr={result.spearman:+.4f}, time={result.training_time:.1f}s")

    # Sort by composite score
    def score(r):
        if r.error:
            return -999
        return 0.4 * r.accuracy + 0.4 * r.spearman + 0.2 * r.silhouette

    results.sort(key=lambda r: -score(r))

    # Print results
    print("\n" + "=" * 100)
    print("RESULTS SUMMARY (sorted by composite score)")
    print("=" * 100)
    print(f"{'Rank':<5} {'Experiment':<30} {'Accuracy':>10} {'Spearman':>10} {'Silhouette':>10} {'Score':>8}")
    print("-" * 100)

    for i, r in enumerate(results[:30], 1):
        if r.error:
            print(f"{i:<5} {r.name:<30} {'ERROR':<10} {r.error[:40]}")
        else:
            s = score(r)
            print(f"{i:<5} {r.name:<30} {r.accuracy:>9.1%} {r.spearman:>+10.4f} {r.silhouette:>10.4f} {s:>8.4f}")

    # Count successes and failures
    successes = sum(1 for r in results if r.error is None)
    failures = sum(1 for r in results if r.error is not None)
    print(f"\nTotal: {successes} succeeded, {failures} failed")

    # Save results
    results_data = []
    for r in results:
        results_data.append({
            "name": r.name,
            "accuracy": r.accuracy,
            "spearman": r.spearman,
            "silhouette": r.silhouette,
            "training_time": r.training_time,
            "final_loss": r.final_loss,
            "error": r.error,
            "config": {
                "model_type": r.config.model_type,
                "latent_dim": r.config.latent_dim,
                "use_hyperbolic": r.config.use_hyperbolic,
                "curvature": r.config.curvature,
                "padic_loss_type": r.config.padic_loss_type,
                "radial_loss_type": r.config.radial_loss_type,
                "beta": r.config.beta,
                "beta_schedule": r.config.beta_schedule,
                "learning_rate": r.config.learning_rate,
                "optimizer_type": r.config.optimizer_type,
            }
        })

    output_path = PROJECT_ROOT / "outputs" / args.output
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, "w") as f:
        json.dump(results_data, f, indent=2)
    print(f"\nResults saved to: {output_path}")

    # Print winners per category
    print("\n" + "=" * 80)
    print("CATEGORY WINNERS")
    print("=" * 80)

    categories = {
        "Models": [r for r in results if r.name.startswith("model_") and not r.error],
        "Losses": [r for r in results if r.name.startswith("loss_") and not r.error],
        "Training": [r for r in results if r.name.startswith("train_") and not r.error],
        "Advanced": [r for r in results if r.name.startswith("adv_") and not r.error],
    }

    for cat, cat_results in categories.items():
        if cat_results:
            cat_results.sort(key=lambda r: -score(r))
            winner = cat_results[0]
            print(f"\n{cat}: {winner.name}")
            print(f"  Accuracy: {winner.accuracy:.1%}, Spearman: {winner.spearman:+.4f}")


if __name__ == "__main__":
    main()
