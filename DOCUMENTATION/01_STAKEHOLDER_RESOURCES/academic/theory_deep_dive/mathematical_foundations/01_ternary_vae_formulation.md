# Part 1: Ternary VAE Formulation & Architecture

> **Context**: This document covers the fundamental problem statement, the ternary operation space, and the core Dual-VAE architecture designed to solve it.

## 1. Problem Statement

### The Ternary Operation Space

A **ternary operation** is a 9-dimensional truth table where each input/output takes values from the ternary alphabet **Î£ = {-1, 0, +1}**.

**Total possible operations**: 3^9 = **19,683**

**Mathematical Representation**:

```
f: Î£^9 â†’ â„^9
where f = (fâ‚, fâ‚‚, ..., fâ‚‰) and each fáµ¢ âˆˆ {-1, 0, +1}
```

### The Challenge

**Goal**: Learn a latent representation that can:

1. **Generate** all 19,683 operations with high probability
2. **Maintain diversity** across the operation space
3. **Avoid collapse** to a subset of operations
4. **Generalize** to unseen combinations

**Why is this hard?**

- Standard VAEs suffer from **posterior collapse**: the decoder ignores the latent code
- Single-pathway models exhibit **mode collapse**: concentrate on easy-to-generate operations
- High-dimensional discrete spaces are difficult to explore systematically

---

## 2. Variational Autoencoder Theory

### Standard VAE Formulation

A Variational Autoencoder (VAE) maximizes the Evidence Lower Bound (ELBO):

```
log p(x) â‰¥ ELBO = ð”¼_{q_Ï†(z|x)}[log p_Î¸(x|z)] - D_KL(q_Ï†(z|x) || p(z))
            ^^^^^^^^^^^^^^^^^^^^^   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
            Reconstruction term      Regularization term
```

**Components**:

- **Encoder**: q_Ï†(z|x) - probabilistic mapping from data to latent space
- **Decoder**: p_Î¸(x|z) - probabilistic mapping from latent to data
- **Prior**: p(z) = ð’©(0, I) - standard Gaussian
- **Parameters**: Ï† (encoder), Î¸ (decoder)

### The Reparameterization Trick

To enable backpropagation through stochastic sampling:

```
z = Î¼_Ï†(x) + Ïƒ_Ï†(x) âŠ™ Îµ,  where Îµ ~ ð’©(0, I)
```

This separates the stochasticity (Îµ) from the learnable parameters (Î¼, Ïƒ).

### Temperature-Scaled Sampling

We extend this with temperature scaling for exploration control:

```
z = Î¼_Ï†(x) + Ï„ Â· Ïƒ_Ï†(x) âŠ™ Îµ
```

where **Ï„** (temperature) controls exploration:

- **Ï„ > 1**: High temperature â†’ more exploration, diverse samples
- **Ï„ < 1**: Low temperature â†’ less exploration, deterministic samples
- **Ï„ = 0**: Deterministic (z = Î¼)

---

## 3. Dual-VAE Architecture

### Why Two VAEs?

A single VAE faces a fundamental trade-off:

- **High Î²** (strong regularization) â†’ poor reconstruction, underfitting
- **Low Î²** (weak regularization) â†’ posterior collapse, ignores latent

**Solution**: Use **two VAEs with complementary objectives**:

1. **VAE-A (Chaotic Regime)**:

   - High temperature â†’ exploration
   - Moderate Î² â†’ balanced KL/reconstruction
   - Task: **Discover new operations**

2. **VAE-B (Frozen Regime)**:
   - Low temperature â†’ exploitation
   - Residual connections â†’ strong reconstruction
   - Task: **Consolidate discoveries**

### Mathematical Formulation

**Joint Objective**:

```
â„’_total = Î»â‚ Â· â„’_A + Î»â‚‚ Â· â„’_B + Î»â‚ƒ Â· |H(z_A) - H(z_B)|
          ^^^^^^^^^^   ^^^^^^^^^^   ^^^^^^^^^^^^^^^^^^^^^
          VAE-A loss   VAE-B loss   Entropy alignment
```

Where:

```
â„’_A = ð”¼_q_A[log p_A(x|z_A)] - Î²_A Â· D_KL(q_A(z_A|x) || p(z))

â„’_B = ð”¼_q_B[log p_B(x|z_B)] - Î²_B Â· D_KL(q_B(z_B|x) || p(z))
      + Î±_ent Â· H_output(p_B(x|z_B))
      + Î±_rep Â· â„’_repulsion(z_B)
```

**Entropy Alignment**:

```
H(z) = -âˆ‘áµ¢ âˆ« p(záµ¢) log p(záµ¢) dzáµ¢   (estimated via histogram)
```

Forces VAE-A and VAE-B to explore similar entropy regimes, preventing divergence.
