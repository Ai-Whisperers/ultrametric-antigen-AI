# Exploration + Exploitation + System Temperature Improvements

**Doc-Type:** Research Direction · Version 1.0 · Updated 2025-12-14

---

## Context

Current training maps 19,683 ternary operations as static points in a 16D latent space. The 3-adic ultrametric structure is intrinsically 1-dimensional (tree depth), but we're not yet capturing compositional dynamics (trajectories, algebraic flows). Extended training runs (7+ hours) could map the entire manifold, but require balancing exploration (coverage) with exploitation (precision).

---

## Problem Statement

1. **Coverage plateau**: Training stalls at ~5% coverage despite architectural complexity
2. **Explore-exploit tradeoff**: Pure exploration risks mode cycling; pure exploitation misses structure
3. **Hyperparameter fragmentation**: VAE-A, VAE-B, and StateNet have ~30 independent hyperparameters tuned separately rather than as a coupled system

---

## Proposed Improvements

### 1. Exploration Harnesses

**Goal**: Maximize manifold coverage during long runs without mode collapse.

| Component | Purpose | Implementation Path |
|-----------|---------|---------------------|
| `CuriosityModule` | Intrinsic motivation for novel states | Already exists in `src/losses/appetitive_losses.py` |
| `ViolationBuffer` | Track hard cases for replay | Already exists in `src/losses/appetitive_losses.py` |
| Novelty Search | Reward latent distance from past embeddings | New: maintain embedding history, penalize proximity |
| Coverage-Gated Temperature | Auto-increase exploration when coverage plateaus | New: if coverage_delta < threshold, increase temp |
| Checkpoint Ensembling | Fork training to preserve diverse attractor basins | New: periodic checkpoint branching with diversity selection |

**Risk Mitigation**: Without ranking signal, exploration can cycle indefinitely. Solution: minimum ranking loss floor even during exploration phases.

---

### 2. Exploitation Harnesses

**Goal**: Achieve precision in 3-adic structure after exploration maps the space.

| Component | Purpose | Implementation Path |
|-----------|---------|---------------------|
| Uncertainty-Guided Focus | Tighten margins where confident, explore where uncertain | New: per-point confidence scores from decoder variance |
| Contrastive Sharpening | Sharpen cluster boundaries post-exploration | New: phase-based margin tightening |
| Hard Negative Mining | Focus on difficult triplets | Exists but disabled (too slow); needs vectorization |
| Progressive Margin Decay | Start with loose margins, tighten as structure emerges | Config change: margin = f(epoch, coverage) |

**Key Insight**: Exploitation should be guided by what exploration discovered, not predetermined.

---

### 3. System Temperature Architecture

**Goal**: Replace ~30 independent hyperparameters with unified system-level control.

#### Current State (Fragmented)

```
VAE-A: beta_start, beta_end, temp_start, temp_end, temp_cyclic, temp_boost_amplitude
VAE-B: beta_start, beta_end, temp_start, temp_end, temp_phase4, entropy_weight, repulsion_weight
StateNet: lr_scale, lambda_scale, ranking_scale, hyp_sigma_scale, hyp_curvature_scale, curriculum_scale
Curriculum: initial_tau, tau_min, tau_max, tau_scale, tau_momentum
Radial: inner_radius, outer_radius, base_weight
Feedback: base_ranking_weight, coverage_threshold, coverage_sensitivity, ...
```

#### Proposed State (Unified)

```
System Temperature (T_sys): Single scalar in [0, 1]
  - T_sys = 0: Maximum exploitation (cold, precise, convergent)
  - T_sys = 1: Maximum exploration (hot, diverse, expansive)

Derived Parameters:
  - VAE-A temp = T_sys * temp_range + temp_min
  - VAE-A beta = (1 - T_sys) * beta_range + beta_min
  - VAE-B temp = 0.8 * VAE-A temp (coupled, slightly cooler)
  - VAE-B beta = 0.6 * VAE-A beta (coupled, more regularized)
  - Ranking margin = (1 - T_sys) * margin_range + margin_min
  - Curriculum tau = (1 - T_sys)  # Cold = ranking focus, Hot = radial focus
  - Exploration bonus = T_sys * curiosity_weight

StateNet Output Extension:
  - Current: 8D (delta_lr, delta_lambdas, delta_ranking, delta_sigma, delta_curvature, delta_curriculum)
  - Proposed: +1D (delta_system_temp)
  - StateNet learns WHEN to explore vs exploit based on training dynamics
```

#### Benefits

1. **Coupled dynamics**: VAE-A and VAE-B move together as intended (chaotic/stable pair)
2. **Emergent scheduling**: No hand-coded phase transitions; StateNet learns optimal trajectory
3. **Fewer hyperparameters**: ~30 reduced to ~10 (bounds and coupling ratios)
4. **Interpretable**: Single dial from "explore everything" to "refine everything"

---

## Extended Training Protocol (7+ Hours)

### Phase 1: Exploration Mapping (0-2 hours)
- T_sys starts high (0.8-1.0)
- Goal: Achieve >80% coverage
- StateNet observes coverage growth, maintains high T_sys while coverage improves
- Checkpoint every 30 minutes with diversity score

### Phase 2: Structure Discovery (2-4 hours)
- T_sys mid-range (0.4-0.6)
- Goal: Attractor basins stabilize, radial correlation improves
- StateNet balances coverage maintenance with structure emergence
- Analyze checkpoints for natural clustering

### Phase 3: Precision Refinement (4-7 hours)
- T_sys drops (0.1-0.3)
- Goal: r > 0.95, sharp cluster boundaries
- StateNet pushes curriculum toward ranking focus
- Hard negative mining activates for final sharpening

### Monitoring

```
Key metrics to track:
- coverage_pct: Target >95%
- radial_correlation: Target >0.90
- ranking_correlation: Target >0.95
- T_sys trajectory: Should show exploration→exploitation arc
- attractor_basin_count: Emergent structure indicator
```

---

## Attractor Basin Analysis

After extended training, analyze checkpoints for:

1. **Natural clustering**: Where do embeddings concentrate without forced structure?
2. **Basin stability**: Which regions persist across checkpoints?
3. **Algebraic correspondence**: Do basins align with cosets, orbits, or subgroups?
4. **Compositional hints**: Do trajectories a→b→(a∘b) show consistent flow?

**Tools needed**:
- Checkpoint embedding extractor
- Clustering analysis (HDBSCAN for variable-density)
- Basin persistence tracking across epochs
- Composition trajectory visualization

---

## Implementation Priority

| Priority | Component | Effort | Impact |
|----------|-----------|--------|--------|
| P0 | System Temperature StateNet output | Medium | High - unifies control |
| P1 | Coverage-gated temperature | Low | Medium - prevents stagnation |
| P1 | Checkpoint ensembling | Low | Medium - preserves diversity |
| P2 | Uncertainty-guided exploitation | Medium | High - smart precision |
| P2 | Attractor basin analysis tools | Medium | High - reveals structure |
| P3 | Full hyperparameter coupling | High | Medium - cleaner system |

---

## Questions for Future Exploration

1. Can StateNet learn the full explore-exploit trajectory, or does it need curriculum hints?
2. What's the minimum training time to map 100% of the 19,683 points distinctly?
3. Do attractor basins correspond to known algebraic structures (cosets, orbits)?
4. Can we detect compositional structure (a∘b prediction) from static embeddings?
5. Is the 3-adic tree the "natural" structure, or will exploration reveal alternatives?

---

## References

- Current codebase: `src/losses/appetitive_losses.py` (CuriosityModule, ViolationBuffer)
- Curriculum control: `src/models/curriculum.py`
- StateNet: `src/models/ternary_vae_v5_10.py`
- Config: `configs/ternary_v5_10.yaml`

---

*This document captures research directions discussed 2025-12-14. No implementation changes made.*
